{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87c9e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, metrics\n",
    "from tensorflow.keras.layers import Input, Conv1D, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization, Concatenate, Lambda\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac525b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('../Data/freesolv.csv')\n",
    "data_clean = data.dropna(subset=['smiles'])\n",
    "smiles = data_clean['smiles']\n",
    "labels = data_clean['freesolv'].astype(float).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2746663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "smiles_dict = {\n",
    "    \"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n",
    "    \"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n",
    "    \"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n",
    "    \"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n",
    "    \"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50,\n",
    "}\n",
    "MAX_SMI_LEN = 100\n",
    "NUM_CHARS = max(smiles_dict.values()) + 1\n",
    "XD = np.zeros((len(smiles), MAX_SMI_LEN, NUM_CHARS))\n",
    "for i, smi in enumerate(smiles):\n",
    "    for j, ch in enumerate(smi[:MAX_SMI_LEN]):\n",
    "        XD[i, j, smiles_dict.get(ch, 0)] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "309d45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "XDinput = Input(shape=(MAX_SMI_LEN, NUM_CHARS), name='XDinput')\n",
    "encode_smiles = Conv1D(filters=64, kernel_size=2, activation='relu')(XDinput)\n",
    "encode_smiles = Conv1D(filters=64, kernel_size=4, activation='relu')(encode_smiles)\n",
    "encode_smiles = Conv1D(filters=128, kernel_size=4, activation='relu')(encode_smiles)\n",
    "model_feature = keras.Model(inputs=XDinput, outputs=encode_smiles, name='model_feature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc635405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TopkRouting(layers.Layer):\n",
    "    def __init__(self, qk_dim, topk=16, qk_scale=None):\n",
    "        super().__init__()\n",
    "        self.topk = topk\n",
    "        self.qk_dim = qk_dim\n",
    "        self.scale = qk_scale if qk_scale is not None else qk_dim**-0.5\n",
    "        self.to_q = layers.Dense(qk_dim)\n",
    "        self.to_k = layers.Dense(qk_dim)\n",
    "        self.to_v = layers.Dense(qk_dim)\n",
    "    def call(self, x):\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "        topk = tf.nn.top_k(attn, k=self.topk).indices\n",
    "        gather = tf.gather(v, topk, batch_dims=1)\n",
    "        return tf.reduce_mean(gather, axis=2)\n",
    "\n",
    "class Block(layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, topk=16, mlp_ratio=3):\n",
    "        super().__init__()\n",
    "        self.attn = TopkRouting(dim, topk)\n",
    "        self.norm1 = layers.LayerNormalization()\n",
    "        self.mlp = keras.Sequential([\n",
    "            layers.Dense(dim*mlp_ratio, activation='relu'),\n",
    "            layers.Dense(dim)\n",
    "        ])\n",
    "        self.norm2 = layers.LayerNormalization()\n",
    "    def call(self, x):\n",
    "        h = self.attn(x)\n",
    "        x = self.norm1(x + h)\n",
    "        h2 = self.mlp(x)\n",
    "        return self.norm2(x + h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "105e496e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"interactionModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"interactionModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n",
       "\u2503<span style=\"font-weight: bold\"> Layer (type)        </span>\u2503<span style=\"font-weight: bold\"> Output Shape      </span>\u2503<span style=\"font-weight: bold\">    Param # </span>\u2503<span style=\"font-weight: bold\"> Connected to      </span>\u2503\n",
       "\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n",
       "\u2502 input_layer         \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>)   \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 -                 \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 model_feature       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   \u2502     <span style=\"color: #00af00; text-decoration-color: #00af00\">55,936</span> \u2502 input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   \u2502    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,864</span> \u2502 model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]\u2026 \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)     \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   \u2502    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,864</span> \u2502 model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]\u2026 \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 concatenate         \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       \u2502                   \u2502            \u2502 lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 global_average_poo\u2026 \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool\u2026</span> \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 global_average_p\u2026 \u2502\n",
       "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n",
       "\u2503\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\u2503\n",
       "\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n",
       "\u2502 input_layer         \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m51\u001b[0m)   \u2502          \u001b[38;5;34m0\u001b[0m \u2502 -                 \u2502\n",
       "\u2502 (\u001b[38;5;33mInputLayer\u001b[0m)        \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 model_feature       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   \u2502     \u001b[38;5;34m55,936\u001b[0m \u2502 input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \u2502\n",
       "\u2502 (\u001b[38;5;33mFunctional\u001b[0m)        \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 block (\u001b[38;5;33mBlock\u001b[0m)       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   \u2502    \u001b[38;5;34m148,864\u001b[0m \u2502 model_feature[\u001b[38;5;34m0\u001b[0m]\u2026 \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 block_1 (\u001b[38;5;33mBlock\u001b[0m)     \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   \u2502    \u001b[38;5;34m148,864\u001b[0m \u2502 model_feature[\u001b[38;5;34m0\u001b[0m]\u2026 \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 lambda (\u001b[38;5;33mLambda\u001b[0m)     \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     \u2502          \u001b[38;5;34m0\u001b[0m \u2502 block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     \u2502          \u001b[38;5;34m0\u001b[0m \u2502 block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 concatenate         \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     \u2502          \u001b[38;5;34m0\u001b[0m \u2502 lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \u2502\n",
       "\u2502 (\u001b[38;5;33mConcatenate\u001b[0m)       \u2502                   \u2502            \u2502 lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 global_average_poo\u2026 \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         \u2502          \u001b[38;5;34m0\u001b[0m \u2502 concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \u2502\n",
       "\u2502 (\u001b[38;5;33mGlobalAveragePool\u2026\u001b[0m \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 lambda_2 (\u001b[38;5;33mLambda\u001b[0m)   \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         \u2502          \u001b[38;5;34m0\u001b[0m \u2502 global_average_p\u2026 \u2502\n",
       "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">353,664</span> (1.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m353,664\u001b[0m (1.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">353,664</span> (1.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m353,664\u001b[0m (1.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dim = 128\n",
    "transformer_block1 = Block(dim)\n",
    "transformer_block2 = Block(dim)\n",
    "\n",
    "input_inter = Input(shape=(MAX_SMI_LEN, NUM_CHARS))\n",
    "features = model_feature(input_inter)\n",
    "trans1 = transformer_block1(features)\n",
    "trans1 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(trans1)\n",
    "trans2 = transformer_block2(features)\n",
    "trans2 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(trans2)\n",
    "combined = Concatenate(axis=-1)([trans1, trans2])\n",
    "pooled = GlobalAveragePooling1D()(combined)\n",
    "difference = Lambda(lambda x: tf.expand_dims(x[:,0]-x[:,1], axis=-1))(pooled)\n",
    "interactionModel = keras.Model(inputs=input_inter, outputs=difference, name='interactionModel')\n",
    "interactionModel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b4a3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "XD_np = np.array(XD)\n",
    "labels_np = labels\n",
    "index = np.where(~np.isnan(labels_np))\n",
    "XD_np = XD_np[index]\n",
    "labels_np = labels_np[index]\n",
    "X_train, X_test, y_train, y_test = train_test_split(XD_np, labels_np, test_size=0.2, random_state=9)\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
    "y_test = scaler_y.transform(y_test.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34682f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:855: UserWarning: Gradients do not exist for variables ['block/topk_routing/dense/kernel', 'block/topk_routing/dense/bias', 'block/topk_routing/dense_1/kernel', 'block/topk_routing/dense_1/bias', 'block_1/topk_routing_1/dense_5/kernel', 'block_1/topk_routing_1/dense_5/bias', 'block_1/topk_routing_1/dense_6/kernel', 'block_1/topk_routing_1/dense_6/bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - loss: 1.1055 - mae: 0.8038 - mse: 1.1055 - r2_score: 0.0307\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 681ms/step - loss: 1.0846 - mae: 0.7944 - mse: 1.0846 - r2_score: 0.0307\n",
      "Epoch 2/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[1m0s\u001b[0m 442ms/step - loss: 1.0083 - mae: 0.7348 - mse: 1.0083 - r2_score: 0.0166\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 478ms/step - loss: 0.9884 - mae: 0.7317 - mse: 0.9884 - r2_score: 0.0166\n",
      "Epoch 3/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - loss: 0.9379 - mae: 0.7244 - mse: 0.9379 - r2_score: 0.0080\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 506ms/step - loss: 0.9688 - mae: 0.7325 - mse: 0.9688 - r2_score: 0.0080\n",
      "Epoch 4/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[1m0s\u001b[0m 506ms/step - loss: 0.8845 - mae: 0.7049 - mse: 0.8845 - r2_score: 0.0379\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 534ms/step - loss: 0.9361 - mae: 0.7201 - mse: 0.9361 - r2_score: 0.0379\n",
      "Epoch 5/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[1m0s\u001b[0m 524ms/step - loss: 0.9098 - mae: 0.7045 - mse: 0.9098 - r2_score: 0.0411\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 550ms/step - loss: 0.9355 - mae: 0.7170 - mse: 0.9355 - r2_score: 0.0411\n",
      "Epoch 6/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[1m0s\u001b[0m 512ms/step - loss: 0.9515 - mae: 0.7337 - mse: 0.9515 - r2_score: 0.0451\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 537ms/step - loss: 0.9557 - mae: 0.7337 - mse: 0.9557 - r2_score: 0.0451\n",
      "Epoch 7/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[1m0s\u001b[0m 514ms/step - loss: 1.0496 - mae: 0.7574 - mse: 1.0496 - r2_score: 0.0467\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 522ms/step - loss: 1.0023 - mae: 0.7460 - mse: 1.0023 - r2_score: 0.0467\n",
      "Epoch 8/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - loss: 0.9204 - mae: 0.7330 - mse: 0.9204 - r2_score: 0.0503\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 542ms/step - loss: 0.9367 - mae: 0.7342 - mse: 0.9367 - r2_score: 0.0503\n",
      "Epoch 9/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[1m0s\u001b[0m 497ms/step - loss: 0.9799 - mae: 0.7408 - mse: 0.9799 - r2_score: 0.0584\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 511ms/step - loss: 0.9644 - mae: 0.7378 - mse: 0.9644 - r2_score: 0.0584\n",
      "Epoch 10/10\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - loss: 0.9496 - mae: 0.7360 - mse: 0.9496 - r2_score: 0.0714\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 535ms/step - loss: 0.9451 - mae: 0.7341 - mse: 0.9451 - r2_score: 0.0714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26b54e92930>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "METRICS_REGRESSION = [metrics.MeanAbsoluteError(name='mae'), metrics.MeanSquaredError(name='mse')]\n",
    "class R2Callback(keras.callbacks.Callback):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.x, self.y = data\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.x, verbose=0)\n",
    "        r2 = sklearn.metrics.r2_score(self.y, y_pred)\n",
    "        logs = logs or {}\n",
    "        logs['r2_score'] = r2\n",
    "        print(f\" - r2_score: {r2:.4f}\")\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=1e-4)\n",
    "interactionModel.compile(optimizer=opt, loss='mse', metrics=METRICS_REGRESSION)\n",
    "cb = [keras.callbacks.EarlyStopping(monitor='loss', patience=30, restore_best_weights=True), R2Callback((X_train, y_train))]\n",
    "interactionModel.fit(X_train, y_train, batch_size=256, epochs=10, callbacks=cb, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ddf311a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer 'block_4' with a weight list of length 14, but the layer was expecting 0 weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m new_transformer_block = Block(dim)\n\u001b[32m      2\u001b[39m new_transformer_block2 = Block(dim)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mnew_transformer_block\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_block1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m new_transformer_block2.set_weights(transformer_block2.get_weights())\n\u001b[32m      5\u001b[39m model_feature.trainable = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:731\u001b[39m, in \u001b[36mLayer.set_weights\u001b[39m\u001b[34m(self, weights)\u001b[39m\n\u001b[32m    729\u001b[39m layer_weights = \u001b[38;5;28mself\u001b[39m.weights\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_weights) != \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    732\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou called `set_weights(weights)` on layer \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    733\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a weight list of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but the layer \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    734\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwas expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m weights.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    735\u001b[39m     )\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variable, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(layer_weights, weights):\n\u001b[32m    737\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m variable.shape != value.shape:\n",
      "\u001b[31mValueError\u001b[39m: You called `set_weights(weights)` on layer 'block_4' with a weight list of length 14, but the layer was expecting 0 weights."
     ]
    }
   ],
   "source": [
    "\n",
    "new_transformer_block = Block(dim)\n",
    "new_transformer_block2 = Block(dim)\n",
    "new_transformer_block.build(model_feature.output_shape)\n",
    "new_transformer_block2.build(model_feature.output_shape)\n",
    "new_transformer_block.set_weights(transformer_block1.get_weights())\n",
    "new_transformer_block2.set_weights(transformer_block2.get_weights())\n",
    "model_feature.trainable = False\n",
    "new_transformer_block.trainable = False\n",
    "new_transformer_block2.trainable = False\n",
    "input_final = Input(shape=(MAX_SMI_LEN, NUM_CHARS))\n",
    "feat_out = model_feature(input_final)\n",
    "tr1 = new_transformer_block(feat_out)\n",
    "tr1 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(tr1)\n",
    "tr2 = new_transformer_block2(feat_out)\n",
    "tr2 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(tr2)\n",
    "concat = Concatenate(axis=-1)([tr1, tr2])\n",
    "pooled_f = GlobalAveragePooling1D()(concat)\n",
    "fc1 = Dense(512, activation='relu')(pooled_f)\n",
    "fc1 = BatchNormalization()(fc1)\n",
    "fc1 = Dropout(0.1)(fc1)\n",
    "fc2 = Dense(256, activation='relu')(fc1)\n",
    "fc2 = BatchNormalization()(fc2)\n",
    "fc2 = Dropout(0.1)(fc2)\n",
    "fc3 = Dense(64, activation='relu')(fc2)\n",
    "output = Dense(1)(fc3)\n",
    "model_final = keras.Model(inputs=input_final, outputs=output, name='regression_model')\n",
    "model_final.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0513ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt_final = optimizers.Adam(learning_rate=1e-4)\n",
    "model_final.compile(optimizer=opt_final, loss='mse', metrics=METRICS_REGRESSION)\n",
    "cb_final = [keras.callbacks.EarlyStopping(monitor='loss', patience=30, restore_best_weights=True), R2Callback((X_train, y_train))]\n",
    "model_final.fit(X_train, y_train, batch_size=256, epochs=10, callbacks=cb_final, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87047cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, X, y_scaled, y_orig):\n",
    "    y_pred_scaled = model.predict(X)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    mae = sklearn.metrics.mean_absolute_error(y_orig, y_pred)\n",
    "    mse = sklearn.metrics.mean_squared_error(y_orig, y_pred)\n",
    "    r2 = sklearn.metrics.r2_score(y_orig, y_pred)\n",
    "    return mae, mse, r2\n",
    "\n",
    "train_metrics = evaluate(model_final, X_train, y_train, scaler_y.inverse_transform(y_train))\n",
    "test_metrics = evaluate(model_final, X_test, y_test, scaler_y.inverse_transform(y_test))\n",
    "print(\"Train MAE, MSE, R2:\", train_metrics)\n",
    "print(\"Test MAE, MSE, R2:\", test_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
