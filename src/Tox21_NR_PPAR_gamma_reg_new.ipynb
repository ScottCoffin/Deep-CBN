{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c9e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, metrics\n",
    "from tensorflow.keras.layers import Input, Conv1D, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization, Concatenate, Lambda\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac525b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('../Data/SMILES_RfD.csv')\n",
    "data_clean = data.dropna(subset=['SMILES'])\n",
    "smiles = data_clean['SMILES']\n",
    "labels = data_clean['target'].astype(float).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "smiles_dict = {\n",
    "    \"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n",
    "    \"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n",
    "    \"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n",
    "    \"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n",
    "    \"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50,\n",
    "}\n",
    "MAX_SMI_LEN = 100\n",
    "NUM_CHARS = max(smiles_dict.values()) + 1\n",
    "XD = np.zeros((len(smiles), MAX_SMI_LEN, NUM_CHARS))\n",
    "for i, smi in enumerate(smiles):\n",
    "    for j, ch in enumerate(smi[:MAX_SMI_LEN]):\n",
    "        XD[i, j, smiles_dict.get(ch, 0)] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "XDinput = Input(shape=(MAX_SMI_LEN, NUM_CHARS), name='XDinput')\n",
    "encode_smiles = Conv1D(filters=64, kernel_size=2, activation='relu')(XDinput)\n",
    "encode_smiles = Conv1D(filters=64, kernel_size=4, activation='relu')(encode_smiles)\n",
    "encode_smiles = Conv1D(filters=128, kernel_size=4, activation='relu')(encode_smiles)\n",
    "model_feature = keras.Model(inputs=XDinput, outputs=encode_smiles, name='model_feature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc635405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TopkRouting(layers.Layer):\n",
    "    def __init__(self, qk_dim, topk=16, qk_scale=None):\n",
    "        super().__init__()\n",
    "        self.topk = topk\n",
    "        self.qk_dim = qk_dim\n",
    "        self.scale = qk_scale if qk_scale is not None else qk_dim**-0.5\n",
    "        self.to_q = layers.Dense(qk_dim)\n",
    "        self.to_k = layers.Dense(qk_dim)\n",
    "        self.to_v = layers.Dense(qk_dim)\n",
    "    def call(self, x):\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "        topk = tf.nn.top_k(attn, k=self.topk).indices\n",
    "        gather = tf.gather(v, topk, batch_dims=1)\n",
    "        return tf.reduce_mean(gather, axis=2)\n",
    "\n",
    "class Block(layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, topk=16, mlp_ratio=3):\n",
    "        super().__init__()\n",
    "        self.attn = TopkRouting(dim, topk)\n",
    "        self.norm1 = layers.LayerNormalization()\n",
    "        self.mlp = keras.Sequential([\n",
    "            layers.Dense(dim*mlp_ratio, activation='relu'),\n",
    "            layers.Dense(dim)\n",
    "        ])\n",
    "        self.norm2 = layers.LayerNormalization()\n",
    "    def call(self, x):\n",
    "        h = self.attn(x)\n",
    "        x = self.norm1(x + h)\n",
    "        h2 = self.mlp(x)\n",
    "        return self.norm2(x + h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dim = 128\n",
    "transformer_block1 = Block(dim)\n",
    "transformer_block2 = Block(dim)\n",
    "\n",
    "input_inter = Input(shape=(MAX_SMI_LEN, NUM_CHARS))\n",
    "features = model_feature(input_inter)\n",
    "trans1 = transformer_block1(features)\n",
    "trans1 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(trans1)\n",
    "trans2 = transformer_block2(features)\n",
    "trans2 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(trans2)\n",
    "combined = Concatenate(axis=-1)([trans1, trans2])\n",
    "pooled = GlobalAveragePooling1D()(combined)\n",
    "difference = Lambda(lambda x: tf.expand_dims(x[:,0]-x[:,1], axis=-1))(pooled)\n",
    "interactionModel = keras.Model(inputs=input_inter, outputs=difference, name='interactionModel')\n",
    "interactionModel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "XD_np = np.array(XD)\n",
    "labels_np = labels\n",
    "index = np.where(~np.isnan(labels_np))\n",
    "XD_np = XD_np[index]\n",
    "labels_np = labels_np[index]\n",
    "X_train, X_test, y_train, y_test = train_test_split(XD_np, labels_np, test_size=0.2, random_state=9)\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
    "y_test = scaler_y.transform(y_test.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34682f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "METRICS_REGRESSION = [metrics.MeanAbsoluteError(name='mae'), metrics.MeanSquaredError(name='mse')]\n",
    "class R2Callback(keras.callbacks.Callback):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.x, self.y = data\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.x, verbose=0)\n",
    "        r2 = sklearn.metrics.r2_score(self.y, y_pred)\n",
    "        logs = logs or {}\n",
    "        logs['r2_score'] = r2\n",
    "        print(f\" - r2_score: {r2:.4f}\")\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=1e-4)\n",
    "interactionModel.compile(optimizer=opt, loss='mse', metrics=METRICS_REGRESSION)\n",
    "cb = [keras.callbacks.EarlyStopping(monitor='loss', patience=30, restore_best_weights=True), R2Callback((X_train, y_train))]\n",
    "interactionModel.fit(X_train, y_train, batch_size=256, epochs=1000, callbacks=cb, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_transformer_block = Block(dim)\n",
    "new_transformer_block2 = Block(dim)\n",
    "new_transformer_block.set_weights(transformer_block1.get_weights())\n",
    "new_transformer_block2.set_weights(transformer_block2.get_weights())\n",
    "model_feature.trainable = False\n",
    "new_transformer_block.trainable = False\n",
    "new_transformer_block2.trainable = False\n",
    "input_final = Input(shape=(MAX_SMI_LEN, NUM_CHARS))\n",
    "feat_out = model_feature(input_final)\n",
    "tr1 = new_transformer_block(feat_out)\n",
    "tr1 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(tr1)\n",
    "tr2 = new_transformer_block2(feat_out)\n",
    "tr2 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(tr2)\n",
    "concat = Concatenate(axis=-1)([tr1, tr2])\n",
    "pooled_f = GlobalAveragePooling1D()(concat)\n",
    "fc1 = Dense(512, activation='relu')(pooled_f)\n",
    "fc1 = BatchNormalization()(fc1)\n",
    "fc1 = Dropout(0.1)(fc1)\n",
    "fc2 = Dense(256, activation='relu')(fc1)\n",
    "fc2 = BatchNormalization()(fc2)\n",
    "fc2 = Dropout(0.1)(fc2)\n",
    "fc3 = Dense(64, activation='relu')(fc2)\n",
    "output = Dense(1)(fc3)\n",
    "model_final = keras.Model(inputs=input_final, outputs=output, name='regression_model')\n",
    "model_final.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0513ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt_final = optimizers.Adam(learning_rate=1e-4)\n",
    "model_final.compile(optimizer=opt_final, loss='mse', metrics=METRICS_REGRESSION)\n",
    "cb_final = [keras.callbacks.EarlyStopping(monitor='loss', patience=30, restore_best_weights=True), R2Callback((X_train, y_train))]\n",
    "model_final.fit(X_train, y_train, batch_size=256, epochs=1000, callbacks=cb_final, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87047cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, X, y_scaled, y_orig):\n",
    "    y_pred_scaled = model.predict(X)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    mae = sklearn.metrics.mean_absolute_error(y_orig, y_pred)\n",
    "    mse = sklearn.metrics.mean_squared_error(y_orig, y_pred)\n",
    "    r2 = sklearn.metrics.r2_score(y_orig, y_pred)\n",
    "    return mae, mse, r2\n",
    "\n",
    "train_metrics = evaluate(model_final, X_train, y_train, scaler_y.inverse_transform(y_train))\n",
    "test_metrics = evaluate(model_final, X_test, y_test, scaler_y.inverse_transform(y_test))\n",
    "print(\"Train MAE, MSE, R2:\", train_metrics)\n",
    "print(\"Test MAE, MSE, R2:\", test_metrics)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
