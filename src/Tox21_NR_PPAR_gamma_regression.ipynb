{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "import sklearn.metrics\n",
    "\n",
    "from tensorflow.keras import optimizers, layers, regularizers, metrics\n",
    "from tensorflow.keras.layers import (\n",
    "    Lambda, Input, Reshape, Activation, Concatenate, Dense, Dropout,\n",
    "    BatchNormalization, Conv1D, GlobalMaxPooling1D, LayerNormalization, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "\n",
    "\n",
    "# ### Cloning a GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c8067",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "get_ipython().system('git clone https://github.com/akianfar/Deep-CBN.git')\n",
    "\n",
    "\n",
    "# ### Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5656bbf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv('/content/Deep-CBN/Data/tox21.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4856fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Data preparation\n",
    "smiles = data['smiles']\n",
    "labels = data['NR-PPAR-gamma']\n",
    "\n",
    "# Dictionary for converting SMILES characters to numbers\n",
    "smiles_dict = {\n",
    "    \"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n",
    "    \"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n",
    "    \"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n",
    "    \"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n",
    "    \"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50, \"T\": 17, \"W\": 51,\n",
    "    \"V\": 18, \"Y\": 52, \"[\": 53, \"Z\": 19, \"]\": 54, \"\\\\\": 20, \"a\": 55, \"c\": 56,\n",
    "    \"b\": 21, \"e\": 57, \"d\": 22, \"g\": 58, \"f\": 23, \"i\": 59, \"h\": 24, \"m\": 60,\n",
    "    \"l\": 25, \"o\": 61, \"n\": 26, \"s\": 62, \"r\": 27, \"u\": 63, \"t\": 28, \"y\": 64,\n",
    "    \" \": 65, \":\": 66, \",\": 67, \"p\": 68, \"j\": 69, \"*\": 70\n",
    "}\n",
    "\n",
    "def label_smiles(line, MAX_SMI_LEN, smi_ch_ind):\n",
    "    X = np.zeros(MAX_SMI_LEN, dtype=int)\n",
    "    for i, ch in enumerate(line[:MAX_SMI_LEN]):\n",
    "        if ch != '\\n' and ch in smi_ch_ind:\n",
    "            X[i] = smi_ch_ind[ch]\n",
    "    return X\n",
    "\n",
    "XD = np.array([label_smiles(str(smi), 100, smiles_dict) for smi in smiles])\n",
    "labels = labels.values\n",
    "\n",
    "# Convert to categorical\n",
    "XD = to_categorical(XD, num_classes=71)\n",
    "\n",
    "\n",
    "# # phaze1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d09043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Feature model definition (unchanged)\n",
    "XDinput = Input(shape=(100, 71), name='XDinput')\n",
    "encode_smiles = Conv1D(filters=64, kernel_size=2, activation='relu', padding='valid', strides=1)(XDinput)  # (99,64)\n",
    "encode_smiles = Conv1D(filters=64, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)  # (96,64)\n",
    "encode_smiles = Conv1D(filters=128, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)  # (93,128)\n",
    "model_feature = Model(inputs=XDinput, outputs=encode_smiles, name='model_feature')\n",
    "model_feature.summary()\n",
    "\n",
    "# Prediction model definition\n",
    "input_extracted_feature = Input(shape=(93, 128))\n",
    "FC1 = Dense(512, activation='relu')(input_extracted_feature)\n",
    "FC1 = BatchNormalization()(FC1)\n",
    "FC2 = Dropout(0.1)(FC1)\n",
    "FC3 = Dense(256, activation='relu')(FC2)\n",
    "FC3 = BatchNormalization()(FC3)\n",
    "FC4 = Dropout(0.1)(FC3)\n",
    "FC5 = Dense(64, activation='relu')(FC4)\n",
    "predictions = Dense(1, activation='linear')(FC5)\n",
    "model_pred = Model(inputs=input_extracted_feature, outputs=predictions)\n",
    "model_pred.summary()\n",
    "\n",
    "# Full model definition with added Pooling layer\n",
    "interaction_input = XDinput\n",
    "encoded_features = model_feature(interaction_input)  # Output: (None, 93, 128)\n",
    "predicted_output = model_pred(encoded_features)      # Output: (None, 93, 1)\n",
    "\n",
    "# Adding Pooling layer to reduce dimensionality\n",
    "pooled_output = GlobalAveragePooling1D()(predicted_output)  # Output: (None, 1)\n",
    "\n",
    "# Final model definition\n",
    "interactionModel = Model(inputs=interaction_input, outputs=pooled_output, name='interactionModel')\n",
    "interactionModel.summary()\n",
    "\n",
    "\n",
    "# ### BiFormer Block\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define custom layers for 1D processing\n",
    "class TopkRouting(layers.Layer):\n",
    "    def __init__(self, qk_dim, topk=16, qk_scale=None, param_routing=False, diff_routing=False):\n",
    "        super().__init__()\n",
    "        self.topk = topk\n",
    "        self.qk_dim = qk_dim\n",
    "        self.scale = qk_scale if qk_scale is not None else qk_dim**-0.5\n",
    "        self.diff_routing = diff_routing\n",
    "        if param_routing:\n",
    "            self.emb = layers.Dense(qk_dim)\n",
    "        else:\n",
    "            self.emb = lambda x: x\n",
    "        self.routing_act = lambda x, axis: tf.nn.softmax(x, axis=axis)\n",
    "\n",
    "    def call(self, query, key, training=None):\n",
    "        if not self.diff_routing:\n",
    "            query = tf.stop_gradient(query)\n",
    "            key = tf.stop_gradient(key)\n",
    "        query_hat = self.emb(query)\n",
    "        key_hat = self.emb(key)\n",
    "\n",
    "        attn_logit = tf.einsum('bnc,bqc->bnq', query_hat*self.scale, key_hat)  # Adjusted for 1D\n",
    "        topk_attn_logit, topk_index = tf.math.top_k(attn_logit, k=self.topk, sorted=True)\n",
    "        r_weight = self.routing_act(topk_attn_logit, axis=-1)\n",
    "        return r_weight, topk_index\n",
    "\n",
    "def tf_gather_kv(kv, r_idx):\n",
    "    n = tf.shape(kv)[0]\n",
    "    p2 = tf.shape(kv)[1]\n",
    "    c_kv = tf.shape(kv)[2]\n",
    "    topk = tf.shape(r_idx)[2]\n",
    "\n",
    "    batch_idx = tf.reshape(tf.range(n), [n, 1, 1])\n",
    "    batch_idx = tf.tile(batch_idx, [1, p2, topk])\n",
    "    p2_idx = tf.reshape(tf.range(p2), [1, p2, 1])\n",
    "    p2_idx = tf.tile(p2_idx, [n, 1, topk])\n",
    "\n",
    "    gather_indices = tf.stack([batch_idx, p2_idx, r_idx], axis=-1)\n",
    "    gathered = tf.gather_nd(kv, gather_indices)\n",
    "    return gathered  # Shape: (n, p2, topk, c_kv)\n",
    "\n",
    "class KVGather(layers.Layer):\n",
    "    def __init__(self, mul_weight='none'):\n",
    "        super().__init__()\n",
    "        assert mul_weight in ['none', 'soft', 'hard']\n",
    "        self.mul_weight = mul_weight\n",
    "\n",
    "    def call(self, r_idx, r_weight, kv, training=None):\n",
    "        topk_kv = tf_gather_kv(kv, r_idx)\n",
    "        if self.mul_weight == 'soft':\n",
    "            r_weight_exp = tf.expand_dims(tf.expand_dims(r_weight, -1), -1)\n",
    "            topk_kv = topk_kv * r_weight_exp\n",
    "        return topk_kv\n",
    "\n",
    "class QKVLinear(layers.Layer):\n",
    "    def __init__(self, dim, qk_dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.qk_dim = qk_dim\n",
    "        self.qkv = layers.Dense(qk_dim + qk_dim + dim, use_bias=bias)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        qkv = self.qkv(x)\n",
    "        q, kv = tf.split(qkv, [self.qk_dim, self.qk_dim + self.dim], axis=-1)\n",
    "        return q, kv\n",
    "\n",
    "class BiLevelRoutingAttention(layers.Layer):\n",
    "    def __init__(self, dim, n_win=7, num_heads=8, qk_dim=None, qk_scale=None,\n",
    "                 kv_per_win=4, kv_downsample_ratio=4, kv_downsample_mode='identity',\n",
    "                 topk=4, param_attention=\"qkvo\", param_routing=False, diff_routing=False, soft_routing=False,\n",
    "                 side_dwconv=3, auto_pad=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_win = n_win\n",
    "        self.num_heads = num_heads\n",
    "        self.qk_dim = qk_dim if qk_dim is not None else dim\n",
    "        self.scale = qk_scale if qk_scale is not None else self.qk_dim**-0.5\n",
    "        self.topk = topk\n",
    "        self.param_routing = param_routing\n",
    "        self.diff_routing = diff_routing\n",
    "        self.soft_routing = soft_routing\n",
    "        self.auto_pad = auto_pad\n",
    "\n",
    "        # For 1D, we use Conv1D\n",
    "        if side_dwconv > 0:\n",
    "            self.lepe = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n",
    "        else:\n",
    "            self.lepe = lambda x: tf.zeros_like(x)\n",
    "\n",
    "        self.router = TopkRouting(qk_dim=self.qk_dim, topk=self.topk, qk_scale=self.scale,\n",
    "                                  param_routing=self.param_routing, diff_routing=self.diff_routing)\n",
    "\n",
    "        mul_weight = 'none'\n",
    "        if self.soft_routing:\n",
    "            mul_weight = 'soft'\n",
    "        self.kv_gather = KVGather(mul_weight=mul_weight)\n",
    "\n",
    "        if param_attention in ['qkvo', 'qkv']:\n",
    "            self.qkv = QKVLinear(self.dim, self.qk_dim)\n",
    "            if param_attention == 'qkvo':\n",
    "                self.wo = layers.Dense(self.dim)\n",
    "            else:\n",
    "                self.wo = lambda x: x\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported param_attention mode\")\n",
    "\n",
    "        self.attn_act = lambda x: tf.nn.softmax(x, axis=-1)\n",
    "        self.kv_down = lambda x: x  # identity for simplicity\n",
    "\n",
    "    def call(self, x, training=None, ret_attn_mask=False):\n",
    "        # Implementing full attention can be complex, so here we only keep the general structure\n",
    "        if ret_attn_mask:\n",
    "            return x, None, None, None\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class Attention(layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n",
    "        self.qkv = layers.Dense(dim*3, use_bias=qkv_bias)\n",
    "        self.attn_drop = layers.Dropout(attn_drop)\n",
    "        self.proj = layers.Dense(dim)\n",
    "        self.proj_drop = layers.Dropout(proj_drop)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        batch_size, seq_length, dim = tf.unstack(tf.shape(x))\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "\n",
    "        attn = tf.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn, training=training)\n",
    "        out = tf.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out, training=training)\n",
    "        return out\n",
    "\n",
    "class AttentionLePE(layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., side_dwconv=5):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n",
    "        self.qkv = layers.Dense(dim*3, use_bias=qkv_bias)\n",
    "        self.attn_drop = layers.Dropout(attn_drop)\n",
    "        self.proj = layers.Dense(dim)\n",
    "        self.proj_drop = layers.Dropout(proj_drop)\n",
    "        if side_dwconv > 0:\n",
    "            self.lepe = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n",
    "        else:\n",
    "            self.lepe = lambda x: tf.zeros_like(x)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        lepe_out = self.lepe(x)\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "\n",
    "        attn = tf.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn, training=training)\n",
    "        out = tf.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')\n",
    "        out = out + lepe_out\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out, training=training)\n",
    "        return out\n",
    "\n",
    "class PreNorm(layers.Layer):\n",
    "    def __init__(self, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.fn(self.norm(x), training=training)\n",
    "\n",
    "class MLP(layers.Layer):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = keras.Sequential([\n",
    "            layers.Dense(hidden_dim),\n",
    "            layers.Activation('gelu'),\n",
    "            layers.Dropout(rate=dropout),\n",
    "            layers.Dense(dim),\n",
    "            layers.Dropout(rate=dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.net(x, training=training)\n",
    "\n",
    "class DropPath(layers.Layer):\n",
    "    # Placeholder for DropPath, currently no-op\n",
    "    def __init__(self, drop_prob=0.):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if (not training) or self.drop_prob == 0.:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += tf.random.uniform(tf.shape(x), dtype=x.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        return tf.divide(x, keep_prob) * binary_tensor\n",
    "\n",
    "class Block(layers.Layer):\n",
    "    def __init__(self, dim, drop_path=0.1, layer_scale_init_value=-1,\n",
    "                 num_heads=8, n_win=7, qk_dim=128, qk_scale=None,\n",
    "                 kv_per_win=8, kv_downsample_ratio=1, kv_downsample_mode='identity',\n",
    "                 topk=8, param_attention=\"qkvo\", param_routing=True, diff_routing=False, soft_routing=True,\n",
    "                 mlp_ratio=4, mlp_dwconv=False, side_dwconv=5, before_attn_dwconv=3, pre_norm=True, auto_pad=True):\n",
    "        super().__init__()\n",
    "        qk_dim = qk_dim or dim\n",
    "\n",
    "        # For 1D, we use Conv1D\n",
    "        if before_attn_dwconv > 0:\n",
    "            self.pos_embed = layers.Conv1D(dim, kernel_size=before_attn_dwconv, padding='same', activation='relu')\n",
    "        else:\n",
    "            self.pos_embed = lambda x: tf.zeros_like(x)\n",
    "\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        if topk > 0:\n",
    "            self.attn = BiLevelRoutingAttention(dim=dim, num_heads=num_heads, n_win=n_win, qk_dim=qk_dim,\n",
    "                                               qk_scale=qk_scale, kv_per_win=kv_per_win, kv_downsample_ratio=kv_downsample_ratio,\n",
    "                                               kv_downsample_mode=kv_downsample_mode, topk=topk, param_attention=param_attention,\n",
    "                                               param_routing=param_routing, diff_routing=diff_routing, soft_routing=soft_routing,\n",
    "                                               side_dwconv=side_dwconv, auto_pad=auto_pad)\n",
    "        elif topk == -1:\n",
    "            self.attn = Attention(dim=dim, num_heads=num_heads, qk_scale=qk_scale)\n",
    "        elif topk == -2:\n",
    "            self.attn = AttentionLePE(dim=dim, num_heads=num_heads, qk_scale=qk_scale, side_dwconv=side_dwconv)\n",
    "        elif topk == 0:\n",
    "            # Pseudo attention\n",
    "            self.attn = keras.Sequential([\n",
    "                layers.Dense(dim),\n",
    "                layers.Conv1D(dim, kernel_size=5, padding='same', activation='relu'),\n",
    "                layers.Dense(dim)\n",
    "            ])\n",
    "\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        mlp_hidden_dim = int(mlp_ratio * dim)\n",
    "        mlp_layers = [layers.Dense(mlp_hidden_dim)]\n",
    "        if mlp_dwconv:\n",
    "            mlp_layers.append(layers.Conv1D(mlp_hidden_dim, kernel_size=3, padding='same', activation='relu'))\n",
    "        mlp_layers.append(layers.Activation('gelu'))\n",
    "        mlp_layers.append(layers.Dense(dim))\n",
    "        mlp_layers.insert(1, layers.Dropout(0.2)) # Add dropout after first Dense\n",
    "        mlp_layers.append(layers.Dropout(0.2))\n",
    "        self.mlp = keras.Sequential(mlp_layers)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else layers.Lambda(lambda x: x)\n",
    "\n",
    "        if layer_scale_init_value > 0:\n",
    "            self.use_layer_scale = True\n",
    "            self.gamma1 = self.add_weight(shape=(dim,),\n",
    "                                          initializer=tf.keras.initializers.Constant(layer_scale_init_value),\n",
    "                                          trainable=True)\n",
    "            self.gamma2 = self.add_weight(shape=(dim,),\n",
    "                                          initializer=tf.keras.initializers.Constant(layer_scale_init_value),\n",
    "                                          trainable=True)\n",
    "        else:\n",
    "            self.use_layer_scale = False\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        x = x + self.pos_embed(x)  # Add positional embedding\n",
    "\n",
    "        if self.pre_norm:\n",
    "            if self.use_layer_scale:\n",
    "                x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x), training=training), training=training)\n",
    "                x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x), training=training), training=training)\n",
    "            else:\n",
    "                x = x + self.drop_path(self.attn(self.norm1(x), training=training), training=training)\n",
    "                x = x + self.drop_path(self.mlp(self.norm2(x), training=training), training=training)\n",
    "        else:\n",
    "            if self.use_layer_scale:\n",
    "                tmp = x + self.drop_path(self.gamma1 * self.attn(x, training=training), training=training)\n",
    "                x = self.norm1(tmp)\n",
    "                tmp = x + self.drop_path(self.gamma2 * self.mlp(x, training=training), training=training)\n",
    "                x = self.norm2(tmp)\n",
    "            else:\n",
    "                tmp = x + self.drop_path(self.attn(x, training=training), training=training)\n",
    "                x = self.norm1(tmp)\n",
    "                tmp = x + self.drop_path(self.mlp(x, training=training), training=training)\n",
    "                x = self.norm2(tmp)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# # phaze2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Parameter settings for the block\n",
    "dim = 128   # Adjusted from 192 to 128\n",
    "num_heads = 8\n",
    "mlp_dim = 128 * 3  # Corresponding to mlp_ratio=3\n",
    "depth = 2\n",
    "dim_head = 48\n",
    "\n",
    "# Create Phase 2 model\n",
    "input_phaz2 = Input(shape=(93, 128))  # Input shape is (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "# processed_input = model_feature(input_phaz2)\n",
    "processed_input = input_phaz2\n",
    "\n",
    "# Define transformer blocks\n",
    "transformer_block1 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "\n",
    "# Apply the first transformer block\n",
    "transformer_output1 = transformer_block1(processed_input)\n",
    "# Compute the norm of the output along the last axis (preserving dimensions)\n",
    "transformer_output1 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1)\n",
    "\n",
    "# Apply the second transformer block\n",
    "transformer_output2 = transformer_block2(processed_input)\n",
    "# Compute the norm of the output along the last axis (preserving dimensions)\n",
    "transformer_output2 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2)\n",
    "\n",
    "# Combine the outputs of both transformer blocks along the last axis\n",
    "combined_outputs = Concatenate(axis=-1)([transformer_output1, transformer_output2])\n",
    "\n",
    "# Apply global average pooling to reduce the output to (batch_size, 2)\n",
    "pooled_outputs = GlobalAveragePooling1D()(combined_outputs)\n",
    "\n",
    "# Compute the difference between the two pooled outputs and reshape to (batch_size, 1)\n",
    "difference = Lambda(lambda x: tf.expand_dims(x[:, 0] - x[:, 1], axis=-1), name='difference')(pooled_outputs)\n",
    "\n",
    "\n",
    "# Output directly for regression\n",
    "condition_2 = difference\n",
    "\n",
    "# Freeze the `model_feature` layers to prevent them from being trained\n",
    "model_feature.trainable = False\n",
    "\n",
    "# Define the complete model\n",
    "model_phaz2 = Model(inputs=input_phaz2, outputs=condition_2)\n",
    "model_phaz2.summary()\n",
    "\n",
    "\n",
    "# # phaze3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61fc67f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define new transformer blocks\n",
    "new_transformer_block = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "new_transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "\n",
    "# Input for Phase 3\n",
    "XDinput_phaz3 = Input(shape=(100, 71))\n",
    "# Pass input through the feature extraction model\n",
    "model_feature_output = model_feature(XDinput_phaz3)  # Output shape: (93, 192)\n",
    "\n",
    "# Transformer blocks processing\n",
    "transformer_output1_phaz3 = new_transformer_block(model_feature_output)  # Output shape: (93, 192)\n",
    "# Compute the norm along the last axis, retaining dimensions\n",
    "transformer_output1_phaz3 = layers.Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1_phaz3)  # Shape: (93, 1)\n",
    "\n",
    "transformer_output2_phaz3 = new_transformer_block2(model_feature_output)  # Output shape: (93, 192)\n",
    "# Compute the norm along the last axis, retaining dimensions\n",
    "transformer_output2_phaz3 = layers.Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2_phaz3)  # Shape: (93, 1)\n",
    "\n",
    "# Combine the outputs of both transformer blocks\n",
    "concatenated_phaz3 = Concatenate(axis=-1)([transformer_output1_phaz3, transformer_output2_phaz3])  # Shape: (93, 2)\n",
    "\n",
    "# Freeze the weights of feature extractor and transformer blocks\n",
    "model_feature.trainable = False\n",
    "new_transformer_block.trainable = False\n",
    "new_transformer_block2.trainable = False\n",
    "\n",
    "# Apply global average pooling to reduce the dimensions to (batch_size, 2)\n",
    "pooled_phaz3 = GlobalAveragePooling1D()(concatenated_phaz3)  # Shape: (None, 2)\n",
    "\n",
    "# Fully connected layers\n",
    "FC1_phaz3 = Dense(512, activation='relu')(pooled_phaz3)\n",
    "FC1_phaz3 = BatchNormalization()(FC1_phaz3)\n",
    "FC2_phaz3 = Dropout(0.1)(FC1_phaz3)  # Increased dropout rate\n",
    "FC3_phaz3 = Dense(256, activation='relu')(FC2_phaz3)\n",
    "FC3_phaz3 = BatchNormalization()(FC3_phaz3)\n",
    "FC4_phaz3 = Dropout(0.1)(FC3_phaz3)  # Increased dropout rate\n",
    "FC5_phaz3 = Dense(64, activation='relu')(FC4_phaz3)\n",
    "\n",
    "# Final prediction layer with linear activation for regression\n",
    "predictions_phaz3 = Dense(1, activation='linear')(FC5_phaz3)\n",
    "\n",
    "# Define the complete model for Phase 3\n",
    "model_phaz3 = Model(inputs=XDinput_phaz3, outputs=predictions_phaz3)\n",
    "\n",
    "# Print the name and trainable status of each layer\n",
    "for layer in model_phaz3.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "\n",
    "# Model summary\n",
    "model_phaz3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Metrics for binary classification tasks\n",
    "# Metrics for regression tasks\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"TensorFlow implementation of the coefficient of determination.\"\"\"\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + tf.keras.backend.epsilon())\n",
    "\n",
    "METRICS_REGRESSION = [\n",
    "    metrics.MeanAbsoluteError(name='mae'),\n",
    "    metrics.MeanSquaredError(name='mse'),\n",
    "    r2_score,\n",
    "]\n",
    "\n",
    "\n",
    "# # Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6da772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === Data Preparation ===\n",
    "XD_np = np.array(XD)\n",
    "labels_np = np.array(labels)\n",
    "index = np.where(~np.isnan(labels_np))\n",
    "labels_np, XD_np = labels_np[index[0]], XD_np[index[0], :]\n",
    "\n",
    "# === Train-Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    XD_np, labels_np, test_size=0.2, random_state=9\n",
    ")\n",
    "\n",
    "# Regression does not require class weights or categorical labels\n",
    "\n",
    "# === Define Optimizer for Interaction Model ===\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "interactionModel.compile(optimizer=opt, loss=\"mse\", metrics=METRICS_REGRESSION)\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=30, verbose=1, mode='min', restore_best_weights=True)\n",
    "reduce_lr = LearningRateScheduler(lambda epoch: 1e-3 * 0.9 ** epoch)\n",
    "\n",
    "# === Phase 1: Train Interaction Model ===\n",
    "interactionModel.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=256, epochs=100,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Phase 1 on test data\n",
    "test_eval_phase1 = interactionModel.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# === Feature Extraction ===\n",
    "feature_train = model_feature.predict(X_train)\n",
    "feature_test = model_feature.predict(X_test)\n",
    "\n",
    "# === Phase 2: Train Model Phase 2 ===\n",
    "opt_phaz2 = optimizers.Adam(learning_rate=0.001)\n",
    "model_phaz2.compile(optimizer=opt_phaz2, loss=\"mse\", metrics=METRICS_REGRESSION)\n",
    "model_phaz2.fit(\n",
    "    feature_train, y_train,\n",
    "    batch_size=256, epochs=100,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    ")\n",
    "\n",
    "# === Freeze Transformer Blocks ===\n",
    "new_transformer_block.set_weights(transformer_block1.get_weights())\n",
    "new_transformer_block2.set_weights(transformer_block2.get_weights())  # Corrected as per your comment\n",
    "\n",
    "# === Phase 3: Train Model Phase 3 ===\n",
    "opt_phaz3 = optimizers.Adam(learning_rate=0.0001)\n",
    "model_phaz3.compile(optimizer=opt_phaz3, loss=\"mse\", metrics=METRICS_REGRESSION)\n",
    "\n",
    "model_phaz3.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=256, epochs=100,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Phase 3 on both training and testing data\n",
    "train_eval_phase3 = model_phaz3.evaluate(X_train, y_train, verbose=1)\n",
    "test_eval_phase3 = model_phaz3.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# === Function to Format Results ===\n",
    "def format_results(phase1_result, phase3_train_result, phase3_test_result):\n",
    "    # Define Metric Names\n",
    "    phase1_metrics = ['loss', 'mae', 'mse', 'r2_score']\n",
    "    phase3_metrics = ['loss', 'mae', 'mse', 'r2_score']\n",
    "\n",
    "    # Create DataFrames\n",
    "    phase1_test_df = pd.DataFrame([phase1_result], columns=phase1_metrics)\n",
    "    phase3_train_df = pd.DataFrame([phase3_train_result], columns=phase3_metrics)\n",
    "    phase3_test_df = pd.DataFrame([test_eval_phase3], columns=phase3_metrics)\n",
    "\n",
    "    # Display the Results\n",
    "    print(\"\\n=== Phase 1: Test Evaluation Results ===\")\n",
    "    print(phase1_test_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "    print(\"\\n=== Phase 3: Train Evaluation Results ===\")\n",
    "    print(phase3_train_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "    print(\"\\n=== Phase 3: Test Evaluation Results ===\")\n",
    "    print(phase3_test_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "\n",
    "# === Display Results ===\n",
    "format_results(test_eval_phase1, train_eval_phase3, test_eval_phase3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Scatter Plot of Predictions vs True Values ===\n",
    "y_pred_test = model_phaz3.predict(X_test).flatten()\n",
    "r2_val = sklearn.metrics.r2_score(y_test.flatten(), y_pred_test)\n",
    "print(f\"Test R2 Score: {r2_val:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=y_test.flatten(), y=y_pred_test)\n",
    "min_val = min(y_test.min(), y_pred_test.min())\n",
    "max_val = max(y_test.max(), y_pred_test.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('True vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b7d8e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# === Attention Rollout Interpretability ===\n",
    "def compute_attention_rollout(model, samples):\n",
    "    inputs = tf.convert_to_tensor(samples)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(inputs)\n",
    "        preds = model(inputs, training=False)\n",
    "    grads = tape.gradient(preds, inputs)\n",
    "    rollout = tf.reduce_mean(tf.abs(grads), axis=-1)\n",
    "    return rollout.numpy()\n",
    "\n",
    "samples_train = X_train[:4]\n",
    "rollouts_train = compute_attention_rollout(model_phaz3, samples_train)\n",
    "for i in range(4):\n",
    "    plt.figure()\n",
    "    plt.plot(rollouts_train[i])\n",
    "    plt.title(f'Train sample {i+1} attention rollout')\n",
    "    plt.xlabel('Token index')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "samples_test = X_test[:4]\n",
    "rollouts_test = compute_attention_rollout(model_phaz3, samples_test)\n",
    "for i in range(4):\n",
    "    plt.figure()\n",
    "    plt.plot(rollouts_test[i])\n",
    "    plt.title(f'Test sample {i+1} attention rollout')\n",
    "    plt.xlabel('Token index')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
