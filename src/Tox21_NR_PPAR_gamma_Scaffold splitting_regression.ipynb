{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w71ntxmiP5Cs"
   },
   "source": [
    "#Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2mc3-OTP210",
    "outputId": "1bcf264f-343b-4610-b609-3c89f2b742f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "from tensorflow.keras import optimizers, layers, regularizers, metrics\n",
    "from tensorflow.keras.layers import (\n",
    "    Lambda, Input, Reshape, Activation, Concatenate, Dense, Dropout,\n",
    "    BatchNormalization, Conv1D, GlobalMaxPooling1D, LayerNormalization, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "from deepchem.data import NumpyDataset\n",
    "from deepchem.splits import ScaffoldSplitter\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWrFzNjSQCtg"
   },
   "source": [
    "### Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "kXFLqk2vja5M",
    "outputId": "3e31a23d-130b-48c7-cfd4-bc76cd6c75bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:17:04] SMILES Parse Error: syntax error while parsing: O|[Al](|O)(|O)|[Al](|O)(|O)Cl\n",
      "[20:17:04] SMILES Parse Error: Failed parsing SMILES 'O|[Al](|O)(|O)|[Al](|O)(|O)Cl' for input: 'O|[Al](|O)(|O)|[Al](|O)(|O)Cl'\n",
      "[20:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:04] Explicit valence for atom # 3 Si, 8, is greater than permitted\n",
      "[20:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows before drop: 4,657\n",
      "Rows after NA drop:  4,222\n",
      "Rows after invalid SMILES drop:  4,218\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CAS</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>log_RfD_mg_kg_d</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100-00-5</td>\n",
       "      <td>C1=CC(=CC=C1[N+](=O)[O-])Cl</td>\n",
       "      <td>-3.060745</td>\n",
       "      <td>-3.060745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>100-01-6</td>\n",
       "      <td>C1=CC(=CC=C1N)[N+](=O)[O-]</td>\n",
       "      <td>-2.193802</td>\n",
       "      <td>-2.193802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10004-44-1</td>\n",
       "      <td>CC1=CC(=O)NO1</td>\n",
       "      <td>-2.015260</td>\n",
       "      <td>-2.015260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1001354-72-8</td>\n",
       "      <td>CCCCC(C(CC)N)O</td>\n",
       "      <td>-1.769892</td>\n",
       "      <td>-1.769892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>10016-20-3</td>\n",
       "      <td>C(C1C2C(C(C(O1)OC3C(OC(C(C3O)O)OC4C(OC(C(C4O)O...</td>\n",
       "      <td>1.032814</td>\n",
       "      <td>1.032814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           CAS  \\\n",
       "0           1      100-00-5   \n",
       "1           2      100-01-6   \n",
       "2           3    10004-44-1   \n",
       "3           4  1001354-72-8   \n",
       "4           6    10016-20-3   \n",
       "\n",
       "                                              SMILES  log_RfD_mg_kg_d  \\\n",
       "0                        C1=CC(=CC=C1[N+](=O)[O-])Cl        -3.060745   \n",
       "1                         C1=CC(=CC=C1N)[N+](=O)[O-]        -2.193802   \n",
       "2                                      CC1=CC(=O)NO1        -2.015260   \n",
       "3                                     CCCCC(C(CC)N)O        -1.769892   \n",
       "4  C(C1C2C(C(C(O1)OC3C(OC(C(C3O)O)OC4C(OC(C(C4O)O...         1.032814   \n",
       "\n",
       "     target  \n",
       "0 -3.060745  \n",
       "1 -2.193802  \n",
       "2 -2.015260  \n",
       "3 -1.769892  \n",
       "4  1.032814  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Loading and Preparing Data\n",
    "data = pd.read_csv('../Data/SMILES_RfD.csv')\n",
    "\n",
    "# Drop any rows where the 'SMILES' column is missing or empty\n",
    "data_clean = data.dropna(subset=['SMILES'])\n",
    "data_clean = data_clean[data_clean['SMILES'].str.strip() != \"\"]\n",
    "\n",
    "data_clean = data_clean.reset_index(drop=True)\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "# Drop rows where the SMILES column is NaN or invalid\n",
    "valid_mask = data_clean['SMILES'].apply(\n",
    "    lambda s: isinstance(s, str) and Chem.MolFromSmiles(s) is not None\n",
    ")\n",
    "data_clean_valid = data_clean[valid_mask]\n",
    "\n",
    "print(f\"\\nRows before drop: {len(data):,}\")\n",
    "print(f\"Rows after NA drop:  {len(data_clean):,}\")\n",
    "print(f\"Rows after invalid SMILES drop:  {len(data_clean_valid):,}\")\n",
    "\n",
    "\n",
    "# Preview the cleaned data\n",
    "data_clean_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AFxos0tgQC9s"
   },
   "outputs": [],
   "source": [
    "smiles = data_clean_valid['SMILES']\n",
    "labels = data_clean_valid['target']\n",
    "\n",
    "# Dictionary to convert SMILES characters into numeric values\n",
    "smiles_dict = {\n",
    "    \"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n",
    "    \"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n",
    "    \"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n",
    "    \"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n",
    "    \"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50, \"T\": 17, \"W\": 51,\n",
    "    \"V\": 18, \"Y\": 52, \"[\": 53, \"Z\": 19, \"]\": 54, \"\\\\\": 20, \"a\": 55, \"c\": 56,\n",
    "    \"b\": 21, \"e\": 57, \"d\": 22, \"g\": 58, \"f\": 23, \"i\": 59, \"h\": 24, \"m\": 60,\n",
    "    \"l\": 25, \"o\": 61, \"n\": 26, \"s\": 62, \"r\": 27, \"u\": 63, \"t\": 28, \"y\": 64,\n",
    "    \" \": 65, \":\": 66, \",\": 67, \"p\": 68, \"j\": 69, \"*\": 70\n",
    "}\n",
    "\n",
    "def label_smiles(line, MAX_SMI_LEN, smi_ch_ind):\n",
    "    X = np.zeros(MAX_SMI_LEN, dtype=int)\n",
    "    for i, ch in enumerate(line[:MAX_SMI_LEN]):\n",
    "        if ch in smi_ch_ind:\n",
    "            X[i] = smi_ch_ind[ch]\n",
    "    return X\n",
    "\n",
    "MAX_SMI_LEN = 100  # You can adjust this based on your needs\n",
    "XD = np.array([label_smiles(str(smi), MAX_SMI_LEN, smiles_dict) for smi in smiles])\n",
    "labels = labels.values\n",
    "\n",
    "# Convert to categorical\n",
    "XD = to_categorical(XD, num_classes=71)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qmjyj91JeXsG"
   },
   "source": [
    "# phaze1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A2VgoLaMQN0X",
    "outputId": "d183f5ed-45d0-496a-8aa7-9347ac07ec00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_feature\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"model_feature\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_39 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m9,152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_40 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_41 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_23\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_23\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_23 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_92 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_40 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_93 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_41 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_94 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_95 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,961</span> (847.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m216,961\u001b[0m (847.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">215,425</span> (841.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m215,425\u001b[0m (841.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"interactionModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"interactionModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ model_feature (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">216,961</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_11     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ model_feature (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m58,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_23 (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │       \u001b[38;5;34m216,961\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_11     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">275,457</span> (1.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m275,457\u001b[0m (1.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">273,921</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m273,921\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature model definition \n",
    "XDinput = Input(shape=(100, 71), name='XDinput')\n",
    "encode_smiles = Conv1D(filters=64, kernel_size=2, activation='relu', padding='valid', strides=1)(XDinput)  # (99,64)\n",
    "encode_smiles = Conv1D(filters=64, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)  # (96,64)\n",
    "encode_smiles = Conv1D(filters=128, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)  # (93,128)\n",
    "model_feature = Model(inputs=XDinput, outputs=encode_smiles, name='model_feature')\n",
    "model_feature.summary()\n",
    "\n",
    "# Prediction model definition\n",
    "input_extracted_feature = Input(shape=(93, 128))\n",
    "FC1 = Dense(512, activation='relu')(input_extracted_feature)\n",
    "FC1 = BatchNormalization()(FC1)\n",
    "FC2 = Dropout(0.1)(FC1)\n",
    "FC3 = Dense(256, activation='relu')(FC2)\n",
    "FC3 = BatchNormalization()(FC3)\n",
    "FC4 = Dropout(0.1)(FC3)\n",
    "FC5 = Dense(64, activation='relu')(FC4)\n",
    "predictions = Dense(1, activation='linear')(FC5)\n",
    "model_pred = Model(inputs=input_extracted_feature, outputs=predictions)\n",
    "model_pred.summary()\n",
    "\n",
    "# Full model definition with added Pooling layer\n",
    "interaction_input = XDinput\n",
     "encoded_features = model_feature(interaction_input)  # Output: (None, 93, 128)\n",
     "predicted_output = model_pred(encoded_features)      # Output: (None, 93, 1)\n",
    "\n",
    "# Adding Pooling layer to reduce dimensionality\n",
     "pooled_output = GlobalAveragePooling1D()(predicted_output)  # Output: (None, 1)\n",
    "\n",
    "# Final model definition\n",
    "interactionModel = Model(inputs=interaction_input, outputs=pooled_output, name='interactionModel')\n",
    "interactionModel.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW4Dc7MfQWbn"
   },
   "source": [
    "### BiFormer Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "C5EAZalFQWpH"
   },
   "outputs": [],
   "source": [
    "# Define custom layers for 1D processing\n",
    "class TopkRouting(layers.Layer):\n",
    "    def __init__(self, qk_dim, topk=16, qk_scale=None, param_routing=False, diff_routing=False):\n",
    "        super().__init__()\n",
    "        self.topk = topk\n",
    "        self.qk_dim = qk_dim\n",
    "        self.scale = qk_scale if qk_scale is not None else qk_dim**-0.5\n",
    "        self.diff_routing = diff_routing\n",
    "        if param_routing:\n",
    "            self.emb = layers.Dense(qk_dim)\n",
    "        else:\n",
    "            self.emb = lambda x: x\n",
    "        self.routing_act = lambda x, axis: tf.nn.softmax(x, axis=axis)\n",
    "\n",
    "    def call(self, query, key, training=None):\n",
    "        if not self.diff_routing:\n",
    "            query = tf.stop_gradient(query)\n",
    "            key = tf.stop_gradient(key)\n",
    "        query_hat = self.emb(query)\n",
    "        key_hat = self.emb(key)\n",
    "\n",
    "        attn_logit = tf.einsum('bnc,bqc->bnq', query_hat*self.scale, key_hat)  # Adjusted for 1D\n",
    "        topk_attn_logit, topk_index = tf.math.top_k(attn_logit, k=self.topk, sorted=True)\n",
    "        r_weight = self.routing_act(topk_attn_logit, axis=-1)\n",
    "        return r_weight, topk_index\n",
    "\n",
    "def tf_gather_kv(kv, r_idx):\n",
    "    n = tf.shape(kv)[0]\n",
    "    p2 = tf.shape(kv)[1]\n",
    "    c_kv = tf.shape(kv)[2]\n",
    "    topk = tf.shape(r_idx)[2]\n",
    "\n",
    "    batch_idx = tf.reshape(tf.range(n), [n, 1, 1])\n",
    "    batch_idx = tf.tile(batch_idx, [1, p2, topk])\n",
    "    p2_idx = tf.reshape(tf.range(p2), [1, p2, 1])\n",
    "    p2_idx = tf.tile(p2_idx, [n, 1, topk])\n",
    "\n",
    "    gather_indices = tf.stack([batch_idx, p2_idx, r_idx], axis=-1)\n",
    "    gathered = tf.gather_nd(kv, gather_indices)\n",
    "    return gathered  # Shape: (n, p2, topk, c_kv)\n",
    "\n",
    "class KVGather(layers.Layer):\n",
    "    def __init__(self, mul_weight='none'):\n",
    "        super().__init__()\n",
    "        assert mul_weight in ['none', 'soft', 'hard']\n",
    "        self.mul_weight = mul_weight\n",
    "\n",
    "    def call(self, r_idx, r_weight, kv, training=None):\n",
    "        topk_kv = tf_gather_kv(kv, r_idx)\n",
    "        if self.mul_weight == 'soft':\n",
    "            r_weight_exp = tf.expand_dims(tf.expand_dims(r_weight, -1), -1)\n",
    "            topk_kv = topk_kv * r_weight_exp\n",
    "        return topk_kv\n",
    "\n",
    "class QKVLinear(layers.Layer):\n",
    "    def __init__(self, dim, qk_dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.qk_dim = qk_dim\n",
    "        self.qkv = layers.Dense(qk_dim + qk_dim + dim, use_bias=bias)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        qkv = self.qkv(x)\n",
    "        q, kv = tf.split(qkv, [self.qk_dim, self.qk_dim + self.dim], axis=-1)\n",
    "        return q, kv\n",
    "\n",
    "class BiLevelRoutingAttention(layers.Layer):\n",
    "    def __init__(self, dim, n_win=7, num_heads=8, qk_dim=None, qk_scale=None,\n",
    "                 kv_per_win=4, kv_downsample_ratio=4, kv_downsample_mode='identity',\n",
    "                 topk=4, param_attention=\"qkvo\", param_routing=False, diff_routing=False, soft_routing=False,\n",
    "                 side_dwconv=3, auto_pad=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_win = n_win\n",
    "        self.num_heads = num_heads\n",
    "        self.qk_dim = qk_dim if qk_dim is not None else dim\n",
    "        self.scale = qk_scale if qk_scale is not None else self.qk_dim**-0.5\n",
    "        self.topk = topk\n",
    "        self.param_routing = param_routing\n",
    "        self.diff_routing = diff_routing\n",
    "        self.soft_routing = soft_routing\n",
    "        self.auto_pad = auto_pad\n",
    "\n",
    "        # For 1D, we use Conv1D\n",
    "        if side_dwconv > 0:\n",
    "            self.lepe = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n",
    "        else:\n",
    "            self.lepe = lambda x: tf.zeros_like(x)\n",
    "\n",
    "        self.router = TopkRouting(qk_dim=self.qk_dim, topk=self.topk, qk_scale=self.scale,\n",
    "                                  param_routing=self.param_routing, diff_routing=self.diff_routing)\n",
    "\n",
    "        mul_weight = 'none'\n",
    "        if self.soft_routing:\n",
    "            mul_weight = 'soft'\n",
    "        self.kv_gather = KVGather(mul_weight=mul_weight)\n",
    "\n",
    "        if param_attention in ['qkvo', 'qkv']:\n",
    "            self.qkv = QKVLinear(self.dim, self.qk_dim)\n",
    "            if param_attention == 'qkvo':\n",
    "                self.wo = layers.Dense(self.dim)\n",
    "            else:\n",
    "                self.wo = lambda x: x\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported param_attention mode\")\n",
    "\n",
    "        self.attn_act = lambda x: tf.nn.softmax(x, axis=-1)\n",
    "        self.kv_down = lambda x: x  # identity for simplicity\n",
    "\n",
    "    def call(self, x, training=None, ret_attn_mask=False):\n",
    "        # Implementing full attention can be complex, so here we only keep the general structure\n",
    "        if ret_attn_mask:\n",
    "            return x, None, None, None\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class Attention(layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n",
    "        self.qkv = layers.Dense(dim*3, use_bias=qkv_bias)\n",
    "        self.attn_drop = layers.Dropout(attn_drop)\n",
    "        self.proj = layers.Dense(dim)\n",
    "        self.proj_drop = layers.Dropout(proj_drop)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        batch_size, seq_length, dim = tf.unstack(tf.shape(x))\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "\n",
    "        attn = tf.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn, training=training)\n",
    "        out = tf.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out, training=training)\n",
    "        return out\n",
    "\n",
    "class AttentionLePE(layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., side_dwconv=5):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n",
    "        self.qkv = layers.Dense(dim*3, use_bias=qkv_bias)\n",
    "        self.attn_drop = layers.Dropout(attn_drop)\n",
    "        self.proj = layers.Dense(dim)\n",
    "        self.proj_drop = layers.Dropout(proj_drop)\n",
    "        if side_dwconv > 0:\n",
    "            self.lepe = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n",
    "        else:\n",
    "            self.lepe = lambda x: tf.zeros_like(x)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        lepe_out = self.lepe(x)\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "\n",
    "        attn = tf.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn, training=training)\n",
    "        out = tf.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')\n",
    "        out = out + lepe_out\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out, training=training)\n",
    "        return out\n",
    "\n",
    "class PreNorm(layers.Layer):\n",
    "    def __init__(self, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.fn(self.norm(x), training=training)\n",
    "\n",
    "class MLP(layers.Layer):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = keras.Sequential([\n",
    "            layers.Dense(hidden_dim),\n",
    "            layers.Activation('gelu'),\n",
    "            layers.Dropout(rate=dropout),\n",
    "            layers.Dense(dim),\n",
    "            layers.Dropout(rate=dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.net(x, training=training)\n",
    "\n",
    "class DropPath(layers.Layer):\n",
    "    # Placeholder for DropPath, currently no-op\n",
    "    def __init__(self, drop_prob=0.):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if (not training) or self.drop_prob == 0.:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += tf.random.uniform(tf.shape(x), dtype=x.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        return tf.divide(x, keep_prob) * binary_tensor\n",
    "\n",
    "class Block(layers.Layer):\n",
    "    def __init__(self, dim, drop_path=0.1, layer_scale_init_value=-1,\n",
    "                 num_heads=8, n_win=7, qk_dim=128, qk_scale=None,\n",
    "                 kv_per_win=8, kv_downsample_ratio=1, kv_downsample_mode='identity',\n",
    "                 topk=8, param_attention=\"qkvo\", param_routing=True, diff_routing=False, soft_routing=True,\n",
    "                 mlp_ratio=4, mlp_dwconv=False, side_dwconv=5, before_attn_dwconv=3, pre_norm=True, auto_pad=True):\n",
    "        super().__init__()\n",
    "        qk_dim = qk_dim or dim\n",
    "\n",
    "        # For 1D, we use Conv1D\n",
    "        if before_attn_dwconv > 0:\n",
    "            self.pos_embed = layers.Conv1D(dim, kernel_size=before_attn_dwconv, padding='same', activation='relu')\n",
    "        else:\n",
    "            self.pos_embed = lambda x: tf.zeros_like(x)\n",
    "\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        if topk > 0:\n",
    "            self.attn = BiLevelRoutingAttention(dim=dim, num_heads=num_heads, n_win=n_win, qk_dim=qk_dim,\n",
    "                                               qk_scale=qk_scale, kv_per_win=kv_per_win, kv_downsample_ratio=kv_downsample_ratio,\n",
    "                                               kv_downsample_mode=kv_downsample_mode, topk=topk, param_attention=param_attention,\n",
    "                                               param_routing=param_routing, diff_routing=diff_routing, soft_routing=soft_routing,\n",
    "                                               side_dwconv=side_dwconv, auto_pad=auto_pad)\n",
    "        elif topk == -1:\n",
    "            self.attn = Attention(dim=dim, num_heads=num_heads, qk_scale=qk_scale)\n",
    "        elif topk == -2:\n",
    "            self.attn = AttentionLePE(dim=dim, num_heads=num_heads, qk_scale=qk_scale, side_dwconv=side_dwconv)\n",
    "        elif topk == 0:\n",
    "            # Pseudo attention\n",
    "            self.attn = keras.Sequential([\n",
    "                layers.Dense(dim),\n",
    "                layers.Conv1D(dim, kernel_size=5, padding='same', activation='relu'),\n",
    "                layers.Dense(dim)\n",
    "            ])\n",
    "\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        mlp_hidden_dim = int(mlp_ratio * dim)\n",
    "        mlp_layers = [layers.Dense(mlp_hidden_dim)]\n",
    "        if mlp_dwconv:\n",
    "            mlp_layers.append(layers.Conv1D(mlp_hidden_dim, kernel_size=3, padding='same', activation='relu'))\n",
    "        mlp_layers.append(layers.Activation('gelu'))\n",
    "        mlp_layers.append(layers.Dense(dim))\n",
    "        mlp_layers.insert(1, layers.Dropout(0.2)) # Add dropout after first Dense\n",
    "        mlp_layers.append(layers.Dropout(0.2))\n",
    "        self.mlp = keras.Sequential(mlp_layers)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else layers.Lambda(lambda x: x)\n",
    "\n",
    "        if layer_scale_init_value > 0:\n",
    "            self.use_layer_scale = True\n",
    "            self.gamma1 = self.add_weight(shape=(dim,),\n",
    "                                          initializer=tf.keras.initializers.Constant(layer_scale_init_value),\n",
    "                                          trainable=True)\n",
    "            self.gamma2 = self.add_weight(shape=(dim,),\n",
    "                                          initializer=tf.keras.initializers.Constant(layer_scale_init_value),\n",
    "                                          trainable=True)\n",
    "        else:\n",
    "            self.use_layer_scale = False\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        x = x + self.pos_embed(x)  # Add positional embedding\n",
    "\n",
    "        if self.pre_norm:\n",
    "            if self.use_layer_scale:\n",
    "                x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x), training=training), training=training)\n",
    "                x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x), training=training), training=training)\n",
    "            else:\n",
    "                x = x + self.drop_path(self.attn(self.norm1(x), training=training), training=training)\n",
    "                x = x + self.drop_path(self.mlp(self.norm2(x), training=training), training=training)\n",
    "        else:\n",
    "            if self.use_layer_scale:\n",
    "                tmp = x + self.drop_path(self.gamma1 * self.attn(x, training=training), training=training)\n",
    "                x = self.norm1(tmp)\n",
    "                tmp = x + self.drop_path(self.gamma2 * self.mlp(x, training=training), training=training)\n",
    "                x = self.norm2(tmp)\n",
    "            else:\n",
    "                tmp = x + self.drop_path(self.attn(x, training=training), training=training)\n",
    "                x = self.norm1(tmp)\n",
    "                tmp = x + self.drop_path(self.mlp(x, training=training), training=training)\n",
    "                x = self.norm2(tmp)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k41fqbmrQxbV"
   },
   "source": [
    "# phaze2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "qFxfd5e4QdV3",
    "outputId": "3ffefc83-bb87-45ff-cb54-e4535be8d055"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_12', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_13', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_26\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_26\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ input_layer_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ input_layer_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ difference (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ difference[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_12 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ input_layer_24[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_13 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ input_layer_24[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_12 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_13 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ difference (\u001b[38;5;33mLambda\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ difference[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">297,216</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m297,216\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">297,216</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m297,216\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameter settings for the block\n",
    "dim = 128   \n",
    "num_heads = 8\n",
    "mlp_dim = 128 * 3  \n",
    "depth = 2\n",
    "dim_head = 48\n",
    "\n",
    "# Create Phase 2 model\n",
    "input_phaz2 = Input(shape=(93, 128))  \n",
    "\n",
    "processed_input = input_phaz2\n",
    "\n",
    "# Define transformer blocks\n",
    "transformer_block1 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "\n",
    "# Apply the first transformer block\n",
    "transformer_output1 = transformer_block1(processed_input)\n",
    "# Compute the norm of the output along the last axis (preserving dimensions)\n",
    "transformer_output1 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1)\n",
    "\n",
    "# Apply the second transformer block\n",
    "transformer_output2 = transformer_block2(processed_input)\n",
    "# Compute the norm of the output along the last axis (preserving dimensions)\n",
    "transformer_output2 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2)\n",
    "\n",
    "# Combine the outputs of both transformer blocks along the last axis\n",
    "combined_outputs = Concatenate(axis=-1)([transformer_output1, transformer_output2])\n",
    "\n",
    "# Apply global average pooling to reduce the output to (batch_size, 2)\n",
    "pooled_outputs = GlobalAveragePooling1D()(combined_outputs)\n",
    "\n",
    "# Compute the difference between the two pooled outputs and reshape to (batch_size, 1)\n",
    "difference = Lambda(lambda x: tf.expand_dims(x[:, 0] - x[:, 1], axis=-1), name='difference')(pooled_outputs)\n",
    "\n",
    "# Apply sigmoid activation to normalize the output to the range [0, 1]\n",
    "condition_2 = Activation('sigmoid', name='activation_11')(difference)\n",
    "\n",
    "# Freeze the `model_feature` layers to prevent them from being trained\n",
    "model_feature.trainable = False\n",
    "\n",
    "# Define the complete model\n",
    "model_phaz2 = Model(inputs=input_phaz2, outputs=condition_2)\n",
    "model_phaz2.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcH6Y5x1e5OS"
   },
   "source": [
    "# phaze3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F6DTjT17Q1W5",
    "outputId": "6bcfc3f8-eeee-418e-cddd-31ff826c0ff5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_14', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_15', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer_27 True\n",
      "model_feature False\n",
      "block_14 False\n",
      "block_15 False\n",
      "lambda_14 True\n",
      "lambda_15 True\n",
      "concatenate_7 True\n",
      "global_average_pooling1d_13 True\n",
      "dense_116 True\n",
      "batch_normalization_18 True\n",
      "dropout_50 True\n",
      "dense_117 True\n",
      "batch_normalization_19 True\n",
      "dropout_51 True\n",
      "dense_118 True\n",
      "dense_119 True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_29\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_29\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_27      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ model_feature       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> │ input_layer_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_116 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_116[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_50          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_117 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_117[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_51          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_118 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ dropout_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_119 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │ dense_118[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_27      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ model_feature       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m58,496\u001b[0m │ input_layer_27[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_14 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ model_feature[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_15 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ model_feature[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_14 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_15 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_116 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m1,536\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ dense_116[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_50          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_117 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_117[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_51          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_118 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m16,448\u001b[0m │ dropout_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_119 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m130\u001b[0m │ dense_118[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">508,226</span> (1.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m508,226\u001b[0m (1.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,978</span> (589.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,978\u001b[0m (589.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">357,248</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m357,248\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define new transformer blocks\n",
    "new_transformer_block = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "new_transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "\n",
    "# Input for Phase 3\n",
    "XDinput_phaz3 = Input(shape=(100, 71))\n",
    "# Pass input through the feature extraction model\n",
    "model_feature_output = model_feature(XDinput_phaz3)  # Output shape: (93, 192)\n",
    "\n",
    "# Transformer blocks processing\n",
    "transformer_output1_phaz3 = new_transformer_block(model_feature_output)  # Output shape: (93, 192)\n",
    "# Compute the norm along the last axis, retaining dimensions\n",
    "transformer_output1_phaz3 = layers.Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1_phaz3)  # Shape: (93, 1)\n",
    "\n",
    "transformer_output2_phaz3 = new_transformer_block2(model_feature_output)  # Output shape: (93, 192)\n",
    "# Compute the norm along the last axis, retaining dimensions\n",
    "transformer_output2_phaz3 = layers.Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2_phaz3)  # Shape: (93, 1)\n",
    "\n",
    "# Combine the outputs of both transformer blocks\n",
    "concatenated_phaz3 = Concatenate(axis=-1)([transformer_output1_phaz3, transformer_output2_phaz3])  # Shape: (93, 2)\n",
    "\n",
    "# Freeze the weights of feature extractor and transformer blocks\n",
    "model_feature.trainable = False\n",
    "new_transformer_block.trainable = False\n",
    "new_transformer_block2.trainable = False\n",
    "\n",
    "# Apply global average pooling to reduce the dimensions to (batch_size, 2)\n",
    "pooled_phaz3 = GlobalAveragePooling1D()(concatenated_phaz3)  # Shape: (None, 2)\n",
    "\n",
    "# Fully connected layers\n",
    "FC1_phaz3 = Dense(512, activation='relu')(pooled_phaz3)\n",
    "FC1_phaz3 = BatchNormalization()(FC1_phaz3)\n",
    "FC2_phaz3 = Dropout(0.1)(FC1_phaz3)  # Increased dropout rate\n",
    "FC3_phaz3 = Dense(256, activation='relu')(FC2_phaz3)\n",
    "FC3_phaz3 = BatchNormalization()(FC3_phaz3)\n",
    "FC4_phaz3 = Dropout(0.1)(FC3_phaz3)  # Increased dropout rate\n",
    "FC5_phaz3 = Dense(64, activation='relu')(FC4_phaz3)\n",
    "\n",
     "# Final prediction layer for regression\n",
     "predictions_phaz3 = Dense(1, activation='linear')(FC5_phaz3)\n",
    "\n",
    "# Define the complete model for Phase 3\n",
    "model_phaz3 = Model(inputs=XDinput_phaz3, outputs=predictions_phaz3)\n",
    "\n",
    "# Print the name and trainable status of each layer\n",
    "for layer in model_phaz3.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "\n",
    "# Model summary\n",
    "model_phaz3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "jF60_7QXQ4lB"
   },
   "outputs": [],
   "source": [
    "# Metrics for regression tasks\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"TensorFlow implementation of the coefficient of determination.\"\"\"\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + tf.keras.backend.epsilon())\n",
    "\n",
    "METRICS_REGRESSION = [\n",
    "    metrics.MeanAbsoluteError(name='mae'),\n",
    "    metrics.MeanSquaredError(name='mse'),\n",
    "    r2_score,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBNS2iz6f9xP"
   },
   "source": [
    "# Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcS3ex1TgCRK",
    "outputId": "7a3df7f6-88ae-459e-8e9d-fde383fd7d9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:23:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3374\n",
      "Number of test samples: 422\n",
      "Epoch 1/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - loss: 3.4608 - mae: 1.5009 - mse: 3.4608 - r2_score: -705.7711\n",
      "Epoch 2/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 837ms/step - loss: 1.2074 - mae: 0.8382 - mse: 1.2074 - r2_score: -262.6396\n",
      "Epoch 3/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 982ms/step - loss: 1.1323 - mae: 0.8261 - mse: 1.1323 - r2_score: -280.7585\n",
      "Epoch 4/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 973ms/step - loss: 1.0957 - mae: 0.8013 - mse: 1.0957 - r2_score: -271.5926\n",
      "Epoch 5/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 953ms/step - loss: 1.0495 - mae: 0.7880 - mse: 1.0495 - r2_score: -268.1882\n",
      "Epoch 6/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 910ms/step - loss: 1.0710 - mae: 0.7805 - mse: 1.0710 - r2_score: -276.0538\n",
      "Epoch 7/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - loss: 1.0494 - mae: 0.7858 - mse: 1.0494 - r2_score: -274.2872\n",
      "Epoch 8/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 652ms/step - loss: 1.0293 - mae: 0.7926 - mse: 1.0293 - r2_score: -275.0028\n",
      "Epoch 9/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 622ms/step - loss: 1.0557 - mae: 0.7928 - mse: 1.0557 - r2_score: -273.6519\n",
      "Epoch 10/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 629ms/step - loss: 1.0437 - mae: 0.7724 - mse: 1.0437 - r2_score: -276.3913\n",
      "Epoch 11/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 817ms/step - loss: 1.0230 - mae: 0.7693 - mse: 1.0230 - r2_score: -277.3341\n",
      "Epoch 12/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 875ms/step - loss: 0.9898 - mae: 0.7585 - mse: 0.9898 - r2_score: -281.9400\n",
      "Epoch 13/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 672ms/step - loss: 0.9869 - mae: 0.7674 - mse: 0.9869 - r2_score: -278.9803\n",
      "Epoch 14/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 864ms/step - loss: 0.9827 - mae: 0.7634 - mse: 0.9827 - r2_score: -278.7934\n",
      "Epoch 15/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 854ms/step - loss: 0.9853 - mae: 0.7628 - mse: 0.9853 - r2_score: -278.8678\n",
      "Epoch 16/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 883ms/step - loss: 0.9432 - mae: 0.7499 - mse: 0.9432 - r2_score: -283.1440\n",
      "Epoch 17/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 818ms/step - loss: 0.9885 - mae: 0.7561 - mse: 0.9885 - r2_score: -283.8681\n",
      "Epoch 18/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 708ms/step - loss: 0.9693 - mae: 0.7604 - mse: 0.9693 - r2_score: -282.4840\n",
      "Epoch 19/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 882ms/step - loss: 0.9485 - mae: 0.7463 - mse: 0.9485 - r2_score: -285.3940\n",
      "Epoch 20/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 739ms/step - loss: 0.9696 - mae: 0.7636 - mse: 0.9696 - r2_score: -285.4673\n",
      "Epoch 21/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 822ms/step - loss: 1.0258 - mae: 0.7747 - mse: 1.0258 - r2_score: -288.0397\n",
      "Epoch 22/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 782ms/step - loss: 0.9641 - mae: 0.7547 - mse: 0.9641 - r2_score: -289.0865\n",
      "Epoch 23/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 875ms/step - loss: 0.8932 - mae: 0.7306 - mse: 0.8932 - r2_score: -289.0086\n",
      "Epoch 24/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 879ms/step - loss: 0.9205 - mae: 0.7343 - mse: 0.9205 - r2_score: -292.1608\n",
      "Epoch 25/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 756ms/step - loss: 0.8991 - mae: 0.7320 - mse: 0.8991 - r2_score: -291.1677\n",
      "Epoch 26/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 869ms/step - loss: 0.9597 - mae: 0.7560 - mse: 0.9597 - r2_score: -289.6737\n",
      "Epoch 27/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 875ms/step - loss: 0.9298 - mae: 0.7433 - mse: 0.9298 - r2_score: -287.9765\n",
      "Epoch 28/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 898ms/step - loss: 0.9183 - mae: 0.7440 - mse: 0.9183 - r2_score: -296.2313\n",
      "Epoch 29/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 881ms/step - loss: 0.9105 - mae: 0.7325 - mse: 0.9105 - r2_score: -293.6644\n",
      "Epoch 30/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 890ms/step - loss: 0.8976 - mae: 0.7266 - mse: 0.8976 - r2_score: -293.1479\n",
      "Epoch 31/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 874ms/step - loss: 0.9244 - mae: 0.7371 - mse: 0.9244 - r2_score: -293.6056\n",
      "Epoch 32/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 866ms/step - loss: 0.8875 - mae: 0.7262 - mse: 0.8875 - r2_score: -295.6156\n",
      "Epoch 33/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 918ms/step - loss: 0.8917 - mae: 0.7308 - mse: 0.8917 - r2_score: -296.2150\n",
      "Epoch 34/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 847ms/step - loss: 0.9123 - mae: 0.7330 - mse: 0.9123 - r2_score: -298.9361\n",
      "Epoch 35/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 703ms/step - loss: 0.8803 - mae: 0.7237 - mse: 0.8803 - r2_score: -298.7829\n",
      "Epoch 36/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 388ms/step - loss: 0.8586 - mae: 0.7182 - mse: 0.8586 - r2_score: -301.7372\n",
      "Epoch 37/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 703ms/step - loss: 0.8633 - mae: 0.7175 - mse: 0.8633 - r2_score: -301.3952\n",
      "Epoch 38/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 744ms/step - loss: 0.8297 - mae: 0.7015 - mse: 0.8297 - r2_score: -301.7308\n",
      "Epoch 39/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 860ms/step - loss: 0.8984 - mae: 0.7244 - mse: 0.8984 - r2_score: -297.5887\n",
      "Epoch 40/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 810ms/step - loss: 0.8870 - mae: 0.7290 - mse: 0.8870 - r2_score: -298.3194\n",
      "Epoch 41/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 687ms/step - loss: 0.8543 - mae: 0.7131 - mse: 0.8543 - r2_score: -298.4524\n",
      "Epoch 42/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 870ms/step - loss: 0.9196 - mae: 0.7344 - mse: 0.9196 - r2_score: -303.2710\n",
      "Epoch 43/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 864ms/step - loss: 0.8744 - mae: 0.7278 - mse: 0.8744 - r2_score: -300.1776\n",
      "Epoch 44/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 877ms/step - loss: 0.8794 - mae: 0.7147 - mse: 0.8794 - r2_score: -304.0407\n",
      "Epoch 45/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 889ms/step - loss: 0.8473 - mae: 0.7069 - mse: 0.8473 - r2_score: -303.6453\n",
      "Epoch 46/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 894ms/step - loss: 0.8279 - mae: 0.7026 - mse: 0.8279 - r2_score: -309.8816\n",
      "Epoch 47/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 886ms/step - loss: 0.8314 - mae: 0.7088 - mse: 0.8314 - r2_score: -305.6054\n",
      "Epoch 48/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 862ms/step - loss: 0.8092 - mae: 0.6920 - mse: 0.8092 - r2_score: -306.7743\n",
      "Epoch 49/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 857ms/step - loss: 0.8197 - mae: 0.6958 - mse: 0.8197 - r2_score: -305.9088\n",
      "Epoch 50/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 820ms/step - loss: 0.8230 - mae: 0.6994 - mse: 0.8230 - r2_score: -306.3448\n",
      "Epoch 51/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 855ms/step - loss: 0.8051 - mae: 0.6927 - mse: 0.8051 - r2_score: -306.6085\n",
      "Epoch 52/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 863ms/step - loss: 0.8111 - mae: 0.6954 - mse: 0.8111 - r2_score: -310.2076\n",
      "Epoch 53/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 849ms/step - loss: 0.8049 - mae: 0.6953 - mse: 0.8049 - r2_score: -312.2764\n",
      "Epoch 54/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 863ms/step - loss: 0.8376 - mae: 0.7015 - mse: 0.8376 - r2_score: -311.6663\n",
      "Epoch 55/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 634ms/step - loss: 0.7643 - mae: 0.6793 - mse: 0.7643 - r2_score: -312.3587\n",
      "Epoch 56/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 900ms/step - loss: 0.8102 - mae: 0.6987 - mse: 0.8102 - r2_score: -315.4694\n",
      "Epoch 57/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 907ms/step - loss: 0.8090 - mae: 0.6983 - mse: 0.8090 - r2_score: -310.7832\n",
      "Epoch 58/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 875ms/step - loss: 0.8249 - mae: 0.7066 - mse: 0.8249 - r2_score: -320.2496\n",
      "Epoch 59/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 853ms/step - loss: 0.8111 - mae: 0.6950 - mse: 0.8111 - r2_score: -313.2469\n",
      "Epoch 60/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 838ms/step - loss: 0.7968 - mae: 0.6849 - mse: 0.7968 - r2_score: -310.4001\n",
      "Epoch 61/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 827ms/step - loss: 0.7840 - mae: 0.6826 - mse: 0.7840 - r2_score: -319.1846\n",
      "Epoch 62/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 871ms/step - loss: 0.7949 - mae: 0.6884 - mse: 0.7949 - r2_score: -319.3557\n",
      "Epoch 63/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 839ms/step - loss: 0.7815 - mae: 0.6733 - mse: 0.7815 - r2_score: -320.7764\n",
      "Epoch 64/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 763ms/step - loss: 0.7815 - mae: 0.6739 - mse: 0.7815 - r2_score: -316.8315\n",
      "Epoch 65/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 726ms/step - loss: 0.7234 - mae: 0.6584 - mse: 0.7234 - r2_score: -323.0613\n",
      "Epoch 66/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 875ms/step - loss: 0.7793 - mae: 0.6832 - mse: 0.7793 - r2_score: -319.6875\n",
      "Epoch 67/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 654ms/step - loss: 0.7838 - mae: 0.6787 - mse: 0.7838 - r2_score: -324.0176\n",
      "Epoch 68/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 871ms/step - loss: 0.8215 - mae: 0.6973 - mse: 0.8215 - r2_score: -323.1091\n",
      "Epoch 69/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 898ms/step - loss: 0.8004 - mae: 0.6837 - mse: 0.8004 - r2_score: -327.9405\n",
      "Epoch 70/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 901ms/step - loss: 0.7833 - mae: 0.6809 - mse: 0.7833 - r2_score: -320.5465\n",
      "Epoch 71/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 641ms/step - loss: 0.7636 - mae: 0.6652 - mse: 0.7636 - r2_score: -320.9546\n",
      "Epoch 72/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 887ms/step - loss: 0.7373 - mae: 0.6606 - mse: 0.7373 - r2_score: -326.7551\n",
      "Epoch 73/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 900ms/step - loss: 0.7741 - mae: 0.6729 - mse: 0.7741 - r2_score: -331.7924\n",
      "Epoch 74/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 650ms/step - loss: 0.7141 - mae: 0.6469 - mse: 0.7141 - r2_score: -324.7583\n",
      "Epoch 75/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 881ms/step - loss: 0.7518 - mae: 0.6685 - mse: 0.7518 - r2_score: -325.0063\n",
      "Epoch 76/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 840ms/step - loss: 0.7291 - mae: 0.6581 - mse: 0.7291 - r2_score: -325.1157\n",
      "Epoch 77/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 804ms/step - loss: 0.7431 - mae: 0.6671 - mse: 0.7431 - r2_score: -329.0079\n",
      "Epoch 78/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 845ms/step - loss: 0.7374 - mae: 0.6629 - mse: 0.7374 - r2_score: -329.3301\n",
      "Epoch 79/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 889ms/step - loss: 0.7756 - mae: 0.6762 - mse: 0.7756 - r2_score: -330.8187\n",
      "Epoch 80/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 907ms/step - loss: 0.7286 - mae: 0.6570 - mse: 0.7286 - r2_score: -331.3231\n",
      "Epoch 81/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 721ms/step - loss: 0.7461 - mae: 0.6680 - mse: 0.7461 - r2_score: -323.0836\n",
      "Epoch 82/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 873ms/step - loss: 0.7096 - mae: 0.6466 - mse: 0.7096 - r2_score: -335.9869\n",
      "Epoch 83/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 851ms/step - loss: 0.7524 - mae: 0.6706 - mse: 0.7524 - r2_score: -332.4932\n",
      "Epoch 84/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 811ms/step - loss: 0.7120 - mae: 0.6461 - mse: 0.7120 - r2_score: -331.2153\n",
      "Epoch 85/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 841ms/step - loss: 0.7087 - mae: 0.6522 - mse: 0.7087 - r2_score: -336.3782\n",
      "Epoch 86/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 871ms/step - loss: 0.7326 - mae: 0.6529 - mse: 0.7326 - r2_score: -327.2464\n",
      "Epoch 87/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 876ms/step - loss: 0.7505 - mae: 0.6667 - mse: 0.7505 - r2_score: -340.0821\n",
      "Epoch 88/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 867ms/step - loss: 0.7403 - mae: 0.6661 - mse: 0.7403 - r2_score: -332.6702\n",
      "Epoch 89/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 651ms/step - loss: 0.7617 - mae: 0.6678 - mse: 0.7617 - r2_score: -329.3371\n",
      "Epoch 90/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 909ms/step - loss: 0.7571 - mae: 0.6623 - mse: 0.7571 - r2_score: -337.5638\n",
      "Epoch 91/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 614ms/step - loss: 0.7088 - mae: 0.6469 - mse: 0.7088 - r2_score: -336.9849\n",
      "Epoch 92/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 901ms/step - loss: 0.7461 - mae: 0.6681 - mse: 0.7461 - r2_score: -333.2249\n",
      "Epoch 93/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 923ms/step - loss: 0.7200 - mae: 0.6443 - mse: 0.7200 - r2_score: -338.6558\n",
      "Epoch 94/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 701ms/step - loss: 0.7057 - mae: 0.6465 - mse: 0.7057 - r2_score: -335.4736\n",
      "Epoch 95/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 823ms/step - loss: 0.6881 - mae: 0.6361 - mse: 0.6881 - r2_score: -339.4605\n",
      "Epoch 96/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 803ms/step - loss: 0.6888 - mae: 0.6423 - mse: 0.6888 - r2_score: -341.6500\n",
      "Epoch 97/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 591ms/step - loss: 0.6975 - mae: 0.6404 - mse: 0.6975 - r2_score: -333.6667\n",
      "Epoch 98/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 605ms/step - loss: 0.7160 - mae: 0.6436 - mse: 0.7160 - r2_score: -336.0848\n",
      "Epoch 99/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 596ms/step - loss: 0.6779 - mae: 0.6306 - mse: 0.6779 - r2_score: -339.9638\n",
      "Epoch 100/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 605ms/step - loss: 0.6814 - mae: 0.6307 - mse: 0.6814 - r2_score: -344.2087\n",
      "Epoch 101/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 607ms/step - loss: 0.6753 - mae: 0.6400 - mse: 0.6753 - r2_score: -346.7786\n",
      "Epoch 102/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 599ms/step - loss: 0.7029 - mae: 0.6481 - mse: 0.7029 - r2_score: -339.9530\n",
      "Epoch 103/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 608ms/step - loss: 0.7072 - mae: 0.6450 - mse: 0.7072 - r2_score: -340.1428\n",
      "Epoch 104/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 535ms/step - loss: 0.6827 - mae: 0.6375 - mse: 0.6827 - r2_score: -343.2745\n",
      "Epoch 105/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 599ms/step - loss: 0.6600 - mae: 0.6302 - mse: 0.6600 - r2_score: -343.4145\n",
      "Epoch 106/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 599ms/step - loss: 0.6412 - mae: 0.6201 - mse: 0.6412 - r2_score: -352.1045\n",
      "Epoch 107/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 588ms/step - loss: 0.7319 - mae: 0.6555 - mse: 0.7319 - r2_score: -341.3773\n",
      "Epoch 108/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 593ms/step - loss: 0.6531 - mae: 0.6206 - mse: 0.6531 - r2_score: -344.8997\n",
      "Epoch 109/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 609ms/step - loss: 0.6842 - mae: 0.6431 - mse: 0.6842 - r2_score: -356.9337\n",
      "Epoch 110/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 448ms/step - loss: 0.6637 - mae: 0.6355 - mse: 0.6637 - r2_score: -345.7135\n",
      "Epoch 111/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 586ms/step - loss: 0.7037 - mae: 0.6381 - mse: 0.7037 - r2_score: -341.5329\n",
      "Epoch 112/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 583ms/step - loss: 0.6877 - mae: 0.6350 - mse: 0.6877 - r2_score: -342.9548\n",
      "Epoch 113/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.6635 - mae: 0.6278 - mse: 0.6635 - r2_score: -355.3843\n",
      "Epoch 114/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 610ms/step - loss: 0.6386 - mae: 0.6142 - mse: 0.6386 - r2_score: -344.9266\n",
      "Epoch 115/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 598ms/step - loss: 0.6798 - mae: 0.6292 - mse: 0.6798 - r2_score: -344.0282\n",
      "Epoch 116/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 606ms/step - loss: 0.6337 - mae: 0.6165 - mse: 0.6337 - r2_score: -351.5584\n",
      "Epoch 117/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 594ms/step - loss: 0.6440 - mae: 0.6115 - mse: 0.6440 - r2_score: -353.0404\n",
      "Epoch 118/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 608ms/step - loss: 0.6820 - mae: 0.6350 - mse: 0.6820 - r2_score: -349.0828\n",
      "Epoch 119/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.6798 - mae: 0.6389 - mse: 0.6798 - r2_score: -347.9189\n",
      "Epoch 120/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 598ms/step - loss: 0.6686 - mae: 0.6254 - mse: 0.6686 - r2_score: -356.7320\n",
      "Epoch 121/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.6836 - mae: 0.6406 - mse: 0.6836 - r2_score: -355.2161\n",
      "Epoch 122/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 613ms/step - loss: 0.6613 - mae: 0.6178 - mse: 0.6613 - r2_score: -353.4497\n",
      "Epoch 123/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 613ms/step - loss: 0.7015 - mae: 0.6363 - mse: 0.7015 - r2_score: -346.1997\n",
      "Epoch 124/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 594ms/step - loss: 0.6275 - mae: 0.6086 - mse: 0.6275 - r2_score: -344.1858\n",
      "Epoch 125/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 579ms/step - loss: 0.6556 - mae: 0.6187 - mse: 0.6556 - r2_score: -346.5774\n",
      "Epoch 126/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 602ms/step - loss: 0.6332 - mae: 0.6143 - mse: 0.6332 - r2_score: -351.7768\n",
      "Epoch 127/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 600ms/step - loss: 0.6470 - mae: 0.6161 - mse: 0.6470 - r2_score: -351.1270\n",
      "Epoch 128/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 605ms/step - loss: 0.6083 - mae: 0.6032 - mse: 0.6083 - r2_score: -354.6038\n",
      "Epoch 129/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 613ms/step - loss: 0.6639 - mae: 0.6222 - mse: 0.6639 - r2_score: -347.6808\n",
      "Epoch 130/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 591ms/step - loss: 0.6222 - mae: 0.6076 - mse: 0.6222 - r2_score: -365.2309\n",
      "Epoch 131/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 602ms/step - loss: 0.6514 - mae: 0.6193 - mse: 0.6514 - r2_score: -351.4367\n",
      "Epoch 132/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - loss: 0.6636 - mae: 0.6224 - mse: 0.6636 - r2_score: -358.3438\n",
      "Epoch 133/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 602ms/step - loss: 0.6339 - mae: 0.6076 - mse: 0.6339 - r2_score: -360.4915\n",
      "Epoch 134/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.6131 - mae: 0.6069 - mse: 0.6131 - r2_score: -354.4668\n",
      "Epoch 135/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 582ms/step - loss: 0.6153 - mae: 0.6011 - mse: 0.6153 - r2_score: -359.4674\n",
      "Epoch 136/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 587ms/step - loss: 0.6205 - mae: 0.6120 - mse: 0.6205 - r2_score: -354.5870\n",
      "Epoch 137/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 517ms/step - loss: 0.6268 - mae: 0.6048 - mse: 0.6268 - r2_score: -356.1596\n",
      "Epoch 138/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 606ms/step - loss: 0.6470 - mae: 0.6167 - mse: 0.6470 - r2_score: -348.8973\n",
      "Epoch 139/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 604ms/step - loss: 0.6534 - mae: 0.6084 - mse: 0.6534 - r2_score: -358.1210\n",
      "Epoch 140/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 555ms/step - loss: 0.6258 - mae: 0.6091 - mse: 0.6258 - r2_score: -356.7788\n",
      "Epoch 141/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 595ms/step - loss: 0.6418 - mae: 0.6142 - mse: 0.6418 - r2_score: -352.4162\n",
      "Epoch 142/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 599ms/step - loss: 0.6546 - mae: 0.6201 - mse: 0.6546 - r2_score: -363.2264\n",
      "Epoch 143/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 601ms/step - loss: 0.5999 - mae: 0.5935 - mse: 0.5999 - r2_score: -361.3524\n",
      "Epoch 144/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 594ms/step - loss: 0.6024 - mae: 0.5961 - mse: 0.6024 - r2_score: -358.0506\n",
      "Epoch 145/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 594ms/step - loss: 0.6313 - mae: 0.6091 - mse: 0.6313 - r2_score: -359.3916\n",
      "Epoch 146/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 591ms/step - loss: 0.6114 - mae: 0.5971 - mse: 0.6114 - r2_score: -353.6768\n",
      "Epoch 147/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 585ms/step - loss: 0.6337 - mae: 0.5990 - mse: 0.6337 - r2_score: -356.4383\n",
      "Epoch 148/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 598ms/step - loss: 0.5951 - mae: 0.5895 - mse: 0.5951 - r2_score: -361.3705\n",
      "Epoch 149/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 599ms/step - loss: 0.5761 - mae: 0.5819 - mse: 0.5761 - r2_score: -367.0920\n",
      "Epoch 150/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 626ms/step - loss: 0.6078 - mae: 0.5991 - mse: 0.6078 - r2_score: -365.2068\n",
      "Epoch 151/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 599ms/step - loss: 0.5786 - mae: 0.5884 - mse: 0.5786 - r2_score: -369.7052\n",
      "Epoch 152/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - loss: 0.6125 - mae: 0.5969 - mse: 0.6125 - r2_score: -370.2485\n",
      "Epoch 153/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 592ms/step - loss: 0.6274 - mae: 0.6142 - mse: 0.6274 - r2_score: -366.3486\n",
      "Epoch 154/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 624ms/step - loss: 0.5948 - mae: 0.5930 - mse: 0.5948 - r2_score: -363.1350\n",
      "Epoch 155/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 606ms/step - loss: 0.5793 - mae: 0.5810 - mse: 0.5793 - r2_score: -365.6145\n",
      "Epoch 156/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 606ms/step - loss: 0.5815 - mae: 0.5840 - mse: 0.5815 - r2_score: -356.7524\n",
      "Epoch 157/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 635ms/step - loss: 0.5970 - mae: 0.5923 - mse: 0.5970 - r2_score: -356.7002\n",
      "Epoch 158/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 707ms/step - loss: 0.5901 - mae: 0.5866 - mse: 0.5901 - r2_score: -364.8341\n",
      "Epoch 159/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 685ms/step - loss: 0.5764 - mae: 0.5816 - mse: 0.5764 - r2_score: -365.5255\n",
      "Epoch 160/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 642ms/step - loss: 0.6095 - mae: 0.6089 - mse: 0.6095 - r2_score: -374.7166\n",
      "Epoch 161/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 627ms/step - loss: 0.6122 - mae: 0.5989 - mse: 0.6122 - r2_score: -368.7620\n",
      "Epoch 162/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 613ms/step - loss: 0.5710 - mae: 0.5775 - mse: 0.5710 - r2_score: -364.6176\n",
      "Epoch 163/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 617ms/step - loss: 0.5844 - mae: 0.5877 - mse: 0.5844 - r2_score: -362.5828\n",
      "Epoch 164/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 630ms/step - loss: 0.5874 - mae: 0.5876 - mse: 0.5874 - r2_score: -369.0173\n",
      "Epoch 165/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 541ms/step - loss: 0.6042 - mae: 0.5861 - mse: 0.6042 - r2_score: -365.3918\n",
      "Epoch 166/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.5892 - mae: 0.5939 - mse: 0.5892 - r2_score: -359.9664\n",
      "Epoch 167/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 595ms/step - loss: 0.5874 - mae: 0.5884 - mse: 0.5874 - r2_score: -363.2801\n",
      "Epoch 168/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 618ms/step - loss: 0.6115 - mae: 0.6001 - mse: 0.6115 - r2_score: -361.5392\n",
      "Epoch 169/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 603ms/step - loss: 0.5940 - mae: 0.5816 - mse: 0.5940 - r2_score: -364.8152\n",
      "Epoch 170/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 614ms/step - loss: 0.5837 - mae: 0.5868 - mse: 0.5837 - r2_score: -366.6300\n",
      "Epoch 171/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 624ms/step - loss: 0.5802 - mae: 0.5895 - mse: 0.5802 - r2_score: -366.1683\n",
      "Epoch 172/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 598ms/step - loss: 0.5745 - mae: 0.5715 - mse: 0.5745 - r2_score: -371.6134\n",
      "Epoch 173/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 601ms/step - loss: 0.5833 - mae: 0.5852 - mse: 0.5833 - r2_score: -366.6632\n",
      "Epoch 174/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 525ms/step - loss: 0.5674 - mae: 0.5801 - mse: 0.5674 - r2_score: -374.3982\n",
      "Epoch 175/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 709ms/step - loss: 0.5660 - mae: 0.5797 - mse: 0.5660 - r2_score: -373.4626\n",
      "Epoch 176/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.5849 - mae: 0.5833 - mse: 0.5849 - r2_score: -365.6947\n",
      "Epoch 177/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 582ms/step - loss: 0.6048 - mae: 0.5894 - mse: 0.6048 - r2_score: -363.2091\n",
      "Epoch 178/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 594ms/step - loss: 0.5885 - mae: 0.5811 - mse: 0.5885 - r2_score: -368.6967\n",
      "Epoch 179/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 579ms/step - loss: 0.5554 - mae: 0.5665 - mse: 0.5554 - r2_score: -376.3326\n",
      "Epoch 180/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 478ms/step - loss: 0.5949 - mae: 0.5920 - mse: 0.5949 - r2_score: -373.7954\n",
      "Epoch 181/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 561ms/step - loss: 0.5951 - mae: 0.5881 - mse: 0.5951 - r2_score: -367.4408\n",
      "Epoch 182/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 578ms/step - loss: 0.5727 - mae: 0.5780 - mse: 0.5727 - r2_score: -372.0433\n",
      "Epoch 183/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.5903 - mae: 0.5847 - mse: 0.5903 - r2_score: -369.0403\n",
      "Epoch 184/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 592ms/step - loss: 0.5779 - mae: 0.5756 - mse: 0.5779 - r2_score: -369.8189\n",
      "Epoch 185/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 595ms/step - loss: 0.5672 - mae: 0.5731 - mse: 0.5672 - r2_score: -366.5081\n",
      "Epoch 186/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 589ms/step - loss: 0.5489 - mae: 0.5597 - mse: 0.5489 - r2_score: -369.5904\n",
      "Epoch 187/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 589ms/step - loss: 0.5317 - mae: 0.5551 - mse: 0.5317 - r2_score: -372.9421\n",
      "Epoch 188/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 611ms/step - loss: 0.5519 - mae: 0.5694 - mse: 0.5519 - r2_score: -368.6323\n",
      "Epoch 189/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 623ms/step - loss: 0.5470 - mae: 0.5570 - mse: 0.5470 - r2_score: -366.9860\n",
      "Epoch 190/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 592ms/step - loss: 0.5324 - mae: 0.5643 - mse: 0.5324 - r2_score: -374.2079\n",
      "Epoch 191/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 593ms/step - loss: 0.5493 - mae: 0.5711 - mse: 0.5493 - r2_score: -372.5669\n",
      "Epoch 192/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 608ms/step - loss: 0.5155 - mae: 0.5503 - mse: 0.5155 - r2_score: -377.1373\n",
      "Epoch 193/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.5242 - mae: 0.5547 - mse: 0.5242 - r2_score: -373.5263\n",
      "Epoch 194/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.5575 - mae: 0.5678 - mse: 0.5575 - r2_score: -371.3798\n",
      "Epoch 195/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 616ms/step - loss: 0.5588 - mae: 0.5626 - mse: 0.5588 - r2_score: -378.1483\n",
      "Epoch 196/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 611ms/step - loss: 0.5624 - mae: 0.5678 - mse: 0.5624 - r2_score: -375.5080\n",
      "Epoch 197/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 606ms/step - loss: 0.5823 - mae: 0.5775 - mse: 0.5823 - r2_score: -373.0605\n",
      "Epoch 198/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 593ms/step - loss: 0.5445 - mae: 0.5619 - mse: 0.5445 - r2_score: -379.9563\n",
      "Epoch 199/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 596ms/step - loss: 0.5343 - mae: 0.5605 - mse: 0.5343 - r2_score: -379.6948\n",
      "Epoch 200/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 535ms/step - loss: 0.5442 - mae: 0.5586 - mse: 0.5442 - r2_score: -372.5149\n",
      "Epoch 201/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 603ms/step - loss: 0.5208 - mae: 0.5579 - mse: 0.5208 - r2_score: -378.8841\n",
      "Epoch 202/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 599ms/step - loss: 0.5684 - mae: 0.5711 - mse: 0.5684 - r2_score: -372.7918\n",
      "Epoch 203/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 533ms/step - loss: 0.5365 - mae: 0.5581 - mse: 0.5365 - r2_score: -376.7279\n",
      "Epoch 204/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 542ms/step - loss: 0.5311 - mae: 0.5585 - mse: 0.5311 - r2_score: -376.8173\n",
      "Epoch 205/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 606ms/step - loss: 0.5334 - mae: 0.5605 - mse: 0.5334 - r2_score: -376.7699\n",
      "Epoch 206/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 596ms/step - loss: 0.5490 - mae: 0.5718 - mse: 0.5490 - r2_score: -377.0709\n",
      "Epoch 207/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 624ms/step - loss: 0.5729 - mae: 0.5785 - mse: 0.5729 - r2_score: -373.7053\n",
      "Epoch 208/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 600ms/step - loss: 0.5431 - mae: 0.5653 - mse: 0.5431 - r2_score: -385.6982\n",
      "Epoch 209/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 594ms/step - loss: 0.5395 - mae: 0.5623 - mse: 0.5395 - r2_score: -377.1089\n",
      "Epoch 210/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 593ms/step - loss: 0.5144 - mae: 0.5492 - mse: 0.5144 - r2_score: -376.8520\n",
      "Epoch 211/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 604ms/step - loss: 0.5144 - mae: 0.5419 - mse: 0.5144 - r2_score: -372.9848\n",
      "Epoch 212/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 585ms/step - loss: 0.5218 - mae: 0.5548 - mse: 0.5218 - r2_score: -375.7266\n",
      "Epoch 213/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 556ms/step - loss: 0.5367 - mae: 0.5540 - mse: 0.5367 - r2_score: -374.6742\n",
      "Epoch 214/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 633ms/step - loss: 0.5551 - mae: 0.5656 - mse: 0.5551 - r2_score: -371.7932\n",
      "Epoch 215/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.5193 - mae: 0.5495 - mse: 0.5193 - r2_score: -378.7881\n",
      "Epoch 216/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 596ms/step - loss: 0.5142 - mae: 0.5494 - mse: 0.5142 - r2_score: -381.4533\n",
      "Epoch 217/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 617ms/step - loss: 0.5486 - mae: 0.5624 - mse: 0.5486 - r2_score: -373.5016\n",
      "Epoch 218/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 628ms/step - loss: 0.5378 - mae: 0.5574 - mse: 0.5378 - r2_score: -383.8968\n",
      "Epoch 219/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 625ms/step - loss: 0.4981 - mae: 0.5439 - mse: 0.4981 - r2_score: -392.2673\n",
      "Epoch 220/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 608ms/step - loss: 0.5181 - mae: 0.5533 - mse: 0.5181 - r2_score: -377.0851\n",
      "Epoch 221/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.5308 - mae: 0.5539 - mse: 0.5308 - r2_score: -381.6560\n",
      "Epoch 222/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 520ms/step - loss: 0.5344 - mae: 0.5559 - mse: 0.5344 - r2_score: -383.6922\n",
      "Epoch 223/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 589ms/step - loss: 0.5281 - mae: 0.5544 - mse: 0.5281 - r2_score: -372.3758\n",
      "Epoch 224/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 589ms/step - loss: 0.5097 - mae: 0.5412 - mse: 0.5097 - r2_score: -382.0202\n",
      "Epoch 225/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 594ms/step - loss: 0.5263 - mae: 0.5586 - mse: 0.5263 - r2_score: -378.6917\n",
      "Epoch 226/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 584ms/step - loss: 0.5377 - mae: 0.5571 - mse: 0.5377 - r2_score: -381.0073\n",
      "Epoch 227/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 602ms/step - loss: 0.5236 - mae: 0.5518 - mse: 0.5236 - r2_score: -385.2796\n",
      "Epoch 228/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 615ms/step - loss: 0.5506 - mae: 0.5691 - mse: 0.5506 - r2_score: -376.4116\n",
      "Epoch 229/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 584ms/step - loss: 0.5099 - mae: 0.5433 - mse: 0.5099 - r2_score: -380.1694\n",
      "Epoch 230/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.4971 - mae: 0.5381 - mse: 0.4971 - r2_score: -385.0356\n",
      "Epoch 231/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 599ms/step - loss: 0.5287 - mae: 0.5583 - mse: 0.5287 - r2_score: -377.0187\n",
      "Epoch 232/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 609ms/step - loss: 0.5724 - mae: 0.5805 - mse: 0.5724 - r2_score: -400.4136\n",
      "Epoch 233/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 595ms/step - loss: 0.5178 - mae: 0.5550 - mse: 0.5178 - r2_score: -387.4279\n",
      "Epoch 234/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 611ms/step - loss: 0.5087 - mae: 0.5471 - mse: 0.5087 - r2_score: -381.9990\n",
      "Epoch 235/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 627ms/step - loss: 0.5228 - mae: 0.5585 - mse: 0.5228 - r2_score: -385.0061\n",
      "Epoch 236/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 603ms/step - loss: 0.5214 - mae: 0.5481 - mse: 0.5214 - r2_score: -385.3643\n",
      "Epoch 237/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 610ms/step - loss: 0.5059 - mae: 0.5439 - mse: 0.5059 - r2_score: -384.1370\n",
      "Epoch 238/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 440ms/step - loss: 0.5111 - mae: 0.5505 - mse: 0.5111 - r2_score: -379.4232\n",
      "Epoch 239/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 606ms/step - loss: 0.5051 - mae: 0.5431 - mse: 0.5051 - r2_score: -384.2605\n",
      "Epoch 240/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 534ms/step - loss: 0.5083 - mae: 0.5419 - mse: 0.5083 - r2_score: -382.3702\n",
      "Epoch 241/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 597ms/step - loss: 0.4961 - mae: 0.5408 - mse: 0.4961 - r2_score: -382.7923\n",
      "Epoch 242/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 587ms/step - loss: 0.5015 - mae: 0.5342 - mse: 0.5015 - r2_score: -388.6747\n",
      "Epoch 243/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 584ms/step - loss: 0.5030 - mae: 0.5393 - mse: 0.5030 - r2_score: -385.0074\n",
      "Epoch 244/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 520ms/step - loss: 0.5009 - mae: 0.5421 - mse: 0.5009 - r2_score: -384.9617\n",
      "Epoch 245/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - loss: 0.5035 - mae: 0.5412 - mse: 0.5035 - r2_score: -383.2610\n",
      "Epoch 246/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 569ms/step - loss: 0.5182 - mae: 0.5449 - mse: 0.5182 - r2_score: -392.2982\n",
      "Epoch 247/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 606ms/step - loss: 0.5117 - mae: 0.5463 - mse: 0.5117 - r2_score: -390.6914\n",
      "Epoch 248/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 585ms/step - loss: 0.4845 - mae: 0.5390 - mse: 0.4845 - r2_score: -385.3933\n",
      "Epoch 249/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - loss: 0.4886 - mae: 0.5343 - mse: 0.4886 - r2_score: -394.4383\n",
      "Epoch 250/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 595ms/step - loss: 0.4965 - mae: 0.5380 - mse: 0.4965 - r2_score: -383.9412\n",
      "Epoch 251/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 512ms/step - loss: 0.5097 - mae: 0.5475 - mse: 0.5097 - r2_score: -384.4908\n",
      "Epoch 252/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 595ms/step - loss: 0.4700 - mae: 0.5281 - mse: 0.4700 - r2_score: -396.4051\n",
      "Epoch 253/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - loss: 0.4918 - mae: 0.5364 - mse: 0.4918 - r2_score: -389.4517\n",
      "Epoch 254/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 591ms/step - loss: 0.4813 - mae: 0.5298 - mse: 0.4813 - r2_score: -389.4079\n",
      "Epoch 255/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 598ms/step - loss: 0.4823 - mae: 0.5347 - mse: 0.4823 - r2_score: -392.7702\n",
      "Epoch 256/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 561ms/step - loss: 0.4803 - mae: 0.5301 - mse: 0.4803 - r2_score: -386.4258\n",
      "Epoch 257/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 561ms/step - loss: 0.4782 - mae: 0.5257 - mse: 0.4782 - r2_score: -389.8225\n",
      "Epoch 258/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 612ms/step - loss: 0.4888 - mae: 0.5327 - mse: 0.4888 - r2_score: -391.8776\n",
      "Epoch 259/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 611ms/step - loss: 0.4707 - mae: 0.5209 - mse: 0.4707 - r2_score: -385.3654\n",
      "Epoch 260/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 593ms/step - loss: 0.4813 - mae: 0.5288 - mse: 0.4813 - r2_score: -390.2432\n",
      "Epoch 261/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 539ms/step - loss: 0.4864 - mae: 0.5338 - mse: 0.4864 - r2_score: -387.5479\n",
      "Epoch 262/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 421ms/step - loss: 0.4846 - mae: 0.5279 - mse: 0.4846 - r2_score: -389.2891\n",
      "Epoch 263/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 368ms/step - loss: 0.5069 - mae: 0.5423 - mse: 0.5069 - r2_score: -394.8011\n",
      "Epoch 264/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 370ms/step - loss: 0.4829 - mae: 0.5329 - mse: 0.4829 - r2_score: -388.2888\n",
      "Epoch 265/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 358ms/step - loss: 0.4786 - mae: 0.5269 - mse: 0.4786 - r2_score: -393.5778\n",
      "Epoch 266/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 460ms/step - loss: 0.5045 - mae: 0.5349 - mse: 0.5045 - r2_score: -390.5379\n",
      "Epoch 267/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 685ms/step - loss: 0.4672 - mae: 0.5160 - mse: 0.4672 - r2_score: -397.0997\n",
      "Epoch 268/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 703ms/step - loss: 0.4997 - mae: 0.5372 - mse: 0.4997 - r2_score: -390.5915\n",
      "Epoch 269/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 703ms/step - loss: 0.4794 - mae: 0.5199 - mse: 0.4794 - r2_score: -389.9286\n",
      "Epoch 270/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 724ms/step - loss: 0.4656 - mae: 0.5186 - mse: 0.4656 - r2_score: -396.6378\n",
      "Epoch 271/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 732ms/step - loss: 0.4511 - mae: 0.5133 - mse: 0.4511 - r2_score: -388.1871\n",
      "Epoch 272/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 355ms/step - loss: 0.4629 - mae: 0.5254 - mse: 0.4629 - r2_score: -400.9289\n",
      "Epoch 273/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 541ms/step - loss: 0.4454 - mae: 0.5093 - mse: 0.4454 - r2_score: -396.9364\n",
      "Epoch 274/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 739ms/step - loss: 0.4636 - mae: 0.5183 - mse: 0.4636 - r2_score: -401.4189\n",
      "Epoch 275/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 742ms/step - loss: 0.4716 - mae: 0.5251 - mse: 0.4716 - r2_score: -391.0620\n",
      "Epoch 276/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 703ms/step - loss: 0.4827 - mae: 0.5297 - mse: 0.4827 - r2_score: -392.1029\n",
      "Epoch 277/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 708ms/step - loss: 0.4693 - mae: 0.5189 - mse: 0.4693 - r2_score: -391.3810\n",
      "Epoch 278/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 711ms/step - loss: 0.4731 - mae: 0.5173 - mse: 0.4731 - r2_score: -387.5643\n",
      "Epoch 279/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 588ms/step - loss: 0.4767 - mae: 0.5223 - mse: 0.4767 - r2_score: -394.9459\n",
      "Epoch 280/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 328ms/step - loss: 0.4760 - mae: 0.5236 - mse: 0.4760 - r2_score: -401.8829\n",
      "Epoch 281/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 698ms/step - loss: 0.4992 - mae: 0.5411 - mse: 0.4992 - r2_score: -391.7617\n",
      "Epoch 282/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 679ms/step - loss: 0.4648 - mae: 0.5158 - mse: 0.4648 - r2_score: -392.6364\n",
      "Epoch 283/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 706ms/step - loss: 0.4780 - mae: 0.5242 - mse: 0.4780 - r2_score: -392.7318\n",
      "Epoch 284/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 713ms/step - loss: 0.4804 - mae: 0.5206 - mse: 0.4804 - r2_score: -395.5237\n",
      "Epoch 285/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 707ms/step - loss: 0.4508 - mae: 0.5124 - mse: 0.4508 - r2_score: -398.1792\n",
      "Epoch 286/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 721ms/step - loss: 0.4825 - mae: 0.5304 - mse: 0.4825 - r2_score: -399.4842\n",
      "Epoch 287/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 717ms/step - loss: 0.4642 - mae: 0.5147 - mse: 0.4642 - r2_score: -397.5967\n",
      "Epoch 288/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 744ms/step - loss: 0.4908 - mae: 0.5257 - mse: 0.4908 - r2_score: -394.9835\n",
      "Epoch 289/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 702ms/step - loss: 0.4744 - mae: 0.5295 - mse: 0.4744 - r2_score: -391.8444\n",
      "Epoch 290/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 698ms/step - loss: 0.4515 - mae: 0.5115 - mse: 0.4515 - r2_score: -395.7397\n",
      "Epoch 291/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 725ms/step - loss: 0.4472 - mae: 0.5093 - mse: 0.4472 - r2_score: -398.1986\n",
      "Epoch 292/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 698ms/step - loss: 0.4499 - mae: 0.5051 - mse: 0.4499 - r2_score: -393.2230\n",
      "Epoch 293/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 723ms/step - loss: 0.4494 - mae: 0.5179 - mse: 0.4494 - r2_score: -401.4303\n",
      "Epoch 294/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 728ms/step - loss: 0.4663 - mae: 0.5193 - mse: 0.4663 - r2_score: -399.4861\n",
      "Epoch 295/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 718ms/step - loss: 0.4524 - mae: 0.5081 - mse: 0.4524 - r2_score: -399.1314\n",
      "Epoch 296/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 704ms/step - loss: 0.4674 - mae: 0.5081 - mse: 0.4674 - r2_score: -397.1021\n",
      "Epoch 297/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 704ms/step - loss: 0.4743 - mae: 0.5210 - mse: 0.4743 - r2_score: -399.4056\n",
      "Epoch 298/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 708ms/step - loss: 0.4506 - mae: 0.5071 - mse: 0.4506 - r2_score: -397.5447\n",
      "Epoch 299/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 684ms/step - loss: 0.4901 - mae: 0.5338 - mse: 0.4901 - r2_score: -403.2048\n",
      "Epoch 300/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 578ms/step - loss: 0.4735 - mae: 0.5217 - mse: 0.4735 - r2_score: -401.2737\n",
      "Epoch 301/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 448ms/step - loss: 0.4566 - mae: 0.5052 - mse: 0.4566 - r2_score: -391.4563\n",
      "Epoch 302/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 753ms/step - loss: 0.4494 - mae: 0.5123 - mse: 0.4494 - r2_score: -397.4769\n",
      "Epoch 303/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 760ms/step - loss: 0.4189 - mae: 0.4939 - mse: 0.4189 - r2_score: -400.8628\n",
      "Epoch 304/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 761ms/step - loss: 0.4533 - mae: 0.5066 - mse: 0.4533 - r2_score: -401.1147\n",
      "Epoch 305/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 431ms/step - loss: 0.4376 - mae: 0.5052 - mse: 0.4376 - r2_score: -398.3613\n",
      "Epoch 306/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 531ms/step - loss: 0.4306 - mae: 0.4979 - mse: 0.4306 - r2_score: -403.9832\n",
      "Epoch 307/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 681ms/step - loss: 0.4461 - mae: 0.5105 - mse: 0.4461 - r2_score: -407.0118\n",
      "Epoch 308/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 691ms/step - loss: 0.4599 - mae: 0.5164 - mse: 0.4599 - r2_score: -397.7669\n",
      "Epoch 309/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 709ms/step - loss: 0.5008 - mae: 0.5350 - mse: 0.5008 - r2_score: -412.2290\n",
      "Epoch 310/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 389ms/step - loss: 0.4380 - mae: 0.5023 - mse: 0.4380 - r2_score: -398.5975\n",
      "Epoch 311/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 487ms/step - loss: 0.4405 - mae: 0.5083 - mse: 0.4405 - r2_score: -400.3794\n",
      "Epoch 312/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 719ms/step - loss: 0.4363 - mae: 0.4995 - mse: 0.4363 - r2_score: -399.0807\n",
      "Epoch 313/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 761ms/step - loss: 0.4364 - mae: 0.4991 - mse: 0.4364 - r2_score: -400.6614\n",
      "Epoch 314/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 739ms/step - loss: 0.4484 - mae: 0.5067 - mse: 0.4484 - r2_score: -401.2945\n",
      "Epoch 315/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 734ms/step - loss: 0.4431 - mae: 0.5056 - mse: 0.4431 - r2_score: -399.6551\n",
      "Epoch 316/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 772ms/step - loss: 0.4394 - mae: 0.4994 - mse: 0.4394 - r2_score: -398.2285\n",
      "Epoch 317/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 765ms/step - loss: 0.4373 - mae: 0.5016 - mse: 0.4373 - r2_score: -407.0642\n",
      "Epoch 318/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 754ms/step - loss: 0.4450 - mae: 0.5135 - mse: 0.4450 - r2_score: -404.8322\n",
      "Epoch 319/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 661ms/step - loss: 0.4677 - mae: 0.5213 - mse: 0.4677 - r2_score: -392.9241\n",
      "Epoch 320/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 695ms/step - loss: 0.4177 - mae: 0.4906 - mse: 0.4177 - r2_score: -401.3310\n",
      "Epoch 321/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 755ms/step - loss: 0.4322 - mae: 0.5048 - mse: 0.4322 - r2_score: -402.3587\n",
      "Epoch 322/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 748ms/step - loss: 0.4658 - mae: 0.5192 - mse: 0.4658 - r2_score: -401.6573\n",
      "Epoch 323/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 709ms/step - loss: 0.4507 - mae: 0.5166 - mse: 0.4507 - r2_score: -402.9822\n",
      "Epoch 324/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 748ms/step - loss: 0.4470 - mae: 0.5048 - mse: 0.4470 - r2_score: -392.7363\n",
      "Epoch 325/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 741ms/step - loss: 0.4213 - mae: 0.4926 - mse: 0.4213 - r2_score: -401.5601\n",
      "Epoch 326/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 799ms/step - loss: 0.4077 - mae: 0.4851 - mse: 0.4077 - r2_score: -407.8936\n",
      "Epoch 327/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 806ms/step - loss: 0.4152 - mae: 0.4912 - mse: 0.4152 - r2_score: -407.0100\n",
      "Epoch 328/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 735ms/step - loss: 0.4454 - mae: 0.5055 - mse: 0.4454 - r2_score: -399.2098\n",
      "Epoch 329/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 733ms/step - loss: 0.4350 - mae: 0.5023 - mse: 0.4350 - r2_score: -402.3372\n",
      "Epoch 330/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 712ms/step - loss: 0.4199 - mae: 0.4885 - mse: 0.4199 - r2_score: -405.3500\n",
      "Epoch 331/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 692ms/step - loss: 0.4345 - mae: 0.5037 - mse: 0.4345 - r2_score: -408.1625\n",
      "Epoch 332/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 674ms/step - loss: 0.4220 - mae: 0.4933 - mse: 0.4220 - r2_score: -398.7025\n",
      "Epoch 333/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 722ms/step - loss: 0.4413 - mae: 0.5021 - mse: 0.4413 - r2_score: -396.9740\n",
      "Epoch 334/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 456ms/step - loss: 0.4293 - mae: 0.4991 - mse: 0.4293 - r2_score: -402.3939\n",
      "Epoch 335/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 498ms/step - loss: 0.4513 - mae: 0.5108 - mse: 0.4513 - r2_score: -404.3557\n",
      "Epoch 336/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 690ms/step - loss: 0.4358 - mae: 0.4989 - mse: 0.4358 - r2_score: -407.9502\n",
      "Epoch 337/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 701ms/step - loss: 0.4133 - mae: 0.4887 - mse: 0.4133 - r2_score: -400.3089\n",
      "Epoch 338/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 732ms/step - loss: 0.4370 - mae: 0.5027 - mse: 0.4370 - r2_score: -410.8261\n",
      "Epoch 339/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 699ms/step - loss: 0.4300 - mae: 0.4978 - mse: 0.4300 - r2_score: -394.3075\n",
      "Epoch 340/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 709ms/step - loss: 0.4176 - mae: 0.4891 - mse: 0.4176 - r2_score: -403.4535\n",
      "Epoch 341/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 723ms/step - loss: 0.4122 - mae: 0.4903 - mse: 0.4122 - r2_score: -405.3664\n",
      "Epoch 342/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 601ms/step - loss: 0.4161 - mae: 0.4900 - mse: 0.4161 - r2_score: -406.7153\n",
      "Epoch 343/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 621ms/step - loss: 0.4286 - mae: 0.4960 - mse: 0.4286 - r2_score: -409.6688\n",
      "Epoch 344/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 745ms/step - loss: 0.4389 - mae: 0.5046 - mse: 0.4389 - r2_score: -405.7421\n",
      "Epoch 345/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 766ms/step - loss: 0.4177 - mae: 0.4844 - mse: 0.4177 - r2_score: -406.6153\n",
      "Epoch 346/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 705ms/step - loss: 0.4053 - mae: 0.4852 - mse: 0.4053 - r2_score: -405.3391\n",
      "Epoch 347/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 825ms/step - loss: 0.4407 - mae: 0.5045 - mse: 0.4407 - r2_score: -405.9047\n",
      "Epoch 348/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 763ms/step - loss: 0.4037 - mae: 0.4787 - mse: 0.4037 - r2_score: -404.1720\n",
      "Epoch 349/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 809ms/step - loss: 0.4429 - mae: 0.4953 - mse: 0.4429 - r2_score: -400.9217\n",
      "Epoch 350/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 747ms/step - loss: 0.4049 - mae: 0.4856 - mse: 0.4049 - r2_score: -412.3419\n",
      "Epoch 351/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 693ms/step - loss: 0.4432 - mae: 0.5037 - mse: 0.4432 - r2_score: -401.3770\n",
      "Epoch 352/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 736ms/step - loss: 0.4085 - mae: 0.4850 - mse: 0.4085 - r2_score: -402.8720\n",
      "Epoch 353/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 476ms/step - loss: 0.4325 - mae: 0.4959 - mse: 0.4325 - r2_score: -406.4651\n",
      "Epoch 354/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 570ms/step - loss: 0.4161 - mae: 0.4950 - mse: 0.4161 - r2_score: -408.9356\n",
      "Epoch 355/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 760ms/step - loss: 0.4489 - mae: 0.5060 - mse: 0.4489 - r2_score: -413.4706\n",
      "Epoch 356/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 406ms/step - loss: 0.4330 - mae: 0.4904 - mse: 0.4330 - r2_score: -404.8180\n",
      "Epoch 357/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 712ms/step - loss: 0.4688 - mae: 0.5192 - mse: 0.4688 - r2_score: -403.9064\n",
      "Epoch 358/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 720ms/step - loss: 0.4458 - mae: 0.5032 - mse: 0.4458 - r2_score: -400.1568\n",
      "Epoch 359/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 722ms/step - loss: 0.4386 - mae: 0.5034 - mse: 0.4386 - r2_score: -402.4051\n",
      "Epoch 360/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 719ms/step - loss: 0.4092 - mae: 0.4861 - mse: 0.4092 - r2_score: -403.8877\n",
      "Epoch 361/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 745ms/step - loss: 0.4068 - mae: 0.4860 - mse: 0.4068 - r2_score: -403.3568\n",
      "Epoch 362/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 786ms/step - loss: 0.4186 - mae: 0.4901 - mse: 0.4186 - r2_score: -404.1474\n",
      "Epoch 363/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 796ms/step - loss: 0.4030 - mae: 0.4792 - mse: 0.4030 - r2_score: -409.5493\n",
      "Epoch 364/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 743ms/step - loss: 0.4240 - mae: 0.4900 - mse: 0.4240 - r2_score: -405.3096\n",
      "Epoch 365/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 680ms/step - loss: 0.4147 - mae: 0.4845 - mse: 0.4147 - r2_score: -406.4861\n",
      "Epoch 366/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 758ms/step - loss: 0.4086 - mae: 0.4846 - mse: 0.4086 - r2_score: -404.4832\n",
      "Epoch 367/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 731ms/step - loss: 0.4185 - mae: 0.4861 - mse: 0.4185 - r2_score: -408.2246\n",
      "Epoch 368/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 742ms/step - loss: 0.4445 - mae: 0.5019 - mse: 0.4445 - r2_score: -408.8257\n",
      "Epoch 369/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 761ms/step - loss: 0.4106 - mae: 0.4893 - mse: 0.4106 - r2_score: -410.6485\n",
      "Epoch 370/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 743ms/step - loss: 0.4160 - mae: 0.4927 - mse: 0.4160 - r2_score: -405.2867\n",
      "Epoch 371/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 773ms/step - loss: 0.4264 - mae: 0.4895 - mse: 0.4264 - r2_score: -409.5945\n",
      "Epoch 372/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 778ms/step - loss: 0.4130 - mae: 0.4900 - mse: 0.4130 - r2_score: -405.2631\n",
      "Epoch 373/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.4273 - mae: 0.4906 - mse: 0.4273 - r2_score: -404.7920\n",
      "Epoch 374/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 597ms/step - loss: 0.4224 - mae: 0.4899 - mse: 0.4224 - r2_score: -412.2996\n",
      "Epoch 375/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 751ms/step - loss: 0.4214 - mae: 0.4902 - mse: 0.4214 - r2_score: -413.8377\n",
      "Epoch 376/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 786ms/step - loss: 0.4198 - mae: 0.4963 - mse: 0.4198 - r2_score: -413.7986\n",
      "Epoch 377/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 767ms/step - loss: 0.4221 - mae: 0.4928 - mse: 0.4221 - r2_score: -405.8033\n",
      "Epoch 378/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.4185 - mae: 0.4840 - mse: 0.4185 - r2_score: -404.4372\n",
      "Epoch 379/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 510ms/step - loss: 0.4054 - mae: 0.4830 - mse: 0.4054 - r2_score: -414.0239\n",
      "Epoch 380/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 853ms/step - loss: 0.4027 - mae: 0.4806 - mse: 0.4027 - r2_score: -408.9185\n",
      "Epoch 381/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 726ms/step - loss: 0.4074 - mae: 0.4859 - mse: 0.4074 - r2_score: -410.6325\n",
      "Epoch 382/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 506ms/step - loss: 0.3993 - mae: 0.4815 - mse: 0.3993 - r2_score: -414.0285\n",
      "Epoch 383/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 415ms/step - loss: 0.3986 - mae: 0.4798 - mse: 0.3986 - r2_score: -409.6980\n",
      "Epoch 384/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 693ms/step - loss: 0.3985 - mae: 0.4734 - mse: 0.3985 - r2_score: -411.8262\n",
      "Epoch 385/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 373ms/step - loss: 0.3874 - mae: 0.4786 - mse: 0.3874 - r2_score: -414.5145\n",
      "Epoch 386/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 528ms/step - loss: 0.4307 - mae: 0.4994 - mse: 0.4307 - r2_score: -411.8604\n",
      "Epoch 387/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 722ms/step - loss: 0.4068 - mae: 0.4823 - mse: 0.4068 - r2_score: -414.3514\n",
      "Epoch 388/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 786ms/step - loss: 0.4329 - mae: 0.4989 - mse: 0.4329 - r2_score: -406.6092\n",
      "Epoch 389/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 752ms/step - loss: 0.4093 - mae: 0.4839 - mse: 0.4093 - r2_score: -408.8257\n",
      "Epoch 390/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 684ms/step - loss: 0.3986 - mae: 0.4775 - mse: 0.3986 - r2_score: -410.1894\n",
      "Epoch 391/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 423ms/step - loss: 0.4145 - mae: 0.4780 - mse: 0.4145 - r2_score: -408.8792\n",
      "Epoch 392/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 760ms/step - loss: 0.3880 - mae: 0.4766 - mse: 0.3880 - r2_score: -413.1676\n",
      "Epoch 393/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 738ms/step - loss: 0.4214 - mae: 0.4925 - mse: 0.4214 - r2_score: -408.7540\n",
      "Epoch 394/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 732ms/step - loss: 0.4018 - mae: 0.4753 - mse: 0.4018 - r2_score: -413.8975\n",
      "Epoch 395/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 716ms/step - loss: 0.4005 - mae: 0.4777 - mse: 0.4005 - r2_score: -412.4331\n",
      "Epoch 396/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 691ms/step - loss: 0.3831 - mae: 0.4699 - mse: 0.3831 - r2_score: -419.3867\n",
      "Epoch 397/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 721ms/step - loss: 0.3897 - mae: 0.4675 - mse: 0.3897 - r2_score: -410.2512\n",
      "Epoch 398/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 703ms/step - loss: 0.4129 - mae: 0.4791 - mse: 0.4129 - r2_score: -409.7878\n",
      "Epoch 399/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 722ms/step - loss: 0.3784 - mae: 0.4665 - mse: 0.3784 - r2_score: -419.8531\n",
      "Epoch 400/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 686ms/step - loss: 0.4067 - mae: 0.4763 - mse: 0.4067 - r2_score: -411.3764\n",
      "Epoch 401/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 719ms/step - loss: 0.4226 - mae: 0.4875 - mse: 0.4226 - r2_score: -418.4784\n",
      "Epoch 402/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 730ms/step - loss: 0.4003 - mae: 0.4688 - mse: 0.4003 - r2_score: -417.4939\n",
      "Epoch 403/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 353ms/step - loss: 0.3909 - mae: 0.4723 - mse: 0.3909 - r2_score: -410.5810\n",
      "Epoch 404/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 614ms/step - loss: 0.4076 - mae: 0.4792 - mse: 0.4076 - r2_score: -408.4543\n",
      "Epoch 405/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 693ms/step - loss: 0.3851 - mae: 0.4688 - mse: 0.3851 - r2_score: -423.2413\n",
      "Epoch 406/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 720ms/step - loss: 0.4057 - mae: 0.4810 - mse: 0.4057 - r2_score: -408.7737\n",
      "Epoch 407/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 368ms/step - loss: 0.3812 - mae: 0.4616 - mse: 0.3812 - r2_score: -409.2201\n",
      "Epoch 408/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 583ms/step - loss: 0.3944 - mae: 0.4699 - mse: 0.3944 - r2_score: -422.1212\n",
      "Epoch 409/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 704ms/step - loss: 0.3770 - mae: 0.4619 - mse: 0.3770 - r2_score: -414.5144\n",
      "Epoch 410/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 717ms/step - loss: 0.3917 - mae: 0.4747 - mse: 0.3917 - r2_score: -414.7840\n",
      "Epoch 411/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 683ms/step - loss: 0.3757 - mae: 0.4519 - mse: 0.3757 - r2_score: -414.4098\n",
      "Epoch 412/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 520ms/step - loss: 0.3977 - mae: 0.4757 - mse: 0.3977 - r2_score: -412.6134\n",
      "Epoch 413/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 452ms/step - loss: 0.3930 - mae: 0.4723 - mse: 0.3930 - r2_score: -415.1625\n",
      "Epoch 414/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 719ms/step - loss: 0.3715 - mae: 0.4627 - mse: 0.3715 - r2_score: -418.3092\n",
      "Epoch 415/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 373ms/step - loss: 0.3526 - mae: 0.4488 - mse: 0.3526 - r2_score: -424.4611\n",
      "Epoch 416/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 598ms/step - loss: 0.3887 - mae: 0.4730 - mse: 0.3887 - r2_score: -414.4278\n",
      "Epoch 417/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 694ms/step - loss: 0.3949 - mae: 0.4718 - mse: 0.3949 - r2_score: -416.5913\n",
      "Epoch 418/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 702ms/step - loss: 0.3982 - mae: 0.4759 - mse: 0.3982 - r2_score: -412.2522\n",
      "Epoch 419/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 437ms/step - loss: 0.3834 - mae: 0.4700 - mse: 0.3834 - r2_score: -421.6348\n",
      "Epoch 420/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 690ms/step - loss: 0.4203 - mae: 0.4958 - mse: 0.4203 - r2_score: -418.7919\n",
      "Epoch 421/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 697ms/step - loss: 0.3957 - mae: 0.4735 - mse: 0.3957 - r2_score: -413.4448\n",
      "Epoch 422/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 695ms/step - loss: 0.3931 - mae: 0.4727 - mse: 0.3931 - r2_score: -420.6235\n",
      "Epoch 423/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 868ms/step - loss: 0.3842 - mae: 0.4707 - mse: 0.3842 - r2_score: -413.3046\n",
      "Epoch 424/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 832ms/step - loss: 0.3823 - mae: 0.4667 - mse: 0.3823 - r2_score: -417.7454\n",
      "Epoch 425/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 934ms/step - loss: 0.3864 - mae: 0.4669 - mse: 0.3864 - r2_score: -411.0205\n",
      "Epoch 426/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 956ms/step - loss: 0.3812 - mae: 0.4688 - mse: 0.3812 - r2_score: -418.0110\n",
      "Epoch 427/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 821ms/step - loss: 0.3962 - mae: 0.4749 - mse: 0.3962 - r2_score: -415.2470\n",
      "Epoch 428/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 721ms/step - loss: 0.3918 - mae: 0.4705 - mse: 0.3918 - r2_score: -410.6505\n",
      "Epoch 429/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 961ms/step - loss: 0.3893 - mae: 0.4698 - mse: 0.3893 - r2_score: -418.0742\n",
      "Epoch 430/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 775ms/step - loss: 0.3930 - mae: 0.4768 - mse: 0.3930 - r2_score: -416.9556\n",
      "Epoch 431/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 925ms/step - loss: 0.3741 - mae: 0.4609 - mse: 0.3741 - r2_score: -418.8706\n",
      "Epoch 432/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 970ms/step - loss: 0.3929 - mae: 0.4708 - mse: 0.3929 - r2_score: -411.8250\n",
      "Epoch 433/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 728ms/step - loss: 0.4077 - mae: 0.4825 - mse: 0.4077 - r2_score: -420.4525\n",
      "Epoch 434/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 928ms/step - loss: 0.3859 - mae: 0.4708 - mse: 0.3859 - r2_score: -414.9308\n",
      "Epoch 435/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 947ms/step - loss: 0.3811 - mae: 0.4674 - mse: 0.3811 - r2_score: -419.1415\n",
      "Epoch 436/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 757ms/step - loss: 0.3843 - mae: 0.4647 - mse: 0.3843 - r2_score: -416.7289\n",
      "Epoch 437/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 949ms/step - loss: 0.3724 - mae: 0.4556 - mse: 0.3724 - r2_score: -414.2520\n",
      "Epoch 438/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 915ms/step - loss: 0.3816 - mae: 0.4635 - mse: 0.3816 - r2_score: -411.3667\n",
      "Epoch 439/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 909ms/step - loss: 0.3655 - mae: 0.4509 - mse: 0.3655 - r2_score: -412.8755\n",
      "Epoch 440/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 973ms/step - loss: 0.3812 - mae: 0.4645 - mse: 0.3812 - r2_score: -417.2296\n",
      "Epoch 441/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 697ms/step - loss: 0.3861 - mae: 0.4690 - mse: 0.3861 - r2_score: -421.1570\n",
      "Epoch 442/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 918ms/step - loss: 0.4086 - mae: 0.4805 - mse: 0.4086 - r2_score: -418.4230\n",
      "Epoch 443/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 888ms/step - loss: 0.3873 - mae: 0.4683 - mse: 0.3873 - r2_score: -417.1173\n",
      "Epoch 444/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 765ms/step - loss: 0.3716 - mae: 0.4625 - mse: 0.3716 - r2_score: -414.6389\n",
      "Epoch 445/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 953ms/step - loss: 0.3727 - mae: 0.4626 - mse: 0.3727 - r2_score: -420.1906\n",
      "Epoch 446/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 915ms/step - loss: 0.3700 - mae: 0.4587 - mse: 0.3700 - r2_score: -421.5452\n",
      "Epoch 447/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 797ms/step - loss: 0.3531 - mae: 0.4492 - mse: 0.3531 - r2_score: -419.8268\n",
      "Epoch 448/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 949ms/step - loss: 0.3920 - mae: 0.4653 - mse: 0.3920 - r2_score: -416.3899\n",
      "Epoch 449/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 914ms/step - loss: 0.3649 - mae: 0.4585 - mse: 0.3649 - r2_score: -422.4737\n",
      "Epoch 450/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 945ms/step - loss: 0.3815 - mae: 0.4626 - mse: 0.3815 - r2_score: -426.0251\n",
      "Epoch 451/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - loss: 0.3722 - mae: 0.4623 - mse: 0.3722 - r2_score: -413.6350  \n",
      "Epoch 452/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 955ms/step - loss: 0.3717 - mae: 0.4597 - mse: 0.3717 - r2_score: -417.2013\n",
      "Epoch 453/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 805ms/step - loss: 0.3902 - mae: 0.4711 - mse: 0.3902 - r2_score: -425.7511\n",
      "Epoch 454/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 820ms/step - loss: 0.3845 - mae: 0.4673 - mse: 0.3845 - r2_score: -413.0746\n",
      "Epoch 455/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 946ms/step - loss: 0.3843 - mae: 0.4687 - mse: 0.3843 - r2_score: -422.1267\n",
      "Epoch 456/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 951ms/step - loss: 0.3867 - mae: 0.4677 - mse: 0.3867 - r2_score: -409.8139\n",
      "Epoch 457/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 861ms/step - loss: 0.3912 - mae: 0.4740 - mse: 0.3912 - r2_score: -419.9026\n",
      "Epoch 458/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 909ms/step - loss: 0.3817 - mae: 0.4741 - mse: 0.3817 - r2_score: -422.2617\n",
      "Epoch 459/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 634ms/step - loss: 0.3837 - mae: 0.4694 - mse: 0.3837 - r2_score: -415.2769\n",
      "Epoch 460/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 847ms/step - loss: 0.3773 - mae: 0.4646 - mse: 0.3773 - r2_score: -414.9803\n",
      "Epoch 461/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 843ms/step - loss: 0.3668 - mae: 0.4531 - mse: 0.3668 - r2_score: -420.1379\n",
      "Epoch 462/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 708ms/step - loss: 0.3664 - mae: 0.4561 - mse: 0.3664 - r2_score: -412.1815\n",
      "Epoch 463/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 784ms/step - loss: 0.3510 - mae: 0.4509 - mse: 0.3510 - r2_score: -422.2903\n",
      "Epoch 464/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 913ms/step - loss: 0.3768 - mae: 0.4600 - mse: 0.3768 - r2_score: -417.4468\n",
      "Epoch 465/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 786ms/step - loss: 0.3686 - mae: 0.4601 - mse: 0.3686 - r2_score: -424.9953\n",
      "Epoch 466/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - loss: 0.3604 - mae: 0.4506 - mse: 0.3604 - r2_score: -419.6210  \n",
      "Epoch 467/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 914ms/step - loss: 0.3810 - mae: 0.4649 - mse: 0.3810 - r2_score: -424.3057\n",
      "Epoch 468/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 693ms/step - loss: 0.3710 - mae: 0.4589 - mse: 0.3710 - r2_score: -422.7052\n",
      "Epoch 469/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 927ms/step - loss: 0.3943 - mae: 0.4688 - mse: 0.3943 - r2_score: -417.6490\n",
      "Epoch 470/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 974ms/step - loss: 0.3607 - mae: 0.4551 - mse: 0.3607 - r2_score: -419.7706\n",
      "Epoch 471/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 939ms/step - loss: 0.3696 - mae: 0.4621 - mse: 0.3696 - r2_score: -418.6420\n",
      "Epoch 472/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 736ms/step - loss: 0.3715 - mae: 0.4611 - mse: 0.3715 - r2_score: -425.9110\n",
      "Epoch 473/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 923ms/step - loss: 0.3824 - mae: 0.4674 - mse: 0.3824 - r2_score: -423.7763\n",
      "Epoch 474/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 838ms/step - loss: 0.3688 - mae: 0.4579 - mse: 0.3688 - r2_score: -423.8386\n",
      "Epoch 475/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 596ms/step - loss: 0.3617 - mae: 0.4512 - mse: 0.3617 - r2_score: -423.6722\n",
      "Epoch 476/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.3541 - mae: 0.4478 - mse: 0.3541 - r2_score: -415.8279\n",
      "Epoch 477/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 987ms/step - loss: 0.3855 - mae: 0.4691 - mse: 0.3855 - r2_score: -418.8314\n",
      "Epoch 478/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 888ms/step - loss: 0.3481 - mae: 0.4413 - mse: 0.3481 - r2_score: -422.1489\n",
      "Epoch 479/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 773ms/step - loss: 0.3830 - mae: 0.4709 - mse: 0.3830 - r2_score: -428.9344\n",
      "Epoch 480/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 939ms/step - loss: 0.3813 - mae: 0.4627 - mse: 0.3813 - r2_score: -424.1138\n",
      "Epoch 481/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 912ms/step - loss: 0.3746 - mae: 0.4650 - mse: 0.3746 - r2_score: -415.5978\n",
      "Epoch 482/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 993ms/step - loss: 0.3607 - mae: 0.4609 - mse: 0.3607 - r2_score: -424.8544\n",
      "Epoch 483/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 934ms/step - loss: 0.3600 - mae: 0.4561 - mse: 0.3600 - r2_score: -424.8777\n",
      "Epoch 484/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 943ms/step - loss: 0.3502 - mae: 0.4550 - mse: 0.3502 - r2_score: -424.8857\n",
      "Epoch 485/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 915ms/step - loss: 0.3556 - mae: 0.4485 - mse: 0.3556 - r2_score: -417.0817\n",
      "Epoch 486/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 928ms/step - loss: 0.3490 - mae: 0.4448 - mse: 0.3490 - r2_score: -419.5807\n",
      "Epoch 487/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 938ms/step - loss: 0.3640 - mae: 0.4557 - mse: 0.3640 - r2_score: -425.1466\n",
      "Epoch 488/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 970ms/step - loss: 0.3400 - mae: 0.4360 - mse: 0.3400 - r2_score: -422.3163\n",
      "Epoch 489/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 955ms/step - loss: 0.3452 - mae: 0.4361 - mse: 0.3452 - r2_score: -425.4335\n",
      "Epoch 490/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 910ms/step - loss: 0.3512 - mae: 0.4493 - mse: 0.3512 - r2_score: -422.4059\n",
      "Epoch 491/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 915ms/step - loss: 0.3650 - mae: 0.4559 - mse: 0.3650 - r2_score: -425.8604\n",
      "Epoch 492/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 903ms/step - loss: 0.3548 - mae: 0.4460 - mse: 0.3548 - r2_score: -419.4861\n",
      "Epoch 493/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 711ms/step - loss: 0.3499 - mae: 0.4434 - mse: 0.3499 - r2_score: -421.1557\n",
      "Epoch 494/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 827ms/step - loss: 0.3480 - mae: 0.4448 - mse: 0.3480 - r2_score: -421.0331\n",
      "Epoch 495/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 777ms/step - loss: 0.3686 - mae: 0.4568 - mse: 0.3686 - r2_score: -416.1944\n",
      "Epoch 496/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 727ms/step - loss: 0.3457 - mae: 0.4429 - mse: 0.3457 - r2_score: -427.3367\n",
      "Epoch 497/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 737ms/step - loss: 0.3478 - mae: 0.4466 - mse: 0.3478 - r2_score: -425.0905\n",
      "Epoch 498/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 721ms/step - loss: 0.4036 - mae: 0.4772 - mse: 0.4036 - r2_score: -427.5467\n",
      "Epoch 499/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 951ms/step - loss: 0.3702 - mae: 0.4599 - mse: 0.3702 - r2_score: -433.4489\n",
      "Epoch 500/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 914ms/step - loss: 0.3743 - mae: 0.4651 - mse: 0.3743 - r2_score: -420.7237\n",
      "Epoch 501/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 706ms/step - loss: 0.3420 - mae: 0.4440 - mse: 0.3420 - r2_score: -429.8578\n",
      "Epoch 502/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 926ms/step - loss: 0.3289 - mae: 0.4354 - mse: 0.3289 - r2_score: -425.8568\n",
      "Epoch 503/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 937ms/step - loss: 0.3616 - mae: 0.4504 - mse: 0.3616 - r2_score: -432.4511\n",
      "Epoch 504/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 899ms/step - loss: 0.3549 - mae: 0.4571 - mse: 0.3549 - r2_score: -426.7750\n",
      "Epoch 505/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 922ms/step - loss: 0.3539 - mae: 0.4482 - mse: 0.3539 - r2_score: -417.2409\n",
      "Epoch 506/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 917ms/step - loss: 0.3516 - mae: 0.4472 - mse: 0.3516 - r2_score: -422.8995\n",
      "Epoch 507/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 891ms/step - loss: 0.3652 - mae: 0.4527 - mse: 0.3652 - r2_score: -429.2646\n",
      "Epoch 508/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 821ms/step - loss: 0.3557 - mae: 0.4486 - mse: 0.3557 - r2_score: -424.1895\n",
      "Epoch 509/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 604ms/step - loss: 0.3505 - mae: 0.4475 - mse: 0.3505 - r2_score: -426.8382\n",
      "Epoch 510/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 582ms/step - loss: 0.3613 - mae: 0.4522 - mse: 0.3613 - r2_score: -424.0696\n",
      "Epoch 511/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 921ms/step - loss: 0.3568 - mae: 0.4512 - mse: 0.3568 - r2_score: -424.1596\n",
      "Epoch 512/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 929ms/step - loss: 0.3510 - mae: 0.4463 - mse: 0.3510 - r2_score: -427.3912\n",
      "Epoch 513/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 916ms/step - loss: 0.3540 - mae: 0.4518 - mse: 0.3540 - r2_score: -430.7007\n",
      "Epoch 514/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 940ms/step - loss: 0.3523 - mae: 0.4474 - mse: 0.3523 - r2_score: -422.7782\n",
      "Epoch 515/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 927ms/step - loss: 0.3728 - mae: 0.4564 - mse: 0.3728 - r2_score: -426.3117\n",
      "Epoch 516/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 929ms/step - loss: 0.3711 - mae: 0.4676 - mse: 0.3711 - r2_score: -426.3069\n",
      "Epoch 517/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 926ms/step - loss: 0.3507 - mae: 0.4469 - mse: 0.3507 - r2_score: -428.0974\n",
      "Epoch 518/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 923ms/step - loss: 0.3714 - mae: 0.4573 - mse: 0.3714 - r2_score: -424.5576\n",
      "Epoch 519/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 751ms/step - loss: 0.3717 - mae: 0.4657 - mse: 0.3717 - r2_score: -432.5054\n",
      "Epoch 520/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 716ms/step - loss: 0.3484 - mae: 0.4476 - mse: 0.3484 - r2_score: -428.3660\n",
      "Epoch 521/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 945ms/step - loss: 0.3516 - mae: 0.4515 - mse: 0.3516 - r2_score: -419.9138\n",
      "Epoch 522/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 900ms/step - loss: 0.3586 - mae: 0.4519 - mse: 0.3586 - r2_score: -426.6980\n",
      "Epoch 523/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 933ms/step - loss: 0.3612 - mae: 0.4488 - mse: 0.3612 - r2_score: -425.8688\n",
      "Epoch 524/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 891ms/step - loss: 0.3380 - mae: 0.4393 - mse: 0.3380 - r2_score: -428.9967\n",
      "Epoch 525/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 912ms/step - loss: 0.3641 - mae: 0.4595 - mse: 0.3641 - r2_score: -434.8568\n",
      "Epoch 526/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 918ms/step - loss: 0.3477 - mae: 0.4428 - mse: 0.3477 - r2_score: -420.2234\n",
      "Epoch 527/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 919ms/step - loss: 0.3569 - mae: 0.4457 - mse: 0.3569 - r2_score: -428.3649\n",
      "Epoch 528/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 919ms/step - loss: 0.3500 - mae: 0.4516 - mse: 0.3500 - r2_score: -424.9807\n",
      "Epoch 529/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 949ms/step - loss: 0.3565 - mae: 0.4484 - mse: 0.3565 - r2_score: -421.4966\n",
      "Epoch 530/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 922ms/step - loss: 0.3438 - mae: 0.4446 - mse: 0.3438 - r2_score: -428.9627\n",
      "Epoch 531/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 823ms/step - loss: 0.3155 - mae: 0.4295 - mse: 0.3155 - r2_score: -428.8889\n",
      "Epoch 532/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 953ms/step - loss: 0.3227 - mae: 0.4237 - mse: 0.3227 - r2_score: -431.2279\n",
      "Epoch 533/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 747ms/step - loss: 0.3468 - mae: 0.4409 - mse: 0.3468 - r2_score: -435.4357\n",
      "Epoch 534/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 785ms/step - loss: 0.3519 - mae: 0.4474 - mse: 0.3519 - r2_score: -425.7176\n",
      "Epoch 535/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 917ms/step - loss: 0.3451 - mae: 0.4433 - mse: 0.3451 - r2_score: -419.6914\n",
      "Epoch 536/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 918ms/step - loss: 0.3532 - mae: 0.4467 - mse: 0.3532 - r2_score: -429.6747\n",
      "Epoch 537/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 722ms/step - loss: 0.3257 - mae: 0.4305 - mse: 0.3257 - r2_score: -431.7023\n",
      "Epoch 538/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 965ms/step - loss: 0.3383 - mae: 0.4411 - mse: 0.3383 - r2_score: -423.5740\n",
      "Epoch 539/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 859ms/step - loss: 0.3268 - mae: 0.4292 - mse: 0.3268 - r2_score: -426.4662\n",
      "Epoch 540/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 626ms/step - loss: 0.3524 - mae: 0.4443 - mse: 0.3524 - r2_score: -431.9124\n",
      "Epoch 541/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 601ms/step - loss: 0.3655 - mae: 0.4448 - mse: 0.3655 - r2_score: -427.3558\n",
      "Epoch 542/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 953ms/step - loss: 0.3314 - mae: 0.4320 - mse: 0.3314 - r2_score: -427.4238\n",
      "Epoch 543/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 808ms/step - loss: 0.3390 - mae: 0.4466 - mse: 0.3390 - r2_score: -432.9207\n",
      "Epoch 544/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 592ms/step - loss: 0.3436 - mae: 0.4315 - mse: 0.3436 - r2_score: -428.3922\n",
      "Epoch 545/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 609ms/step - loss: 0.3383 - mae: 0.4313 - mse: 0.3383 - r2_score: -426.0055\n",
      "Epoch 546/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 905ms/step - loss: 0.3560 - mae: 0.4472 - mse: 0.3560 - r2_score: -429.0424\n",
      "Epoch 547/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 851ms/step - loss: 0.3614 - mae: 0.4530 - mse: 0.3614 - r2_score: -424.1117\n",
      "Epoch 548/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 451ms/step - loss: 0.3206 - mae: 0.4278 - mse: 0.3206 - r2_score: -431.2351\n",
      "Epoch 549/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 938ms/step - loss: 0.3413 - mae: 0.4442 - mse: 0.3413 - r2_score: -425.8908\n",
      "Epoch 550/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 765ms/step - loss: 0.3138 - mae: 0.4225 - mse: 0.3138 - r2_score: -430.4494\n",
      "Epoch 551/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 660ms/step - loss: 0.3345 - mae: 0.4391 - mse: 0.3345 - r2_score: -428.9977\n",
      "Epoch 552/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 962ms/step - loss: 0.3459 - mae: 0.4362 - mse: 0.3459 - r2_score: -422.7388\n",
      "Epoch 553/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 952ms/step - loss: 0.3497 - mae: 0.4367 - mse: 0.3497 - r2_score: -420.6491\n",
      "Epoch 554/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 879ms/step - loss: 0.3407 - mae: 0.4312 - mse: 0.3407 - r2_score: -435.8921\n",
      "Epoch 555/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 949ms/step - loss: 0.3529 - mae: 0.4477 - mse: 0.3529 - r2_score: -422.4253\n",
      "Epoch 556/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 751ms/step - loss: 0.3270 - mae: 0.4245 - mse: 0.3270 - r2_score: -427.7419\n",
      "Epoch 557/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 960ms/step - loss: 0.3316 - mae: 0.4280 - mse: 0.3316 - r2_score: -429.2031\n",
      "Epoch 558/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 919ms/step - loss: 0.3286 - mae: 0.4307 - mse: 0.3286 - r2_score: -427.5970\n",
      "Epoch 559/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 992ms/step - loss: 0.3306 - mae: 0.4309 - mse: 0.3306 - r2_score: -424.9211\n",
      "Epoch 560/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 949ms/step - loss: 0.3355 - mae: 0.4339 - mse: 0.3355 - r2_score: -429.8632\n",
      "Epoch 561/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 933ms/step - loss: 0.3365 - mae: 0.4328 - mse: 0.3365 - r2_score: -433.3082\n",
      "Epoch 562/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 958ms/step - loss: 0.3423 - mae: 0.4486 - mse: 0.3423 - r2_score: -432.2792\n",
      "Epoch 563/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 729ms/step - loss: 0.3469 - mae: 0.4446 - mse: 0.3469 - r2_score: -422.9705\n",
      "Epoch 564/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 869ms/step - loss: 0.3422 - mae: 0.4402 - mse: 0.3422 - r2_score: -427.4391\n",
      "Epoch 565/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 924ms/step - loss: 0.3451 - mae: 0.4421 - mse: 0.3451 - r2_score: -432.8859\n",
      "Epoch 566/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 923ms/step - loss: 0.3292 - mae: 0.4346 - mse: 0.3292 - r2_score: -424.3048\n",
      "Epoch 567/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 929ms/step - loss: 0.3444 - mae: 0.4328 - mse: 0.3444 - r2_score: -431.9309\n",
      "Epoch 568/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 695ms/step - loss: 0.3283 - mae: 0.4310 - mse: 0.3283 - r2_score: -426.1930\n",
      "Epoch 569/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 892ms/step - loss: 0.3373 - mae: 0.4324 - mse: 0.3373 - r2_score: -427.7697\n",
      "Epoch 570/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 908ms/step - loss: 0.3380 - mae: 0.4317 - mse: 0.3380 - r2_score: -428.1406\n",
      "Epoch 571/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 904ms/step - loss: 0.3696 - mae: 0.4479 - mse: 0.3696 - r2_score: -422.5494\n",
      "Epoch 572/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 745ms/step - loss: 0.3410 - mae: 0.4371 - mse: 0.3410 - r2_score: -439.0014\n",
      "Epoch 573/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 972ms/step - loss: 0.3390 - mae: 0.4347 - mse: 0.3390 - r2_score: -428.3661\n",
      "Epoch 574/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 574ms/step - loss: 0.3237 - mae: 0.4257 - mse: 0.3237 - r2_score: -426.1579\n",
      "Epoch 575/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 334ms/step - loss: 0.3289 - mae: 0.4341 - mse: 0.3289 - r2_score: -431.6638\n",
      "Epoch 576/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 496ms/step - loss: 0.3402 - mae: 0.4398 - mse: 0.3402 - r2_score: -428.3569\n",
      "Epoch 577/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 558ms/step - loss: 0.3387 - mae: 0.4351 - mse: 0.3387 - r2_score: -429.9979\n",
      "Epoch 578/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 910ms/step - loss: 0.3180 - mae: 0.4257 - mse: 0.3180 - r2_score: -428.5750\n",
      "Epoch 579/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 911ms/step - loss: 0.3281 - mae: 0.4297 - mse: 0.3281 - r2_score: -429.5594\n",
      "Epoch 580/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 819ms/step - loss: 0.3368 - mae: 0.4304 - mse: 0.3368 - r2_score: -427.6777\n",
      "Epoch 581/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 864ms/step - loss: 0.3249 - mae: 0.4284 - mse: 0.3249 - r2_score: -427.4062\n",
      "Epoch 582/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 891ms/step - loss: 0.3220 - mae: 0.4287 - mse: 0.3220 - r2_score: -432.4247\n",
      "Epoch 583/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 905ms/step - loss: 0.3292 - mae: 0.4254 - mse: 0.3292 - r2_score: -429.9892\n",
      "Epoch 584/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 967ms/step - loss: 0.3283 - mae: 0.4272 - mse: 0.3283 - r2_score: -432.8214\n",
      "Epoch 585/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 945ms/step - loss: 0.3233 - mae: 0.4332 - mse: 0.3233 - r2_score: -430.5780\n",
      "Epoch 586/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 988ms/step - loss: 0.3250 - mae: 0.4262 - mse: 0.3250 - r2_score: -429.8976\n",
      "Epoch 587/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 826ms/step - loss: 0.3293 - mae: 0.4283 - mse: 0.3293 - r2_score: -429.6764\n",
      "Epoch 588/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 855ms/step - loss: 0.3160 - mae: 0.4232 - mse: 0.3160 - r2_score: -438.0613\n",
      "Epoch 589/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 781ms/step - loss: 0.3298 - mae: 0.4384 - mse: 0.3298 - r2_score: -436.7854\n",
      "Epoch 590/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 939ms/step - loss: 0.3187 - mae: 0.4258 - mse: 0.3187 - r2_score: -427.7416\n",
      "Epoch 591/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 935ms/step - loss: 0.3204 - mae: 0.4273 - mse: 0.3204 - r2_score: -432.7383\n",
      "Epoch 592/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 946ms/step - loss: 0.3484 - mae: 0.4373 - mse: 0.3484 - r2_score: -438.5502\n",
      "Epoch 593/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 750ms/step - loss: 0.3473 - mae: 0.4407 - mse: 0.3473 - r2_score: -429.7215\n",
      "Epoch 594/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 724ms/step - loss: 0.3307 - mae: 0.4305 - mse: 0.3307 - r2_score: -435.2766\n",
      "Epoch 595/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 689ms/step - loss: 0.3342 - mae: 0.4368 - mse: 0.3342 - r2_score: -435.6020\n",
      "Epoch 596/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 964ms/step - loss: 0.3281 - mae: 0.4284 - mse: 0.3281 - r2_score: -433.9041\n",
      "Epoch 597/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 914ms/step - loss: 0.3472 - mae: 0.4407 - mse: 0.3472 - r2_score: -433.3679\n",
      "Epoch 598/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 728ms/step - loss: 0.3240 - mae: 0.4243 - mse: 0.3240 - r2_score: -425.0186\n",
      "Epoch 599/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 874ms/step - loss: 0.3341 - mae: 0.4300 - mse: 0.3341 - r2_score: -428.8114\n",
      "Epoch 600/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 955ms/step - loss: 0.3276 - mae: 0.4241 - mse: 0.3276 - r2_score: -434.4590\n",
      "Epoch 601/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 956ms/step - loss: 0.3264 - mae: 0.4275 - mse: 0.3264 - r2_score: -434.2319\n",
      "Epoch 602/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 912ms/step - loss: 0.3227 - mae: 0.4295 - mse: 0.3227 - r2_score: -431.4765\n",
      "Epoch 603/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 901ms/step - loss: 0.3288 - mae: 0.4293 - mse: 0.3288 - r2_score: -434.0639\n",
      "Epoch 604/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 682ms/step - loss: 0.3100 - mae: 0.4204 - mse: 0.3100 - r2_score: -428.2782\n",
      "Epoch 605/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 935ms/step - loss: 0.3352 - mae: 0.4355 - mse: 0.3352 - r2_score: -429.0638\n",
      "Epoch 606/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 973ms/step - loss: 0.3232 - mae: 0.4228 - mse: 0.3232 - r2_score: -430.7155\n",
      "Epoch 607/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 874ms/step - loss: 0.3076 - mae: 0.4127 - mse: 0.3076 - r2_score: -442.9713\n",
      "Epoch 608/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 933ms/step - loss: 0.3344 - mae: 0.4356 - mse: 0.3344 - r2_score: -429.1251\n",
      "Epoch 609/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 839ms/step - loss: 0.3208 - mae: 0.4284 - mse: 0.3208 - r2_score: -423.1501\n",
      "Epoch 610/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 925ms/step - loss: 0.3238 - mae: 0.4273 - mse: 0.3238 - r2_score: -432.9802\n",
      "Epoch 611/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 935ms/step - loss: 0.3281 - mae: 0.4329 - mse: 0.3281 - r2_score: -428.3765\n",
      "Epoch 612/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 944ms/step - loss: 0.3285 - mae: 0.4341 - mse: 0.3285 - r2_score: -435.9636\n",
      "Epoch 613/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 892ms/step - loss: 0.3331 - mae: 0.4273 - mse: 0.3331 - r2_score: -434.9555\n",
      "Epoch 614/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 934ms/step - loss: 0.3315 - mae: 0.4273 - mse: 0.3315 - r2_score: -434.0905\n",
      "Epoch 615/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 925ms/step - loss: 0.3230 - mae: 0.4259 - mse: 0.3230 - r2_score: -430.4437\n",
      "Epoch 616/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 870ms/step - loss: 0.3223 - mae: 0.4235 - mse: 0.3223 - r2_score: -430.9537\n",
      "Epoch 617/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 616ms/step - loss: 0.3215 - mae: 0.4236 - mse: 0.3215 - r2_score: -433.8255\n",
      "Epoch 618/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 597ms/step - loss: 0.3232 - mae: 0.4183 - mse: 0.3232 - r2_score: -436.1310\n",
      "Epoch 619/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 938ms/step - loss: 0.3067 - mae: 0.4156 - mse: 0.3067 - r2_score: -437.5820\n",
      "Epoch 620/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 800ms/step - loss: 0.3054 - mae: 0.4184 - mse: 0.3054 - r2_score: -431.4121\n",
      "Epoch 621/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 583ms/step - loss: 0.3232 - mae: 0.4235 - mse: 0.3232 - r2_score: -437.9468\n",
      "Epoch 622/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - loss: 0.3390 - mae: 0.4319 - mse: 0.3390 - r2_score: -442.8430\n",
      "Epoch 623/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 966ms/step - loss: 0.3380 - mae: 0.4378 - mse: 0.3380 - r2_score: -434.9497\n",
      "Epoch 624/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 560ms/step - loss: 0.3157 - mae: 0.4192 - mse: 0.3157 - r2_score: -437.5536\n",
      "Epoch 625/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 951ms/step - loss: 0.2969 - mae: 0.4089 - mse: 0.2969 - r2_score: -433.4058\n",
      "Epoch 626/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 878ms/step - loss: 0.3096 - mae: 0.4235 - mse: 0.3096 - r2_score: -429.4614\n",
      "Epoch 627/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 917ms/step - loss: 0.3040 - mae: 0.4108 - mse: 0.3040 - r2_score: -435.9438\n",
      "Epoch 628/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 941ms/step - loss: 0.3030 - mae: 0.4156 - mse: 0.3030 - r2_score: -431.0398\n",
      "Epoch 629/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 694ms/step - loss: 0.3162 - mae: 0.4225 - mse: 0.3162 - r2_score: -432.8788\n",
      "Epoch 630/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 930ms/step - loss: 0.3148 - mae: 0.4158 - mse: 0.3148 - r2_score: -433.3404\n",
      "Epoch 631/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 916ms/step - loss: 0.2990 - mae: 0.4120 - mse: 0.2990 - r2_score: -435.1821\n",
      "Epoch 632/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 928ms/step - loss: 0.2960 - mae: 0.4062 - mse: 0.2960 - r2_score: -444.3910\n",
      "Epoch 633/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 939ms/step - loss: 0.3264 - mae: 0.4250 - mse: 0.3264 - r2_score: -437.8142\n",
      "Epoch 634/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 944ms/step - loss: 0.3082 - mae: 0.4172 - mse: 0.3082 - r2_score: -436.7108\n",
      "Epoch 635/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 939ms/step - loss: 0.3158 - mae: 0.4235 - mse: 0.3158 - r2_score: -429.3250\n",
      "Epoch 636/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 963ms/step - loss: 0.3120 - mae: 0.4177 - mse: 0.3120 - r2_score: -440.4196\n",
      "Epoch 637/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 986ms/step - loss: 0.3160 - mae: 0.4229 - mse: 0.3160 - r2_score: -435.5741\n",
      "Epoch 638/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 583ms/step - loss: 0.3336 - mae: 0.4282 - mse: 0.3336 - r2_score: -431.3375\n",
      "Epoch 639/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 699ms/step - loss: 0.3119 - mae: 0.4178 - mse: 0.3119 - r2_score: -438.2638\n",
      "Epoch 640/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 945ms/step - loss: 0.3136 - mae: 0.4129 - mse: 0.3136 - r2_score: -431.2829\n",
      "Epoch 641/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 894ms/step - loss: 0.2962 - mae: 0.4060 - mse: 0.2962 - r2_score: -439.1309\n",
      "Epoch 642/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 818ms/step - loss: 0.3096 - mae: 0.4146 - mse: 0.3096 - r2_score: -436.0357\n",
      "Epoch 643/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 925ms/step - loss: 0.3152 - mae: 0.4209 - mse: 0.3152 - r2_score: -433.2306\n",
      "Epoch 644/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 880ms/step - loss: 0.2916 - mae: 0.4099 - mse: 0.2916 - r2_score: -440.1884\n",
      "Epoch 645/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 818ms/step - loss: 0.3034 - mae: 0.4180 - mse: 0.3034 - r2_score: -438.5162\n",
      "Epoch 646/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 770ms/step - loss: 0.3130 - mae: 0.4173 - mse: 0.3130 - r2_score: -438.7865\n",
      "Epoch 647/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 959ms/step - loss: 0.2990 - mae: 0.4126 - mse: 0.2990 - r2_score: -442.4948\n",
      "Epoch 648/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 667ms/step - loss: 0.3116 - mae: 0.4139 - mse: 0.3116 - r2_score: -432.4156\n",
      "Epoch 649/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 628ms/step - loss: 0.3105 - mae: 0.4164 - mse: 0.3105 - r2_score: -430.1401\n",
      "Epoch 650/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 879ms/step - loss: 0.3132 - mae: 0.4217 - mse: 0.3132 - r2_score: -439.3449\n",
      "Epoch 651/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 985ms/step - loss: 0.3255 - mae: 0.4304 - mse: 0.3255 - r2_score: -438.3637\n",
      "Epoch 652/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 962ms/step - loss: 0.3147 - mae: 0.4203 - mse: 0.3147 - r2_score: -431.1500\n",
      "Epoch 653/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 965ms/step - loss: 0.2987 - mae: 0.4096 - mse: 0.2987 - r2_score: -433.0766\n",
      "Epoch 654/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 755ms/step - loss: 0.3278 - mae: 0.4277 - mse: 0.3278 - r2_score: -440.3040\n",
      "Epoch 655/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 599ms/step - loss: 0.3276 - mae: 0.4261 - mse: 0.3276 - r2_score: -438.4873\n",
      "Epoch 656/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 562ms/step - loss: 0.3198 - mae: 0.4274 - mse: 0.3198 - r2_score: -436.3386\n",
      "Epoch 657/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 964ms/step - loss: 0.3041 - mae: 0.4146 - mse: 0.3041 - r2_score: -437.3453\n",
      "Epoch 658/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 949ms/step - loss: 0.3112 - mae: 0.4185 - mse: 0.3112 - r2_score: -436.4864\n",
      "Epoch 659/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 948ms/step - loss: 0.3219 - mae: 0.4316 - mse: 0.3219 - r2_score: -428.0644\n",
      "Epoch 660/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 941ms/step - loss: 0.3268 - mae: 0.4294 - mse: 0.3268 - r2_score: -437.6412\n",
      "Epoch 661/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 963ms/step - loss: 0.2806 - mae: 0.4010 - mse: 0.2806 - r2_score: -439.4416\n",
      "Epoch 662/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 767ms/step - loss: 0.3082 - mae: 0.4168 - mse: 0.3082 - r2_score: -436.3050\n",
      "Epoch 663/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 883ms/step - loss: 0.3026 - mae: 0.4107 - mse: 0.3026 - r2_score: -438.0503\n",
      "Epoch 664/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 967ms/step - loss: 0.3048 - mae: 0.4104 - mse: 0.3048 - r2_score: -437.6164\n",
      "Epoch 665/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 895ms/step - loss: 0.3149 - mae: 0.4226 - mse: 0.3149 - r2_score: -435.9243\n",
      "Epoch 666/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 855ms/step - loss: 0.2883 - mae: 0.4008 - mse: 0.2883 - r2_score: -435.1971\n",
      "Epoch 667/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 382ms/step - loss: 0.3013 - mae: 0.4106 - mse: 0.3013 - r2_score: -436.1912\n",
      "Epoch 668/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 582ms/step - loss: 0.3007 - mae: 0.4107 - mse: 0.3007 - r2_score: -434.5094\n",
      "Epoch 669/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 937ms/step - loss: 0.2921 - mae: 0.4067 - mse: 0.2921 - r2_score: -441.5561\n",
      "Epoch 670/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 787ms/step - loss: 0.2980 - mae: 0.4091 - mse: 0.2980 - r2_score: -439.3519\n",
      "Epoch 671/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 823ms/step - loss: 0.2959 - mae: 0.4058 - mse: 0.2959 - r2_score: -439.3218\n",
      "Epoch 672/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 939ms/step - loss: 0.3112 - mae: 0.4170 - mse: 0.3112 - r2_score: -434.1144\n",
      "Epoch 673/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 939ms/step - loss: 0.3050 - mae: 0.4104 - mse: 0.3050 - r2_score: -437.3540\n",
      "Epoch 674/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - loss: 0.3134 - mae: 0.4208 - mse: 0.3134 - r2_score: -446.6127  \n",
      "Epoch 675/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 801ms/step - loss: 0.3023 - mae: 0.4106 - mse: 0.3023 - r2_score: -441.5836\n",
      "Epoch 676/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 847ms/step - loss: 0.3055 - mae: 0.4135 - mse: 0.3055 - r2_score: -438.7758\n",
      "Epoch 677/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 949ms/step - loss: 0.3010 - mae: 0.4076 - mse: 0.3010 - r2_score: -430.9214\n",
      "Epoch 678/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 597ms/step - loss: 0.3070 - mae: 0.4129 - mse: 0.3070 - r2_score: -437.3572\n",
      "Epoch 679/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 597ms/step - loss: 0.3140 - mae: 0.4155 - mse: 0.3140 - r2_score: -437.0273\n",
      "Epoch 680/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 843ms/step - loss: 0.3167 - mae: 0.4172 - mse: 0.3167 - r2_score: -437.0090\n",
      "Epoch 681/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 703ms/step - loss: 0.3153 - mae: 0.4219 - mse: 0.3153 - r2_score: -431.9124\n",
      "Epoch 682/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 967ms/step - loss: 0.3187 - mae: 0.4161 - mse: 0.3187 - r2_score: -438.6853\n",
      "Epoch 683/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 955ms/step - loss: 0.3112 - mae: 0.4191 - mse: 0.3112 - r2_score: -432.5607\n",
      "Epoch 684/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 906ms/step - loss: 0.2944 - mae: 0.4093 - mse: 0.2944 - r2_score: -440.0427\n",
      "Epoch 685/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 944ms/step - loss: 0.2896 - mae: 0.4036 - mse: 0.2896 - r2_score: -437.0426\n",
      "Epoch 686/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 964ms/step - loss: 0.2993 - mae: 0.4114 - mse: 0.2993 - r2_score: -435.3466\n",
      "Epoch 687/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 940ms/step - loss: 0.3098 - mae: 0.4197 - mse: 0.3098 - r2_score: -437.0315\n",
      "Epoch 688/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 787ms/step - loss: 0.2956 - mae: 0.4051 - mse: 0.2956 - r2_score: -437.7158\n",
      "Epoch 689/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 868ms/step - loss: 0.2922 - mae: 0.4059 - mse: 0.2922 - r2_score: -436.2629\n",
      "Epoch 690/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 885ms/step - loss: 0.3041 - mae: 0.4153 - mse: 0.3041 - r2_score: -444.7916\n",
      "Epoch 691/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 906ms/step - loss: 0.3012 - mae: 0.4083 - mse: 0.3012 - r2_score: -441.1631\n",
      "Epoch 692/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 771ms/step - loss: 0.3045 - mae: 0.4132 - mse: 0.3045 - r2_score: -431.3897\n",
      "Epoch 693/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 880ms/step - loss: 0.2887 - mae: 0.4107 - mse: 0.2887 - r2_score: -447.3542\n",
      "Epoch 694/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 920ms/step - loss: 0.2986 - mae: 0.4108 - mse: 0.2986 - r2_score: -436.0369\n",
      "Epoch 695/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 923ms/step - loss: 0.2970 - mae: 0.4047 - mse: 0.2970 - r2_score: -443.5516\n",
      "Epoch 696/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 772ms/step - loss: 0.2911 - mae: 0.3954 - mse: 0.2911 - r2_score: -441.1247\n",
      "Epoch 697/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 894ms/step - loss: 0.3074 - mae: 0.4183 - mse: 0.3074 - r2_score: -432.4988\n",
      "Epoch 698/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 969ms/step - loss: 0.3129 - mae: 0.4178 - mse: 0.3129 - r2_score: -442.5781\n",
      "Epoch 699/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 907ms/step - loss: 0.2967 - mae: 0.4116 - mse: 0.2967 - r2_score: -432.9567\n",
      "Epoch 700/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 926ms/step - loss: 0.2942 - mae: 0.4047 - mse: 0.2942 - r2_score: -436.6283\n",
      "Epoch 701/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 925ms/step - loss: 0.2915 - mae: 0.4055 - mse: 0.2915 - r2_score: -443.4111\n",
      "Epoch 702/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 676ms/step - loss: 0.3021 - mae: 0.4138 - mse: 0.3021 - r2_score: -443.4955\n",
      "Epoch 703/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 942ms/step - loss: 0.3138 - mae: 0.4189 - mse: 0.3138 - r2_score: -438.9692\n",
      "Epoch 704/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 954ms/step - loss: 0.3082 - mae: 0.4174 - mse: 0.3082 - r2_score: -436.6106\n",
      "Epoch 705/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 932ms/step - loss: 0.2856 - mae: 0.4047 - mse: 0.2856 - r2_score: -440.7261\n",
      "Epoch 706/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 944ms/step - loss: 0.3119 - mae: 0.4171 - mse: 0.3119 - r2_score: -437.4764\n",
      "Epoch 707/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 912ms/step - loss: 0.2960 - mae: 0.4044 - mse: 0.2960 - r2_score: -443.7038\n",
      "Epoch 708/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 696ms/step - loss: 0.2998 - mae: 0.4146 - mse: 0.2998 - r2_score: -441.2518\n",
      "Epoch 709/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 950ms/step - loss: 0.3074 - mae: 0.4204 - mse: 0.3074 - r2_score: -435.4511\n",
      "Epoch 710/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 728ms/step - loss: 0.2921 - mae: 0.4070 - mse: 0.2921 - r2_score: -441.6490\n",
      "Epoch 711/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 537ms/step - loss: 0.3099 - mae: 0.4199 - mse: 0.3099 - r2_score: -434.8181\n",
      "Epoch 712/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 963ms/step - loss: 0.2842 - mae: 0.3990 - mse: 0.2842 - r2_score: -439.9908\n",
      "Epoch 713/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 898ms/step - loss: 0.3086 - mae: 0.4160 - mse: 0.3086 - r2_score: -430.6368\n",
      "Epoch 714/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 662ms/step - loss: 0.3009 - mae: 0.4045 - mse: 0.3009 - r2_score: -434.5760\n",
      "Epoch 715/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 913ms/step - loss: 0.2891 - mae: 0.3991 - mse: 0.2891 - r2_score: -441.3078\n",
      "Epoch 716/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 919ms/step - loss: 0.2905 - mae: 0.4023 - mse: 0.2905 - r2_score: -437.6083\n",
      "Epoch 717/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 930ms/step - loss: 0.3018 - mae: 0.4048 - mse: 0.3018 - r2_score: -441.8070\n",
      "Epoch 718/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 793ms/step - loss: 0.2966 - mae: 0.4064 - mse: 0.2966 - r2_score: -432.0829\n",
      "Epoch 719/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 805ms/step - loss: 0.2904 - mae: 0.4040 - mse: 0.2904 - r2_score: -442.3634\n",
      "Epoch 720/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 881ms/step - loss: 0.2876 - mae: 0.3965 - mse: 0.2876 - r2_score: -442.2010\n",
      "Epoch 721/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 913ms/step - loss: 0.2860 - mae: 0.3967 - mse: 0.2860 - r2_score: -440.4191\n",
      "Epoch 722/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 921ms/step - loss: 0.2973 - mae: 0.4078 - mse: 0.2973 - r2_score: -445.1813\n",
      "Epoch 723/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 971ms/step - loss: 0.2914 - mae: 0.4094 - mse: 0.2914 - r2_score: -442.9100\n",
      "Epoch 724/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 899ms/step - loss: 0.3014 - mae: 0.4156 - mse: 0.3014 - r2_score: -444.3755\n",
      "Epoch 725/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 941ms/step - loss: 0.2824 - mae: 0.3988 - mse: 0.2824 - r2_score: -439.6360\n",
      "Epoch 726/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 966ms/step - loss: 0.3074 - mae: 0.4159 - mse: 0.3074 - r2_score: -429.9548\n",
      "Epoch 727/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 935ms/step - loss: 0.2892 - mae: 0.4001 - mse: 0.2892 - r2_score: -439.5651\n",
      "Epoch 728/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 925ms/step - loss: 0.2890 - mae: 0.4035 - mse: 0.2890 - r2_score: -436.4527\n",
      "Epoch 729/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 961ms/step - loss: 0.2800 - mae: 0.3928 - mse: 0.2800 - r2_score: -448.3837\n",
      "Epoch 730/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 875ms/step - loss: 0.2971 - mae: 0.4062 - mse: 0.2971 - r2_score: -428.9923\n",
      "Epoch 731/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 924ms/step - loss: 0.2862 - mae: 0.3990 - mse: 0.2862 - r2_score: -441.4607\n",
      "Epoch 732/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 928ms/step - loss: 0.2968 - mae: 0.4051 - mse: 0.2968 - r2_score: -429.9217\n",
      "Epoch 733/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 595ms/step - loss: 0.2724 - mae: 0.3924 - mse: 0.2724 - r2_score: -445.2971\n",
      "Epoch 734/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 611ms/step - loss: 0.2905 - mae: 0.4059 - mse: 0.2905 - r2_score: -433.5580\n",
      "Epoch 735/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 876ms/step - loss: 0.2729 - mae: 0.3974 - mse: 0.2729 - r2_score: -450.6852\n",
      "Epoch 736/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 836ms/step - loss: 0.3101 - mae: 0.4158 - mse: 0.3101 - r2_score: -435.6030\n",
      "Epoch 737/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 644ms/step - loss: 0.3059 - mae: 0.4161 - mse: 0.3059 - r2_score: -441.7151\n",
      "Epoch 738/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 821ms/step - loss: 0.2940 - mae: 0.4036 - mse: 0.2940 - r2_score: -437.8360\n",
      "Epoch 739/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 964ms/step - loss: 0.2797 - mae: 0.3995 - mse: 0.2797 - r2_score: -441.8857\n",
      "Epoch 740/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 975ms/step - loss: 0.2920 - mae: 0.4014 - mse: 0.2920 - r2_score: -442.5480\n",
      "Epoch 741/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 965ms/step - loss: 0.2946 - mae: 0.4037 - mse: 0.2946 - r2_score: -435.9015\n",
      "Epoch 742/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 917ms/step - loss: 0.2935 - mae: 0.3955 - mse: 0.2935 - r2_score: -441.2832\n",
      "Epoch 743/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 940ms/step - loss: 0.2882 - mae: 0.4037 - mse: 0.2882 - r2_score: -434.6274\n",
      "Epoch 744/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 554ms/step - loss: 0.2842 - mae: 0.3964 - mse: 0.2842 - r2_score: -439.5747\n",
      "Epoch 745/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 368ms/step - loss: 0.2969 - mae: 0.4079 - mse: 0.2969 - r2_score: -440.8621\n",
      "Epoch 746/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 595ms/step - loss: 0.2829 - mae: 0.3985 - mse: 0.2829 - r2_score: -453.0038\n",
      "Epoch 747/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 786ms/step - loss: 0.2826 - mae: 0.3949 - mse: 0.2826 - r2_score: -444.7109\n",
      "Epoch 748/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 887ms/step - loss: 0.2786 - mae: 0.3956 - mse: 0.2786 - r2_score: -438.2870\n",
      "Epoch 749/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 811ms/step - loss: 0.2974 - mae: 0.3992 - mse: 0.2974 - r2_score: -434.9895\n",
      "Epoch 750/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 808ms/step - loss: 0.2831 - mae: 0.3957 - mse: 0.2831 - r2_score: -442.7471\n",
      "Epoch 751/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 947ms/step - loss: 0.2817 - mae: 0.3953 - mse: 0.2817 - r2_score: -441.6845\n",
      "Epoch 752/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 921ms/step - loss: 0.2977 - mae: 0.4041 - mse: 0.2977 - r2_score: -444.8337\n",
      "Epoch 753/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 922ms/step - loss: 0.2629 - mae: 0.3854 - mse: 0.2629 - r2_score: -440.7615\n",
      "Epoch 754/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 747ms/step - loss: 0.2834 - mae: 0.3984 - mse: 0.2834 - r2_score: -443.9969\n",
      "Epoch 755/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 971ms/step - loss: 0.2921 - mae: 0.4044 - mse: 0.2921 - r2_score: -437.3046\n",
      "Epoch 756/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 935ms/step - loss: 0.2724 - mae: 0.3866 - mse: 0.2724 - r2_score: -445.8590\n",
      "Epoch 757/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 857ms/step - loss: 0.2981 - mae: 0.4082 - mse: 0.2981 - r2_score: -440.6128\n",
      "Epoch 758/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 839ms/step - loss: 0.3034 - mae: 0.4146 - mse: 0.3034 - r2_score: -438.1367\n",
      "Epoch 759/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 934ms/step - loss: 0.3121 - mae: 0.4164 - mse: 0.3121 - r2_score: -445.2048\n",
      "Epoch 760/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 949ms/step - loss: 0.2899 - mae: 0.4035 - mse: 0.2899 - r2_score: -436.6701\n",
      "Epoch 761/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 720ms/step - loss: 0.2732 - mae: 0.3890 - mse: 0.2732 - r2_score: -440.7470\n",
      "Epoch 762/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 777ms/step - loss: 0.2902 - mae: 0.4003 - mse: 0.2902 - r2_score: -443.2163\n",
      "Epoch 763/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 937ms/step - loss: 0.2899 - mae: 0.4085 - mse: 0.2899 - r2_score: -435.8229\n",
      "Epoch 764/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 957ms/step - loss: 0.3058 - mae: 0.4139 - mse: 0.3058 - r2_score: -439.4016\n",
      "Epoch 765/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 930ms/step - loss: 0.2918 - mae: 0.4029 - mse: 0.2918 - r2_score: -441.8799\n",
      "Epoch 766/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 916ms/step - loss: 0.2803 - mae: 0.3982 - mse: 0.2803 - r2_score: -440.6595\n",
      "Epoch 767/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 705ms/step - loss: 0.2769 - mae: 0.3921 - mse: 0.2769 - r2_score: -442.0826\n",
      "Epoch 768/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 926ms/step - loss: 0.2987 - mae: 0.4102 - mse: 0.2987 - r2_score: -440.9533\n",
      "Epoch 769/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 941ms/step - loss: 0.2842 - mae: 0.3980 - mse: 0.2842 - r2_score: -444.3560\n",
      "Epoch 770/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 761ms/step - loss: 0.2817 - mae: 0.3949 - mse: 0.2817 - r2_score: -434.1594\n",
      "Epoch 771/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 942ms/step - loss: 0.2888 - mae: 0.3956 - mse: 0.2888 - r2_score: -441.8695\n",
      "Epoch 772/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 968ms/step - loss: 0.2821 - mae: 0.3954 - mse: 0.2821 - r2_score: -443.3531\n",
      "Epoch 773/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 957ms/step - loss: 0.2945 - mae: 0.4029 - mse: 0.2945 - r2_score: -447.5689\n",
      "Epoch 774/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 919ms/step - loss: 0.2746 - mae: 0.4008 - mse: 0.2746 - r2_score: -439.3747\n",
      "Epoch 775/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 931ms/step - loss: 0.2801 - mae: 0.4008 - mse: 0.2801 - r2_score: -449.4230\n",
      "Epoch 776/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 939ms/step - loss: 0.2719 - mae: 0.3914 - mse: 0.2719 - r2_score: -441.6302\n",
      "Epoch 777/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 925ms/step - loss: 0.2878 - mae: 0.3984 - mse: 0.2878 - r2_score: -444.2290\n",
      "Epoch 778/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 792ms/step - loss: 0.2849 - mae: 0.3957 - mse: 0.2849 - r2_score: -452.0249\n",
      "Epoch 779/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 837ms/step - loss: 0.2857 - mae: 0.3996 - mse: 0.2857 - r2_score: -439.6461\n",
      "Epoch 780/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 827ms/step - loss: 0.2939 - mae: 0.3955 - mse: 0.2939 - r2_score: -438.6877\n",
      "Epoch 781/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 973ms/step - loss: 0.2876 - mae: 0.4009 - mse: 0.2876 - r2_score: -441.9264\n",
      "Epoch 782/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 972ms/step - loss: 0.2848 - mae: 0.3968 - mse: 0.2848 - r2_score: -440.8681\n",
      "Epoch 783/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 828ms/step - loss: 0.2676 - mae: 0.3852 - mse: 0.2676 - r2_score: -447.9484\n",
      "Epoch 783: early stopping\n",
      "Restoring model weights from the end of the best epoch: 753.\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - loss: 2.2723 - mae: 1.1511 - mse: 2.2723 - r2_score: -63.8293\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Epoch 1/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - loss: 3.6776 - mae: 1.4744 - mse: 3.2883 - r2_score: -769.6963\n",
      "Epoch 2/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 810ms/step - loss: 3.3886 - mae: 1.5267 - mse: 3.3886 - r2_score: -728.1997\n",
      "Epoch 3/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - loss: 3.4377 - mae: 1.5335 - mse: 3.4377 - r2_score: -728.2183\n",
      "Epoch 4/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.3350 - mae: 1.5126 - mse: 3.3350 - r2_score: -742.2749\n",
      "Epoch 5/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 3.3262 - mae: 1.5088 - mse: 3.3262 - r2_score: -728.5213\n",
      "Epoch 6/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.4620 - mae: 1.5324 - mse: 3.4620 - r2_score: -714.1641\n",
      "Epoch 7/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.4269 - mae: 1.5274 - mse: 3.4269 - r2_score: -715.6182\n",
      "Epoch 8/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 3.3616 - mae: 1.5186 - mse: 3.3616 - r2_score: -741.5502\n",
      "Epoch 9/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.4414 - mae: 1.5325 - mse: 3.4414 - r2_score: -729.2180\n",
      "Epoch 10/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - loss: 3.3654 - mae: 1.5259 - mse: 3.3654 - r2_score: -753.3119\n",
      "Epoch 11/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.4008 - mae: 1.5287 - mse: 3.4008 - r2_score: -728.3113\n",
      "Epoch 12/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.4220 - mae: 1.5309 - mse: 3.4220 - r2_score: -728.0673\n",
      "Epoch 13/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - loss: 3.3984 - mae: 1.5374 - mse: 3.3984 - r2_score: -754.1449\n",
      "Epoch 14/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.4003 - mae: 1.5284 - mse: 3.4003 - r2_score: -735.0801\n",
      "Epoch 15/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - loss: 3.3502 - mae: 1.5202 - mse: 3.3502 - r2_score: -733.3611\n",
      "Epoch 16/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 847ms/step - loss: 3.4204 - mae: 1.5253 - mse: 3.4204 - r2_score: -718.5867\n",
      "Epoch 17/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - loss: 3.3745 - mae: 1.5224 - mse: 3.3745 - r2_score: -743.7742\n",
      "Epoch 18/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - loss: 3.3623 - mae: 1.5190 - mse: 3.3623 - r2_score: -739.3491\n",
      "Epoch 19/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - loss: 3.4271 - mae: 1.5435 - mse: 3.4271 - r2_score: -754.8746\n",
      "Epoch 20/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.3998 - mae: 1.5374 - mse: 3.3998 - r2_score: -759.8154\n",
      "Epoch 21/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - loss: 3.4190 - mae: 1.5339 - mse: 3.4190 - r2_score: -745.4498\n",
      "Epoch 22/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - loss: 3.3745 - mae: 1.5205 - mse: 3.3745 - r2_score: -729.0699\n",
      "Epoch 23/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 774ms/step - loss: 3.3054 - mae: 1.5081 - mse: 3.3054 - r2_score: -731.9053\n",
      "Epoch 24/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - loss: 3.3712 - mae: 1.5262 - mse: 3.3712 - r2_score: -753.5556\n",
      "Epoch 25/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.4260 - mae: 1.5330 - mse: 3.4260 - r2_score: -739.0316\n",
      "Epoch 26/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.3111 - mae: 1.5133 - mse: 3.3111 - r2_score: -743.4552\n",
      "Epoch 27/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - loss: 3.4262 - mae: 1.5414 - mse: 3.4262 - r2_score: -758.7975\n",
      "Epoch 28/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.2279 - mae: 1.4906 - mse: 3.2279 - r2_score: -728.4794\n",
      "Epoch 29/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.3439 - mae: 1.5198 - mse: 3.3439 - r2_score: -748.9501\n",
      "Epoch 30/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - loss: 3.4394 - mae: 1.5379 - mse: 3.4394 - r2_score: -740.8427\n",
      "Epoch 31/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.3784 - mae: 1.5217 - mse: 3.3784 - r2_score: -724.6632\n",
      "Epoch 32/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 3.3688 - mae: 1.5212 - mse: 3.3688 - r2_score: -737.9747\n",
      "Epoch 33/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - loss: 3.3535 - mae: 1.5160 - mse: 3.3535 - r2_score: -720.6359\n",
      "Epoch 34/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.4773 - mae: 1.5486 - mse: 3.4773 - r2_score: -739.9602\n",
      "Epoch 35/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - loss: 3.3595 - mae: 1.5256 - mse: 3.3595 - r2_score: -752.0861\n",
      "Epoch 36/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 3.4343 - mae: 1.5363 - mse: 3.4343 - r2_score: -744.2369\n",
      "Epoch 37/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - loss: 3.3645 - mae: 1.5262 - mse: 3.3645 - r2_score: -747.8643\n",
      "Epoch 38/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 828ms/step - loss: 3.4054 - mae: 1.5368 - mse: 3.4054 - r2_score: -754.3657\n",
      "Epoch 39/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 951ms/step - loss: 3.3465 - mae: 1.5089 - mse: 3.3465 - r2_score: -716.9462\n",
      "Epoch 40/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - loss: 3.4015 - mae: 1.5211 - mse: 3.4015 - r2_score: -722.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 807ms/step - loss: 3.4041 - mae: 1.5213 - mse: 3.4041 - r2_score: -723.6192\n",
      "Epoch 41: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node gradient_tape/compile_loss/mse/sub/BroadcastGradientArgs defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3365, in run_cell_async\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3610, in run_ast_nodes\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3670, in run_code\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Local\\Temp\\ipykernel_11548\\1298772170.py\", line 79, in <module>\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 377, in fit\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 220, in function\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 133, in multi_step_on_iterator\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 114, in one_step_on_data\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 78, in train_step\n\nIncompatible shapes: [256] vs. [256,2]\n\t [[{{node gradient_tape/compile_loss/mse/sub/BroadcastGradientArgs}}]] [Op:__inference_multi_step_on_iterator_420338]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     76\u001b[39m opt_phaz3 = optimizers.Adam(learning_rate=\u001b[32m0.0001\u001b[39m)\n\u001b[32m     77\u001b[39m model_phaz3.compile(optimizer=opt_phaz3, loss=\u001b[33m'\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m'\u001b[39m, metrics=METRICS_REGRESSION)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43mmodel_phaz3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     83\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Evaluate Phase 3 on both training and testing data\u001b[39;00m\n\u001b[32m     86\u001b[39m train_eval_phase3 = model_phaz3.evaluate(X_train, y_train, verbose=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Graph execution error:\n\nDetected at node gradient_tape/compile_loss/mse/sub/BroadcastGradientArgs defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3365, in run_cell_async\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3610, in run_ast_nodes\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3670, in run_code\n\n  File \"C:\\Users\\Scott.Coffin\\AppData\\Local\\Temp\\ipykernel_11548\\1298772170.py\", line 79, in <module>\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 377, in fit\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 220, in function\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 133, in multi_step_on_iterator\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 114, in one_step_on_data\n\n  File \"c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 78, in train_step\n\nIncompatible shapes: [256] vs. [256,2]\n\t [[{{node gradient_tape/compile_loss/mse/sub/BroadcastGradientArgs}}]] [Op:__inference_multi_step_on_iterator_420338]"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "XD_np = np.array(XD)\n",
    "labels_np = np.array(labels)\n",
    "\n",
    "# Remove samples that don't have labels\n",
    "index = ~np.isnan(labels_np)\n",
    "labels_np = labels_np[index]\n",
    "XD_np = XD_np[index]\n",
    "\n",
    "# Filter the corresponding SMILES\n",
    "filtered_smiles = smiles.iloc[index].values\n",
    "\n",
    "# Create a Dataset using SMILES as identifiers\n",
    "dataset = NumpyDataset(X=XD_np, y=labels_np, ids=filtered_smiles)\n",
    "\n",
    "# Create a ScaffoldSplitter\n",
    "splitter = ScaffoldSplitter()\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(\n",
    "    dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1\n",
    ")\n",
    "\n",
    "# Extract training and test indices\n",
    "train_indices = train_dataset.ids\n",
    "test_indices = test_dataset.ids\n",
    "smiles_to_index = {smile: idx for idx, smile in enumerate(filtered_smiles)}\n",
    "\n",
    "# Convert SMILES to indices\n",
    "train_indices = np.array([smiles_to_index[smile] for smile in train_indices])\n",
    "test_indices = np.array([smiles_to_index[smile] for smile in test_indices])\n",
    "\n",
    "# Split the data based on the indices\n",
    "X_train, X_test = XD_np[train_indices], XD_np[test_indices]\n",
    "y_train, y_test = labels_np[train_indices], labels_np[test_indices]\n",
    "\n",
    "print(f\"Number of training samples: {X_train.shape[0]}\")\n",
    "print(f\"Number of test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# === Define Optimizer for Interaction Model ===\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "interactionModel.compile(optimizer=opt, loss='mse', metrics=METRICS_REGRESSION)\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=30, verbose=1, mode='min', restore_best_weights=True)\n",
    "reduce_lr = LearningRateScheduler(lambda epoch: 1e-3 * 0.9 ** epoch)\n",
    "\n",
    "# === Phase 1: Train Interaction Model ===\n",
    "interactionModel.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=256, epochs=1000,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Phase 1 on test data\n",
    "test_eval_phase1 = interactionModel.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# === Feature Extraction ===\n",
    "feature_train = model_feature.predict(X_train)\n",
    "feature_test = model_feature.predict(X_test)\n",
    "\n",
    "# === Phase 2: Train Model Phase 2 ===\n",
    "opt_phaz2 = optimizers.Adam(learning_rate=0.001)\n",
    "model_phaz2.compile(optimizer=opt_phaz2, loss='mse', metrics=METRICS_REGRESSION)\n",
    "model_phaz2.fit(\n",
    "    feature_train, y_train,\n",
    "    batch_size=256, epochs=1000,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    ")\n",
    "\n",
    "# === Freeze Transformer Blocks ===\n",
    "new_transformer_block.set_weights(transformer_block1.get_weights())\n",
    "new_transformer_block2.set_weights(transformer_block2.get_weights())\n",
    "\n",
    "# === Phase 3: Train Model Phase 3 ===\n",
    "opt_phaz3 = optimizers.Adam(learning_rate=0.0001)\n",
    "model_phaz3.compile(optimizer=opt_phaz3, loss='mse', metrics=METRICS_REGRESSION)\n",
    "\n",
    "model_phaz3.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=256, epochs=1000,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Phase 3 on both training and testing data\n",
    "train_eval_phase3 = model_phaz3.evaluate(X_train, y_train, verbose=1)\n",
    "test_eval_phase3 = model_phaz3.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# === Function to Format Results ===\n",
    "def format_results(phase1_result, phase3_train_result, phase3_test_result):\n",
    "    # Define Metric Names\n",
    "    phase1_metrics = ['loss', 'mae', 'mse', 'r2_score']\n",
    "    phase3_metrics = ['loss', 'mae', 'mse', 'r2_score']\n",
    "\n",
    "    # Create DataFrames\n",
    "    phase1_test_df = pd.DataFrame([phase1_result], columns=phase1_metrics)\n",
    "    phase3_train_df = pd.DataFrame([phase3_train_result], columns=phase3_metrics)\n",
    "    phase3_test_df = pd.DataFrame([test_eval_phase3], columns=phase3_metrics)\n",
    "\n",
    "    # Display the Results\n",
    "    print('\\n=== Phase 1: Test Evaluation Results ===')\n",
    "    print(phase1_test_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "\n",
    "    print('\\n=== Phase 3: Train Evaluation Results ===')\n",
    "    print(phase3_train_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "\n",
    "    print('\\n=== Phase 3: Test Evaluation Results ===')\n",
    "    print(phase3_test_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "\n",
    "# === Display Results ===\n",
    "format_results(test_eval_phase1, train_eval_phase3, test_eval_phase3)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
