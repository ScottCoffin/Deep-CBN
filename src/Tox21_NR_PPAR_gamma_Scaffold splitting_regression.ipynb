{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w71ntxmiP5Cs"
   },
   "source": [
    "#Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2mc3-OTP210",
    "outputId": "1bcf264f-343b-4610-b609-3c89f2b742f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "from tensorflow.keras import optimizers, layers, regularizers, metrics\n",
    "from tensorflow.keras.layers import (\n",
    "    Lambda, Input, Reshape, Activation, Concatenate, Dense, Dropout,\n",
    "    BatchNormalization, Conv1D, GlobalMaxPooling1D, LayerNormalization, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "from deepchem.data import NumpyDataset\n",
    "from deepchem.splits import ScaffoldSplitter\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWrFzNjSQCtg"
   },
   "source": [
    "### Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "kXFLqk2vja5M",
    "outputId": "3e31a23d-130b-48c7-cfd4-bc76cd6c75bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08:25:53] SMILES Parse Error: syntax error while parsing: O|[Al](|O)(|O)|[Al](|O)(|O)Cl\n",
      "[08:25:53] SMILES Parse Error: Failed parsing SMILES 'O|[Al](|O)(|O)|[Al](|O)(|O)Cl' for input: 'O|[Al](|O)(|O)|[Al](|O)(|O)Cl'\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:53] Explicit valence for atom # 3 Si, 8, is greater than permitted\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:53] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows before drop: 4,657\n",
      "Rows after NA drop:  4,222\n",
      "Rows after invalid SMILES drop:  4,218\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CAS</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>log_RfD_mg_kg_d</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100-00-5</td>\n",
       "      <td>C1=CC(=CC=C1[N+](=O)[O-])Cl</td>\n",
       "      <td>-3.060745</td>\n",
       "      <td>-3.060745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>100-01-6</td>\n",
       "      <td>C1=CC(=CC=C1N)[N+](=O)[O-]</td>\n",
       "      <td>-2.193802</td>\n",
       "      <td>-2.193802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10004-44-1</td>\n",
       "      <td>CC1=CC(=O)NO1</td>\n",
       "      <td>-2.015260</td>\n",
       "      <td>-2.015260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1001354-72-8</td>\n",
       "      <td>CCCCC(C(CC)N)O</td>\n",
       "      <td>-1.769892</td>\n",
       "      <td>-1.769892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>10016-20-3</td>\n",
       "      <td>C(C1C2C(C(C(O1)OC3C(OC(C(C3O)O)OC4C(OC(C(C4O)O...</td>\n",
       "      <td>1.032814</td>\n",
       "      <td>1.032814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           CAS  \\\n",
       "0           1      100-00-5   \n",
       "1           2      100-01-6   \n",
       "2           3    10004-44-1   \n",
       "3           4  1001354-72-8   \n",
       "4           6    10016-20-3   \n",
       "\n",
       "                                              SMILES  log_RfD_mg_kg_d  \\\n",
       "0                        C1=CC(=CC=C1[N+](=O)[O-])Cl        -3.060745   \n",
       "1                         C1=CC(=CC=C1N)[N+](=O)[O-]        -2.193802   \n",
       "2                                      CC1=CC(=O)NO1        -2.015260   \n",
       "3                                     CCCCC(C(CC)N)O        -1.769892   \n",
       "4  C(C1C2C(C(C(O1)OC3C(OC(C(C3O)O)OC4C(OC(C(C4O)O...         1.032814   \n",
       "\n",
       "     target  \n",
       "0 -3.060745  \n",
       "1 -2.193802  \n",
       "2 -2.015260  \n",
       "3 -1.769892  \n",
       "4  1.032814  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Loading and Preparing Data\n",
    "data = pd.read_csv('../Data/SMILES_RfD.csv')\n",
    "\n",
    "# Drop any rows where the 'SMILES' column is missing or empty\n",
    "data_clean = data.dropna(subset=['SMILES'])\n",
    "data_clean = data_clean[data_clean['SMILES'].str.strip() != \"\"]\n",
    "\n",
    "data_clean = data_clean.reset_index(drop=True)\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "# Drop rows where the SMILES column is NaN or invalid\n",
    "valid_mask = data_clean['SMILES'].apply(\n",
    "    lambda s: isinstance(s, str) and Chem.MolFromSmiles(s) is not None\n",
    ")\n",
    "data_clean_valid = data_clean[valid_mask]\n",
    "\n",
    "print(f\"\\nRows before drop: {len(data):,}\")\n",
    "print(f\"Rows after NA drop:  {len(data_clean):,}\")\n",
    "print(f\"Rows after invalid SMILES drop:  {len(data_clean_valid):,}\")\n",
    "\n",
    "\n",
    "# Preview the cleaned data\n",
    "data_clean_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AFxos0tgQC9s"
   },
   "outputs": [],
   "source": [
    "smiles = data_clean_valid['SMILES']\n",
    "labels = data_clean_valid['target']\n",
    "\n",
    "# Dictionary to convert SMILES characters into numeric values\n",
    "smiles_dict = {\n",
    "    \"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n",
    "    \"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n",
    "    \"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n",
    "    \"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n",
    "    \"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50, \"T\": 17, \"W\": 51,\n",
    "    \"V\": 18, \"Y\": 52, \"[\": 53, \"Z\": 19, \"]\": 54, \"\\\\\": 20, \"a\": 55, \"c\": 56,\n",
    "    \"b\": 21, \"e\": 57, \"d\": 22, \"g\": 58, \"f\": 23, \"i\": 59, \"h\": 24, \"m\": 60,\n",
    "    \"l\": 25, \"o\": 61, \"n\": 26, \"s\": 62, \"r\": 27, \"u\": 63, \"t\": 28, \"y\": 64,\n",
    "    \" \": 65, \":\": 66, \",\": 67, \"p\": 68, \"j\": 69, \"*\": 70\n",
    "}\n",
    "\n",
    "def label_smiles(line, MAX_SMI_LEN, smi_ch_ind):\n",
    "    X = np.zeros(MAX_SMI_LEN, dtype=int)\n",
    "    for i, ch in enumerate(line[:MAX_SMI_LEN]):\n",
    "        if ch in smi_ch_ind:\n",
    "            X[i] = smi_ch_ind[ch]\n",
    "    return X\n",
    "\n",
    "MAX_SMI_LEN = 100  # You can adjust this based on your needs\n",
    "XD = np.array([label_smiles(str(smi), MAX_SMI_LEN, smiles_dict) for smi in smiles])\n",
    "labels = labels.values\n",
    "\n",
    "# Convert to categorical\n",
    "XD = to_categorical(XD, num_classes=71)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qmjyj91JeXsG"
   },
   "source": [
    "# phaze1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A2VgoLaMQN0X",
    "outputId": "d183f5ed-45d0-496a-8aa7-9347ac07ec00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_feature\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"model_feature\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m9,152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,961</span> (847.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m216,961\u001b[0m (847.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">215,425</span> (841.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m215,425\u001b[0m (841.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"interactionModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"interactionModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ model_feature (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">216,961</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ model_feature (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m58,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional (\u001b[38;5;33mFunctional\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │       \u001b[38;5;34m216,961\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">275,457</span> (1.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m275,457\u001b[0m (1.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">273,921</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m273,921\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature model definition \n",
    "XDinput = Input(shape=(100, 71), name='XDinput')\n",
    "encode_smiles = Conv1D(filters=64, kernel_size=2, activation='relu', padding='valid', strides=1)(XDinput)  # (99,64)\n",
    "encode_smiles = Conv1D(filters=64, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)  # (96,64)\n",
    "encode_smiles = Conv1D(filters=128, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)  # (93,128)\n",
    "model_feature = Model(inputs=XDinput, outputs=encode_smiles, name='model_feature')\n",
    "model_feature.summary()\n",
    "\n",
    "# Prediction model definition\n",
    "input_extracted_feature = Input(shape=(93, 128))\n",
    "FC1 = Dense(512, activation='relu')(input_extracted_feature)\n",
    "FC1 = BatchNormalization()(FC1)\n",
    "FC2 = Dropout(0.1)(FC1)\n",
    "FC3 = Dense(256, activation='relu')(FC2)\n",
    "FC3 = BatchNormalization()(FC3)\n",
    "FC4 = Dropout(0.1)(FC3)\n",
    "FC5 = Dense(64, activation='relu')(FC4)\n",
    "predictions = Dense(1, activation='linear')(FC5)\n",
    "model_pred = Model(inputs=input_extracted_feature, outputs=predictions)\n",
    "model_pred.summary()\n",
    "\n",
    "# Full model definition with added Pooling layer\n",
    "interaction_input = XDinput\n",
    "encoded_features = model_feature(interaction_input)  # Output: (None, 93, 128)\n",
    "predicted_output = model_pred(encoded_features)      # Output: (None, 93, 1)\n",
    "\n",
    "# Adding Pooling layer to reduce dimensionality\n",
    "pooled_output = GlobalAveragePooling1D()(predicted_output)  # Output: (None, 1)\n",
    "\n",
    "# Final model definition\n",
    "interactionModel = Model(inputs=interaction_input, outputs=pooled_output, name='interactionModel')\n",
    "interactionModel.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW4Dc7MfQWbn"
   },
   "source": [
    "### BiFormer Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C5EAZalFQWpH"
   },
   "outputs": [],
   "source": [
    "# Define custom layers for 1D processing\n",
    "class TopkRouting(layers.Layer):\n",
    "    def __init__(self, qk_dim, topk=16, qk_scale=None, param_routing=False, diff_routing=False):\n",
    "        super().__init__()\n",
    "        self.topk = topk\n",
    "        self.qk_dim = qk_dim\n",
    "        self.scale = qk_scale if qk_scale is not None else qk_dim**-0.5\n",
    "        self.diff_routing = diff_routing\n",
    "        if param_routing:\n",
    "            self.emb = layers.Dense(qk_dim)\n",
    "        else:\n",
    "            self.emb = lambda x: x\n",
    "        self.routing_act = lambda x, axis: tf.nn.softmax(x, axis=axis)\n",
    "\n",
    "    def call(self, query, key, training=None):\n",
    "        if not self.diff_routing:\n",
    "            query = tf.stop_gradient(query)\n",
    "            key = tf.stop_gradient(key)\n",
    "        query_hat = self.emb(query)\n",
    "        key_hat = self.emb(key)\n",
    "\n",
    "        attn_logit = tf.einsum('bnc,bqc->bnq', query_hat*self.scale, key_hat)  # Adjusted for 1D\n",
    "        topk_attn_logit, topk_index = tf.math.top_k(attn_logit, k=self.topk, sorted=True)\n",
    "        r_weight = self.routing_act(topk_attn_logit, axis=-1)\n",
    "        return r_weight, topk_index\n",
    "\n",
    "def tf_gather_kv(kv, r_idx):\n",
    "    n = tf.shape(kv)[0]\n",
    "    p2 = tf.shape(kv)[1]\n",
    "    c_kv = tf.shape(kv)[2]\n",
    "    topk = tf.shape(r_idx)[2]\n",
    "\n",
    "    batch_idx = tf.reshape(tf.range(n), [n, 1, 1])\n",
    "    batch_idx = tf.tile(batch_idx, [1, p2, topk])\n",
    "    p2_idx = tf.reshape(tf.range(p2), [1, p2, 1])\n",
    "    p2_idx = tf.tile(p2_idx, [n, 1, topk])\n",
    "\n",
    "    gather_indices = tf.stack([batch_idx, p2_idx, r_idx], axis=-1)\n",
    "    gathered = tf.gather_nd(kv, gather_indices)\n",
    "    return gathered  # Shape: (n, p2, topk, c_kv)\n",
    "\n",
    "class KVGather(layers.Layer):\n",
    "    def __init__(self, mul_weight='none'):\n",
    "        super().__init__()\n",
    "        assert mul_weight in ['none', 'soft', 'hard']\n",
    "        self.mul_weight = mul_weight\n",
    "\n",
    "    def call(self, r_idx, r_weight, kv, training=None):\n",
    "        topk_kv = tf_gather_kv(kv, r_idx)\n",
    "        if self.mul_weight == 'soft':\n",
    "            r_weight_exp = tf.expand_dims(tf.expand_dims(r_weight, -1), -1)\n",
    "            topk_kv = topk_kv * r_weight_exp\n",
    "        return topk_kv\n",
    "\n",
    "class QKVLinear(layers.Layer):\n",
    "    def __init__(self, dim, qk_dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.qk_dim = qk_dim\n",
    "        self.qkv = layers.Dense(qk_dim + qk_dim + dim, use_bias=bias)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        qkv = self.qkv(x)\n",
    "        q, kv = tf.split(qkv, [self.qk_dim, self.qk_dim + self.dim], axis=-1)\n",
    "        return q, kv\n",
    "\n",
    "class BiLevelRoutingAttention(layers.Layer):\n",
    "    def __init__(self, dim, n_win=7, num_heads=8, qk_dim=None, qk_scale=None,\n",
    "                 kv_per_win=4, kv_downsample_ratio=4, kv_downsample_mode='identity',\n",
    "                 topk=4, param_attention=\"qkvo\", param_routing=False, diff_routing=False, soft_routing=False,\n",
    "                 side_dwconv=3, auto_pad=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_win = n_win\n",
    "        self.num_heads = num_heads\n",
    "        self.qk_dim = qk_dim if qk_dim is not None else dim\n",
    "        self.scale = qk_scale if qk_scale is not None else self.qk_dim**-0.5\n",
    "        self.topk = topk\n",
    "        self.param_routing = param_routing\n",
    "        self.diff_routing = diff_routing\n",
    "        self.soft_routing = soft_routing\n",
    "        self.auto_pad = auto_pad\n",
    "\n",
    "        # For 1D, we use Conv1D\n",
    "        if side_dwconv > 0:\n",
    "            self.lepe = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n",
    "        else:\n",
    "            self.lepe = lambda x: tf.zeros_like(x)\n",
    "\n",
    "        self.router = TopkRouting(qk_dim=self.qk_dim, topk=self.topk, qk_scale=self.scale,\n",
    "                                  param_routing=self.param_routing, diff_routing=self.diff_routing)\n",
    "\n",
    "        mul_weight = 'none'\n",
    "        if self.soft_routing:\n",
    "            mul_weight = 'soft'\n",
    "        self.kv_gather = KVGather(mul_weight=mul_weight)\n",
    "\n",
    "        if param_attention in ['qkvo', 'qkv']:\n",
    "            self.qkv = QKVLinear(self.dim, self.qk_dim)\n",
    "            if param_attention == 'qkvo':\n",
    "                self.wo = layers.Dense(self.dim)\n",
    "            else:\n",
    "                self.wo = lambda x: x\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported param_attention mode\")\n",
    "\n",
    "        self.attn_act = lambda x: tf.nn.softmax(x, axis=-1)\n",
    "        self.kv_down = lambda x: x  # identity for simplicity\n",
    "\n",
    "    def call(self, x, training=None, ret_attn_mask=False):\n",
    "        # Implementing full attention can be complex, so here we only keep the general structure\n",
    "        if ret_attn_mask:\n",
    "            return x, None, None, None\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class Attention(layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n",
    "        self.qkv = layers.Dense(dim*3, use_bias=qkv_bias)\n",
    "        self.attn_drop = layers.Dropout(attn_drop)\n",
    "        self.proj = layers.Dense(dim)\n",
    "        self.proj_drop = layers.Dropout(proj_drop)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        batch_size, seq_length, dim = tf.unstack(tf.shape(x))\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "\n",
    "        attn = tf.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn, training=training)\n",
    "        out = tf.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out, training=training)\n",
    "        return out\n",
    "\n",
    "class AttentionLePE(layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., side_dwconv=5):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n",
    "        self.qkv = layers.Dense(dim*3, use_bias=qkv_bias)\n",
    "        self.attn_drop = layers.Dropout(attn_drop)\n",
    "        self.proj = layers.Dense(dim)\n",
    "        self.proj_drop = layers.Dropout(proj_drop)\n",
    "        if side_dwconv > 0:\n",
    "            self.lepe = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n",
    "        else:\n",
    "            self.lepe = lambda x: tf.zeros_like(x)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        lepe_out = self.lepe(x)\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "\n",
    "        attn = tf.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn, training=training)\n",
    "        out = tf.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')\n",
    "        out = out + lepe_out\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out, training=training)\n",
    "        return out\n",
    "\n",
    "class PreNorm(layers.Layer):\n",
    "    def __init__(self, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.fn(self.norm(x), training=training)\n",
    "\n",
    "class MLP(layers.Layer):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = keras.Sequential([\n",
    "            layers.Dense(hidden_dim),\n",
    "            layers.Activation('gelu'),\n",
    "            layers.Dropout(rate=dropout),\n",
    "            layers.Dense(dim),\n",
    "            layers.Dropout(rate=dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.net(x, training=training)\n",
    "\n",
    "class DropPath(layers.Layer):\n",
    "    # Placeholder for DropPath, currently no-op\n",
    "    def __init__(self, drop_prob=0.):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if (not training) or self.drop_prob == 0.:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += tf.random.uniform(tf.shape(x), dtype=x.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        return tf.divide(x, keep_prob) * binary_tensor\n",
    "\n",
    "class Block(layers.Layer):\n",
    "    def __init__(self, dim, drop_path=0.1, layer_scale_init_value=-1,\n",
    "                 num_heads=8, n_win=7, qk_dim=128, qk_scale=None,\n",
    "                 kv_per_win=8, kv_downsample_ratio=1, kv_downsample_mode='identity',\n",
    "                 topk=8, param_attention=\"qkvo\", param_routing=True, diff_routing=False, soft_routing=True,\n",
    "                 mlp_ratio=4, mlp_dwconv=False, side_dwconv=5, before_attn_dwconv=3, pre_norm=True, auto_pad=True):\n",
    "        super().__init__()\n",
    "        qk_dim = qk_dim or dim\n",
    "\n",
    "        # For 1D, we use Conv1D\n",
    "        if before_attn_dwconv > 0:\n",
    "            self.pos_embed = layers.Conv1D(dim, kernel_size=before_attn_dwconv, padding='same', activation='relu')\n",
    "        else:\n",
    "            self.pos_embed = lambda x: tf.zeros_like(x)\n",
    "\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        if topk > 0:\n",
    "            self.attn = BiLevelRoutingAttention(dim=dim, num_heads=num_heads, n_win=n_win, qk_dim=qk_dim,\n",
    "                                               qk_scale=qk_scale, kv_per_win=kv_per_win, kv_downsample_ratio=kv_downsample_ratio,\n",
    "                                               kv_downsample_mode=kv_downsample_mode, topk=topk, param_attention=param_attention,\n",
    "                                               param_routing=param_routing, diff_routing=diff_routing, soft_routing=soft_routing,\n",
    "                                               side_dwconv=side_dwconv, auto_pad=auto_pad)\n",
    "        elif topk == -1:\n",
    "            self.attn = Attention(dim=dim, num_heads=num_heads, qk_scale=qk_scale)\n",
    "        elif topk == -2:\n",
    "            self.attn = AttentionLePE(dim=dim, num_heads=num_heads, qk_scale=qk_scale, side_dwconv=side_dwconv)\n",
    "        elif topk == 0:\n",
    "            # Pseudo attention\n",
    "            self.attn = keras.Sequential([\n",
    "                layers.Dense(dim),\n",
    "                layers.Conv1D(dim, kernel_size=5, padding='same', activation='relu'),\n",
    "                layers.Dense(dim)\n",
    "            ])\n",
    "\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        mlp_hidden_dim = int(mlp_ratio * dim)\n",
    "        mlp_layers = [layers.Dense(mlp_hidden_dim)]\n",
    "        if mlp_dwconv:\n",
    "            mlp_layers.append(layers.Conv1D(mlp_hidden_dim, kernel_size=3, padding='same', activation='relu'))\n",
    "        mlp_layers.append(layers.Activation('gelu'))\n",
    "        mlp_layers.append(layers.Dense(dim))\n",
    "        mlp_layers.insert(1, layers.Dropout(0.2)) # Add dropout after first Dense\n",
    "        mlp_layers.append(layers.Dropout(0.2))\n",
    "        self.mlp = keras.Sequential(mlp_layers)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else layers.Lambda(lambda x: x)\n",
    "\n",
    "        if layer_scale_init_value > 0:\n",
    "            self.use_layer_scale = True\n",
    "            self.gamma1 = self.add_weight(shape=(dim,),\n",
    "                                          initializer=tf.keras.initializers.Constant(layer_scale_init_value),\n",
    "                                          trainable=True)\n",
    "            self.gamma2 = self.add_weight(shape=(dim,),\n",
    "                                          initializer=tf.keras.initializers.Constant(layer_scale_init_value),\n",
    "                                          trainable=True)\n",
    "        else:\n",
    "            self.use_layer_scale = False\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        x = x + self.pos_embed(x)  # Add positional embedding\n",
    "\n",
    "        if self.pre_norm:\n",
    "            if self.use_layer_scale:\n",
    "                x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x), training=training), training=training)\n",
    "                x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x), training=training), training=training)\n",
    "            else:\n",
    "                x = x + self.drop_path(self.attn(self.norm1(x), training=training), training=training)\n",
    "                x = x + self.drop_path(self.mlp(self.norm2(x), training=training), training=training)\n",
    "        else:\n",
    "            if self.use_layer_scale:\n",
    "                tmp = x + self.drop_path(self.gamma1 * self.attn(x, training=training), training=training)\n",
    "                x = self.norm1(tmp)\n",
    "                tmp = x + self.drop_path(self.gamma2 * self.mlp(x, training=training), training=training)\n",
    "                x = self.norm2(tmp)\n",
    "            else:\n",
    "                tmp = x + self.drop_path(self.attn(x, training=training), training=training)\n",
    "                x = self.norm1(tmp)\n",
    "                tmp = x + self.drop_path(self.mlp(x, training=training), training=training)\n",
    "                x = self.norm2(tmp)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k41fqbmrQxbV"
   },
   "source": [
    "# phaze2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "qFxfd5e4QdV3",
    "outputId": "3ffefc83-bb87-45ff-cb54-e4535be8d055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ difference (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ difference[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block (\u001b[38;5;33mBlock\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_1 (\u001b[38;5;33mBlock\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ difference (\u001b[38;5;33mLambda\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ difference[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">297,216</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m297,216\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">297,216</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m297,216\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameter settings for the block\n",
    "dim = 128   \n",
    "num_heads = 8\n",
    "mlp_dim = 128 * 3  \n",
    "depth = 2\n",
    "dim_head = 48\n",
    "\n",
    "# Create Phase 2 model\n",
    "input_phaz2 = Input(shape=(93, 128))  \n",
    "\n",
    "processed_input = input_phaz2\n",
    "\n",
    "# Define transformer blocks\n",
    "transformer_block1 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "\n",
    "# Apply the first transformer block\n",
    "transformer_output1 = transformer_block1(processed_input)\n",
    "# Compute the norm of the output along the last axis (preserving dimensions)\n",
    "transformer_output1 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1)\n",
    "\n",
    "# Apply the second transformer block\n",
    "transformer_output2 = transformer_block2(processed_input)\n",
    "# Compute the norm of the output along the last axis (preserving dimensions)\n",
    "transformer_output2 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2)\n",
    "\n",
    "# Combine the outputs of both transformer blocks along the last axis\n",
    "combined_outputs = Concatenate(axis=-1)([transformer_output1, transformer_output2])\n",
    "\n",
    "# Apply global average pooling to reduce the output to (batch_size, 2)\n",
    "pooled_outputs = GlobalAveragePooling1D()(combined_outputs)\n",
    "\n",
    "# Compute the difference between the two pooled outputs and reshape to (batch_size, 1)\n",
    "difference = Lambda(lambda x: tf.expand_dims(x[:, 0] - x[:, 1], axis=-1), name='difference')(pooled_outputs)\n",
    "\n",
    "# Apply sigmoid activation to normalize the output to the range [0, 1]\n",
    "condition_2 = Activation('sigmoid', name='activation_11')(difference)\n",
    "\n",
    "# Freeze the `model_feature` layers to prevent them from being trained\n",
    "model_feature.trainable = False\n",
    "\n",
    "# Define the complete model\n",
    "model_phaz2 = Model(inputs=input_phaz2, outputs=condition_2)\n",
    "model_phaz2.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcH6Y5x1e5OS"
   },
   "source": [
    "# phaze3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F6DTjT17Q1W5",
    "outputId": "6bcfc3f8-eeee-418e-cddd-31ff826c0ff5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer_4 True\n",
      "model_feature False\n",
      "block_2 False\n",
      "block_3 False\n",
      "lambda_2 True\n",
      "lambda_3 True\n",
      "concatenate_1 True\n",
      "global_average_pooling1d_2 True\n",
      "dense_24 True\n",
      "batch_normalization_2 True\n",
      "dropout_10 True\n",
      "dense_25 True\n",
      "batch_normalization_3 True\n",
      "dropout_11 True\n",
      "dense_26 True\n",
      "dense_27 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ model_feature       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ model_feature       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m58,496\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_2 (\u001b[38;5;33mBlock\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ model_feature[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_3 (\u001b[38;5;33mBlock\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ model_feature[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_3 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m1,536\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ dense_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m16,448\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">508,161</span> (1.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m508,161\u001b[0m (1.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,913</span> (589.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,913\u001b[0m (589.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">357,248</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m357,248\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define new transformer blocks\n",
    "new_transformer_block = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "new_transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "\n",
    "# Input for Phase 3\n",
    "XDinput_phaz3 = Input(shape=(100, 71))\n",
    "# Pass input through the feature extraction model\n",
    "model_feature_output = model_feature(XDinput_phaz3)  # Output shape: (93, 192)\n",
    "\n",
    "# Transformer blocks processing\n",
    "transformer_output1_phaz3 = new_transformer_block(model_feature_output)  # Output shape: (93, 192)\n",
    "# Compute the norm along the last axis, retaining dimensions\n",
    "transformer_output1_phaz3 = layers.Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1_phaz3)  # Shape: (93, 1)\n",
    "\n",
    "transformer_output2_phaz3 = new_transformer_block2(model_feature_output)  # Output shape: (93, 192)\n",
    "# Compute the norm along the last axis, retaining dimensions\n",
    "transformer_output2_phaz3 = layers.Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2_phaz3)  # Shape: (93, 1)\n",
    "\n",
    "# Combine the outputs of both transformer blocks\n",
    "concatenated_phaz3 = Concatenate(axis=-1)([transformer_output1_phaz3, transformer_output2_phaz3])  # Shape: (93, 2)\n",
    "\n",
    "# Freeze the weights of feature extractor and transformer blocks\n",
    "model_feature.trainable = False\n",
    "new_transformer_block.trainable = False\n",
    "new_transformer_block2.trainable = False\n",
    "\n",
    "# Apply global average pooling to reduce the dimensions to (batch_size, 2)\n",
    "pooled_phaz3 = GlobalAveragePooling1D()(concatenated_phaz3)  # Shape: (None, 2)\n",
    "\n",
    "# Fully connected layers\n",
    "FC1_phaz3 = Dense(512, activation='relu')(pooled_phaz3)\n",
    "FC1_phaz3 = BatchNormalization()(FC1_phaz3)\n",
    "FC2_phaz3 = Dropout(0.1)(FC1_phaz3)  # Increased dropout rate\n",
    "FC3_phaz3 = Dense(256, activation='relu')(FC2_phaz3)\n",
    "FC3_phaz3 = BatchNormalization()(FC3_phaz3)\n",
    "FC4_phaz3 = Dropout(0.1)(FC3_phaz3)  # Increased dropout rate\n",
    "FC5_phaz3 = Dense(64, activation='relu')(FC4_phaz3)\n",
    "\n",
    "# Final prediction layer for regression\n",
    "predictions_phaz3 = Dense(1, activation='linear')(FC5_phaz3)\n",
    "\n",
    "# Define the complete model for Phase 3\n",
    "model_phaz3 = Model(inputs=XDinput_phaz3, outputs=predictions_phaz3)\n",
    "\n",
    "# Print the name and trainable status of each layer\n",
    "for layer in model_phaz3.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "\n",
    "# Model summary\n",
    "model_phaz3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jF60_7QXQ4lB"
   },
   "outputs": [],
   "source": [
    "# Metrics for regression tasks\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"TensorFlow implementation of the coefficient of determination.\"\"\"\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + tf.keras.backend.epsilon())\n",
    "\n",
    "METRICS_REGRESSION = [\n",
    "    metrics.MeanAbsoluteError(name='mae'),\n",
    "    metrics.MeanSquaredError(name='mse'),\n",
    "    r2_score,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBNS2iz6f9xP"
   },
   "source": [
    "# Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcS3ex1TgCRK",
    "outputId": "7a3df7f6-88ae-459e-8e9d-fde383fd7d9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08:25:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[08:25:55] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3374\n",
      "Number of test samples: 422\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 272ms/step - loss: 2.5108 - mae: 1.2449 - mse: 2.5108 - r2_score: -528.3309\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 249ms/step - loss: 1.1783 - mae: 0.8580 - mse: 1.1783 - r2_score: -266.6622\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 253ms/step - loss: 1.0837 - mae: 0.8038 - mse: 1.0837 - r2_score: -273.2347\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 257ms/step - loss: 1.0933 - mae: 0.7963 - mse: 1.0933 - r2_score: -268.0968\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 297ms/step - loss: 1.0414 - mae: 0.7959 - mse: 1.0414 - r2_score: -272.1313\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 258ms/step - loss: 1.0340 - mae: 0.7828 - mse: 1.0340 - r2_score: -275.4993\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 247ms/step - loss: 1.0260 - mae: 0.7835 - mse: 1.0260 - r2_score: -274.9243\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 254ms/step - loss: 1.0096 - mae: 0.7741 - mse: 1.0096 - r2_score: -274.8286\n",
      "Epoch 9/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251ms/step - loss: 0.9865 - mae: 0.7620 - mse: 0.9865 - r2_score: -278.7759\n",
      "Epoch 10/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 243ms/step - loss: 1.0052 - mae: 0.7713 - mse: 1.0052 - r2_score: -278.4744\n",
      "Epoch 11/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 241ms/step - loss: 1.0293 - mae: 0.7841 - mse: 1.0293 - r2_score: -283.9112\n",
      "Epoch 12/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 243ms/step - loss: 1.0251 - mae: 0.7709 - mse: 1.0251 - r2_score: -281.3954\n",
      "Epoch 13/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 249ms/step - loss: 1.0109 - mae: 0.7682 - mse: 1.0109 - r2_score: -281.2463\n",
      "Epoch 14/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 253ms/step - loss: 0.9434 - mae: 0.7514 - mse: 0.9434 - r2_score: -284.3029\n",
      "Epoch 15/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 267ms/step - loss: 0.9637 - mae: 0.7562 - mse: 0.9637 - r2_score: -286.4261\n",
      "Epoch 16/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 264ms/step - loss: 0.9564 - mae: 0.7458 - mse: 0.9564 - r2_score: -285.3087\n",
      "Epoch 17/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 252ms/step - loss: 0.9416 - mae: 0.7505 - mse: 0.9416 - r2_score: -286.4724\n",
      "Epoch 18/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 256ms/step - loss: 0.9270 - mae: 0.7411 - mse: 0.9270 - r2_score: -288.9937\n",
      "Epoch 19/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 250ms/step - loss: 0.9240 - mae: 0.7453 - mse: 0.9240 - r2_score: -289.3418\n",
      "Epoch 20/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 249ms/step - loss: 0.9455 - mae: 0.7503 - mse: 0.9455 - r2_score: -287.1965\n",
      "Epoch 21/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 250ms/step - loss: 0.9172 - mae: 0.7376 - mse: 0.9172 - r2_score: -289.0403\n",
      "Epoch 22/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 250ms/step - loss: 0.8994 - mae: 0.7335 - mse: 0.8994 - r2_score: -291.8371\n",
      "Epoch 23/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251ms/step - loss: 0.9187 - mae: 0.7408 - mse: 0.9187 - r2_score: -294.0310\n",
      "Epoch 24/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 255ms/step - loss: 0.9309 - mae: 0.7421 - mse: 0.9309 - r2_score: -296.3264\n",
      "Epoch 25/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 252ms/step - loss: 0.8635 - mae: 0.7171 - mse: 0.8635 - r2_score: -298.4734\n",
      "Epoch 26/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251ms/step - loss: 0.9097 - mae: 0.7352 - mse: 0.9097 - r2_score: -299.0183\n",
      "Epoch 27/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 255ms/step - loss: 0.9108 - mae: 0.7330 - mse: 0.9108 - r2_score: -293.6292\n",
      "Epoch 28/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251ms/step - loss: 0.8594 - mae: 0.7213 - mse: 0.8594 - r2_score: -298.2051\n",
      "Epoch 29/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 256ms/step - loss: 0.9413 - mae: 0.7482 - mse: 0.9413 - r2_score: -301.6550\n",
      "Epoch 30/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 257ms/step - loss: 0.8749 - mae: 0.7237 - mse: 0.8749 - r2_score: -299.4842\n",
      "Epoch 31/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 259ms/step - loss: 0.8809 - mae: 0.7227 - mse: 0.8809 - r2_score: -296.7341\n",
      "Epoch 32/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 263ms/step - loss: 0.9018 - mae: 0.7267 - mse: 0.9018 - r2_score: -305.0294\n",
      "Epoch 33/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 257ms/step - loss: 0.8424 - mae: 0.7112 - mse: 0.8424 - r2_score: -299.5510\n",
      "Epoch 34/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 260ms/step - loss: 0.8438 - mae: 0.7113 - mse: 0.8438 - r2_score: -304.3718\n",
      "Epoch 35/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 268ms/step - loss: 0.8451 - mae: 0.7138 - mse: 0.8451 - r2_score: -305.1064\n",
      "Epoch 36/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 261ms/step - loss: 0.9019 - mae: 0.7323 - mse: 0.9019 - r2_score: -306.4342\n",
      "Epoch 37/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 269ms/step - loss: 0.8457 - mae: 0.7097 - mse: 0.8457 - r2_score: -301.8961\n",
      "Epoch 38/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 274ms/step - loss: 0.8314 - mae: 0.7065 - mse: 0.8314 - r2_score: -308.1729\n",
      "Epoch 39/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 278ms/step - loss: 0.8446 - mae: 0.7115 - mse: 0.8446 - r2_score: -305.6183\n",
      "Epoch 40/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 268ms/step - loss: 0.8781 - mae: 0.7273 - mse: 0.8781 - r2_score: -303.0002\n",
      "Epoch 41/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 269ms/step - loss: 0.8272 - mae: 0.7005 - mse: 0.8272 - r2_score: -311.3885\n",
      "Epoch 42/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 275ms/step - loss: 0.8361 - mae: 0.7066 - mse: 0.8361 - r2_score: -306.0724\n",
      "Epoch 43/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 274ms/step - loss: 0.8539 - mae: 0.7091 - mse: 0.8539 - r2_score: -306.8778\n",
      "Epoch 44/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 269ms/step - loss: 0.8300 - mae: 0.7111 - mse: 0.8300 - r2_score: -307.5342\n",
      "Epoch 45/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 273ms/step - loss: 0.8341 - mae: 0.7013 - mse: 0.8341 - r2_score: -305.1705\n",
      "Epoch 46/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 273ms/step - loss: 0.8289 - mae: 0.7057 - mse: 0.8289 - r2_score: -306.5511\n",
      "Epoch 47/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 274ms/step - loss: 0.8004 - mae: 0.6872 - mse: 0.8004 - r2_score: -308.7320\n",
      "Epoch 48/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 289ms/step - loss: 0.8308 - mae: 0.7129 - mse: 0.8308 - r2_score: -307.8946\n",
      "Epoch 49/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 292ms/step - loss: 0.8395 - mae: 0.7125 - mse: 0.8395 - r2_score: -310.1072\n",
      "Epoch 50/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 277ms/step - loss: 0.8555 - mae: 0.7268 - mse: 0.8555 - r2_score: -318.8931\n",
      "Epoch 51/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 279ms/step - loss: 0.8347 - mae: 0.7061 - mse: 0.8347 - r2_score: -317.6973\n",
      "Epoch 52/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 276ms/step - loss: 0.7842 - mae: 0.6860 - mse: 0.7842 - r2_score: -313.7189\n",
      "Epoch 53/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 279ms/step - loss: 0.8204 - mae: 0.6964 - mse: 0.8204 - r2_score: -309.3363\n",
      "Epoch 54/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 281ms/step - loss: 0.8253 - mae: 0.7016 - mse: 0.8253 - r2_score: -309.6518\n",
      "Epoch 55/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 281ms/step - loss: 0.7972 - mae: 0.6850 - mse: 0.7972 - r2_score: -317.6273\n",
      "Epoch 56/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 284ms/step - loss: 0.8177 - mae: 0.6928 - mse: 0.8177 - r2_score: -317.4186\n",
      "Epoch 57/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 284ms/step - loss: 0.8029 - mae: 0.6917 - mse: 0.8029 - r2_score: -313.0060\n",
      "Epoch 58/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 284ms/step - loss: 0.7654 - mae: 0.6735 - mse: 0.7654 - r2_score: -313.5652\n",
      "Epoch 59/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 285ms/step - loss: 0.7873 - mae: 0.6832 - mse: 0.7873 - r2_score: -321.3629\n",
      "Epoch 60/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 291ms/step - loss: 0.7835 - mae: 0.6801 - mse: 0.7835 - r2_score: -320.9156\n",
      "Epoch 61/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 288ms/step - loss: 0.7830 - mae: 0.6806 - mse: 0.7830 - r2_score: -322.5710\n",
      "Epoch 62/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 301ms/step - loss: 0.7718 - mae: 0.6804 - mse: 0.7718 - r2_score: -319.3821\n",
      "Epoch 63/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 291ms/step - loss: 0.7907 - mae: 0.6771 - mse: 0.7907 - r2_score: -314.2815\n",
      "Epoch 64/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 289ms/step - loss: 0.7513 - mae: 0.6688 - mse: 0.7513 - r2_score: -325.3023\n",
      "Epoch 65/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 293ms/step - loss: 0.7843 - mae: 0.6805 - mse: 0.7843 - r2_score: -318.3582\n",
      "Epoch 66/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 295ms/step - loss: 0.7711 - mae: 0.6774 - mse: 0.7711 - r2_score: -320.0197\n",
      "Epoch 67/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 291ms/step - loss: 0.7508 - mae: 0.6769 - mse: 0.7508 - r2_score: -329.5385\n",
      "Epoch 68/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 293ms/step - loss: 0.7759 - mae: 0.6811 - mse: 0.7759 - r2_score: -324.1265\n",
      "Epoch 69/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 295ms/step - loss: 0.7671 - mae: 0.6817 - mse: 0.7671 - r2_score: -322.7879\n",
      "Epoch 70/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 295ms/step - loss: 0.7508 - mae: 0.6678 - mse: 0.7508 - r2_score: -321.3651\n",
      "Epoch 71/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 296ms/step - loss: 0.7697 - mae: 0.6727 - mse: 0.7697 - r2_score: -320.2217\n",
      "Epoch 72/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 297ms/step - loss: 0.7565 - mae: 0.6741 - mse: 0.7565 - r2_score: -320.7865\n",
      "Epoch 73/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 313ms/step - loss: 0.7302 - mae: 0.6648 - mse: 0.7302 - r2_score: -325.5905\n",
      "Epoch 74/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 302ms/step - loss: 0.7790 - mae: 0.6833 - mse: 0.7790 - r2_score: -325.9880\n",
      "Epoch 75/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 303ms/step - loss: 0.7558 - mae: 0.6722 - mse: 0.7558 - r2_score: -328.1739\n",
      "Epoch 76/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 310ms/step - loss: 0.7788 - mae: 0.6730 - mse: 0.7788 - r2_score: -328.4427\n",
      "Epoch 77/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 307ms/step - loss: 0.7654 - mae: 0.6797 - mse: 0.7654 - r2_score: -332.6269\n",
      "Epoch 78/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 303ms/step - loss: 0.7525 - mae: 0.6709 - mse: 0.7525 - r2_score: -330.1759\n",
      "Epoch 79/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 306ms/step - loss: 0.7372 - mae: 0.6635 - mse: 0.7372 - r2_score: -326.2247\n",
      "Epoch 80/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 306ms/step - loss: 0.7504 - mae: 0.6679 - mse: 0.7504 - r2_score: -331.9799\n",
      "Epoch 81/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 308ms/step - loss: 0.7468 - mae: 0.6686 - mse: 0.7468 - r2_score: -331.3848\n",
      "Epoch 82/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 307ms/step - loss: 0.7370 - mae: 0.6739 - mse: 0.7370 - r2_score: -333.8651\n",
      "Epoch 83/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 309ms/step - loss: 0.7625 - mae: 0.6770 - mse: 0.7625 - r2_score: -337.5475\n",
      "Epoch 84/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 306ms/step - loss: 0.7466 - mae: 0.6636 - mse: 0.7466 - r2_score: -328.3291\n",
      "Epoch 85/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 312ms/step - loss: 0.7174 - mae: 0.6520 - mse: 0.7174 - r2_score: -331.8778\n",
      "Epoch 86/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 311ms/step - loss: 0.7225 - mae: 0.6563 - mse: 0.7225 - r2_score: -335.5099\n",
      "Epoch 87/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 305ms/step - loss: 0.7222 - mae: 0.6565 - mse: 0.7222 - r2_score: -333.5340\n",
      "Epoch 88/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 296ms/step - loss: 0.6979 - mae: 0.6430 - mse: 0.6979 - r2_score: -336.2161\n",
      "Epoch 89/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 290ms/step - loss: 0.7304 - mae: 0.6562 - mse: 0.7304 - r2_score: -331.3672\n",
      "Epoch 90/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 304ms/step - loss: 0.7135 - mae: 0.6569 - mse: 0.7135 - r2_score: -340.2340\n",
      "Epoch 91/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 294ms/step - loss: 0.6909 - mae: 0.6442 - mse: 0.6909 - r2_score: -336.7561\n",
      "Epoch 92/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 289ms/step - loss: 0.6765 - mae: 0.6343 - mse: 0.6765 - r2_score: -341.6792\n",
      "Epoch 93/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 290ms/step - loss: 0.7159 - mae: 0.6533 - mse: 0.7159 - r2_score: -337.1891\n",
      "Epoch 94/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 290ms/step - loss: 0.7073 - mae: 0.6479 - mse: 0.7073 - r2_score: -334.5693\n",
      "Epoch 95/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 294ms/step - loss: 0.7066 - mae: 0.6477 - mse: 0.7066 - r2_score: -330.6551\n",
      "Epoch 96/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 295ms/step - loss: 0.6785 - mae: 0.6385 - mse: 0.6785 - r2_score: -337.3554\n",
      "Epoch 97/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 295ms/step - loss: 0.6739 - mae: 0.6310 - mse: 0.6739 - r2_score: -337.1517\n",
      "Epoch 98/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 290ms/step - loss: 0.6796 - mae: 0.6338 - mse: 0.6796 - r2_score: -338.1132\n",
      "Epoch 99/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 292ms/step - loss: 0.6568 - mae: 0.6271 - mse: 0.6568 - r2_score: -340.8062\n",
      "Epoch 100/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 291ms/step - loss: 0.6902 - mae: 0.6434 - mse: 0.6902 - r2_score: -339.8601\n",
      "Restoring model weights from the end of the best epoch: 99.\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.5153 - mae: 0.9036 - mse: 1.5153 - r2_score: -45.2673\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 428ms/step - loss: 3.6438 - mae: 1.4256 - mse: 3.1109 - r2_score: -774.5267\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 427ms/step - loss: 3.3416 - mae: 1.5105 - mse: 3.3416 - r2_score: -732.5687\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 430ms/step - loss: 3.4094 - mae: 1.5307 - mse: 3.4094 - r2_score: -744.7897\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 426ms/step - loss: 3.3082 - mae: 1.5091 - mse: 3.3082 - r2_score: -732.1693\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 429ms/step - loss: 3.3678 - mae: 1.5209 - mse: 3.3678 - r2_score: -736.6092\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 431ms/step - loss: 3.3803 - mae: 1.5318 - mse: 3.3803 - r2_score: -761.2535\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 427ms/step - loss: 3.3875 - mae: 1.5283 - mse: 3.3875 - r2_score: -750.2092\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 435ms/step - loss: 3.3825 - mae: 1.5230 - mse: 3.3825 - r2_score: -727.7117\n",
      "Epoch 9/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 429ms/step - loss: 3.4742 - mae: 1.5470 - mse: 3.4742 - r2_score: -753.1647\n",
      "Epoch 10/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 431ms/step - loss: 3.2800 - mae: 1.5019 - mse: 3.2800 - r2_score: -734.0642\n",
      "Epoch 11/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 441ms/step - loss: 3.3636 - mae: 1.5245 - mse: 3.3636 - r2_score: -741.3231\n",
      "Epoch 12/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 433ms/step - loss: 3.3964 - mae: 1.5318 - mse: 3.3964 - r2_score: -740.3480\n",
      "Epoch 13/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 440ms/step - loss: 3.3267 - mae: 1.5107 - mse: 3.3267 - r2_score: -732.0806\n",
      "Epoch 14/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 436ms/step - loss: 3.3660 - mae: 1.5192 - mse: 3.3660 - r2_score: -732.2128\n",
      "Epoch 15/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 434ms/step - loss: 3.2906 - mae: 1.5002 - mse: 3.2906 - r2_score: -726.3179\n",
      "Epoch 16/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 435ms/step - loss: 3.3833 - mae: 1.5250 - mse: 3.3833 - r2_score: -729.6832\n",
      "Epoch 17/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 432ms/step - loss: 3.4432 - mae: 1.5415 - mse: 3.4432 - r2_score: -754.9167\n",
      "Epoch 18/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 434ms/step - loss: 3.3555 - mae: 1.5169 - mse: 3.3555 - r2_score: -735.7673\n",
      "Epoch 19/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 433ms/step - loss: 3.3113 - mae: 1.5183 - mse: 3.3113 - r2_score: -759.8467\n",
      "Epoch 20/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 434ms/step - loss: 3.4057 - mae: 1.5287 - mse: 3.4057 - r2_score: -736.2550\n",
      "Epoch 21/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 438ms/step - loss: 3.4637 - mae: 1.5424 - mse: 3.4637 - r2_score: -736.1046\n",
      "Epoch 22/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 433ms/step - loss: 3.3210 - mae: 1.5123 - mse: 3.3210 - r2_score: -741.7464\n",
      "Epoch 23/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 441ms/step - loss: 3.2649 - mae: 1.4980 - mse: 3.2649 - r2_score: -735.0485\n",
      "Epoch 24/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 439ms/step - loss: 3.3599 - mae: 1.5180 - mse: 3.3599 - r2_score: -732.9835\n",
      "Epoch 25/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 439ms/step - loss: 3.4030 - mae: 1.5307 - mse: 3.4030 - r2_score: -749.2885\n",
      "Epoch 26/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 438ms/step - loss: 3.3117 - mae: 1.5071 - mse: 3.3117 - r2_score: -741.7928\n",
      "Epoch 27/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 437ms/step - loss: 3.4535 - mae: 1.5419 - mse: 3.4535 - r2_score: -748.6691\n",
      "Epoch 28/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 451ms/step - loss: 3.3470 - mae: 1.5261 - mse: 3.3470 - r2_score: -758.8717\n",
      "Epoch 29/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 437ms/step - loss: 3.4170 - mae: 1.5257 - mse: 3.4170 - r2_score: -729.9245\n",
      "Epoch 30/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 436ms/step - loss: 3.3840 - mae: 1.5339 - mse: 3.3840 - r2_score: -760.6152\n",
      "Epoch 31/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 438ms/step - loss: 3.3878 - mae: 1.5325 - mse: 3.3878 - r2_score: -745.0844\n",
      "Epoch 32/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 445ms/step - loss: 3.2379 - mae: 1.4905 - mse: 3.2379 - r2_score: -733.7131\n",
      "Epoch 33/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 463ms/step - loss: 3.4422 - mae: 1.5282 - mse: 3.4422 - r2_score: -716.9290\n",
      "Epoch 34/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 475ms/step - loss: 3.3384 - mae: 1.5129 - mse: 3.3384 - r2_score: -738.3775\n",
      "Epoch 35/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 444ms/step - loss: 3.3856 - mae: 1.5296 - mse: 3.3856 - r2_score: -744.2746\n",
      "Epoch 36/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 454ms/step - loss: 3.2643 - mae: 1.5102 - mse: 3.2643 - r2_score: -760.7458\n",
      "Epoch 37/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 452ms/step - loss: 3.4440 - mae: 1.5390 - mse: 3.4440 - r2_score: -739.9789\n",
      "Epoch 38/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 442ms/step - loss: 3.3892 - mae: 1.5163 - mse: 3.3892 - r2_score: -720.4637\n",
      "Epoch 39/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 449ms/step - loss: 3.3323 - mae: 1.5089 - mse: 3.3323 - r2_score: -723.5378\n",
      "Epoch 40/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 444ms/step - loss: 3.4029 - mae: 1.5383 - mse: 3.4029 - r2_score: -757.9996\n",
      "Epoch 41/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 465ms/step - loss: 3.3725 - mae: 1.5275 - mse: 3.3725 - r2_score: -745.1647\n",
      "Epoch 42/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 455ms/step - loss: 3.4188 - mae: 1.5222 - mse: 3.4188 - r2_score: -713.0070\n",
      "Epoch 43/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 442ms/step - loss: 3.4139 - mae: 1.5226 - mse: 3.4139 - r2_score: -726.4176\n",
      "Epoch 44/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 456ms/step - loss: 3.4395 - mae: 1.5349 - mse: 3.4395 - r2_score: -733.7837\n",
      "Epoch 45/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 442ms/step - loss: 3.3340 - mae: 1.5107 - mse: 3.3340 - r2_score: -731.1004\n",
      "Epoch 46/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 447ms/step - loss: 3.4112 - mae: 1.5226 - mse: 3.4112 - r2_score: -723.1996\n",
      "Epoch 47/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 443ms/step - loss: 3.4430 - mae: 1.5350 - mse: 3.4430 - r2_score: -730.8767\n",
      "Epoch 48/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 447ms/step - loss: 3.4590 - mae: 1.5336 - mse: 3.4590 - r2_score: -718.7933\n",
      "Epoch 49/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 444ms/step - loss: 3.3938 - mae: 1.5223 - mse: 3.3938 - r2_score: -733.5471\n",
      "Epoch 50/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 444ms/step - loss: 3.3519 - mae: 1.5188 - mse: 3.3519 - r2_score: -738.6310\n",
      "Epoch 51/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 462ms/step - loss: 3.3936 - mae: 1.5300 - mse: 3.3936 - r2_score: -739.6941\n",
      "Epoch 52/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 456ms/step - loss: 3.3884 - mae: 1.5263 - mse: 3.3884 - r2_score: -737.1486\n",
      "Epoch 53/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 443ms/step - loss: 3.3698 - mae: 1.5151 - mse: 3.3698 - r2_score: -722.5027\n",
      "Epoch 54/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 443ms/step - loss: 3.3119 - mae: 1.5124 - mse: 3.3119 - r2_score: -738.8278\n",
      "Epoch 55/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 445ms/step - loss: 3.3531 - mae: 1.5117 - mse: 3.3531 - r2_score: -717.8955\n",
      "Epoch 56/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 449ms/step - loss: 3.3426 - mae: 1.5160 - mse: 3.3426 - r2_score: -738.6669\n",
      "Epoch 57/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 443ms/step - loss: 3.3223 - mae: 1.5184 - mse: 3.3223 - r2_score: -739.6270\n",
      "Epoch 58/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 444ms/step - loss: 3.3418 - mae: 1.5209 - mse: 3.3418 - r2_score: -739.5841\n",
      "Epoch 59/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 442ms/step - loss: 3.4175 - mae: 1.5267 - mse: 3.4175 - r2_score: -720.1044\n",
      "Epoch 60/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 444ms/step - loss: 3.4449 - mae: 1.5345 - mse: 3.4449 - r2_score: -722.1866\n",
      "Epoch 61/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 440ms/step - loss: 3.3765 - mae: 1.5169 - mse: 3.3765 - r2_score: -725.4775\n",
      "Epoch 62/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 442ms/step - loss: 3.2959 - mae: 1.5002 - mse: 3.2959 - r2_score: -719.7383\n",
      "Epoch 63/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 449ms/step - loss: 3.3306 - mae: 1.5169 - mse: 3.3306 - r2_score: -755.8964\n",
      "Epoch 63: early stopping\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 221ms/step - loss: 2.6171 - mae: 1.4067 - mse: 3.0888 - r2_score: -543.6416\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.8900 - mae: 1.0288 - mse: 1.8900 - r2_score: -425.2760\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 1.6823 - mae: 0.9974 - mse: 1.6823 - r2_score: -396.3711\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 232ms/step - loss: 1.5540 - mae: 0.9621 - mse: 1.5540 - r2_score: -349.9922\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 1.6056 - mae: 0.9667 - mse: 1.6056 - r2_score: -360.4879\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.5398 - mae: 0.9484 - mse: 1.5398 - r2_score: -339.8748\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.5273 - mae: 0.9390 - mse: 1.5273 - r2_score: -343.7285\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 229ms/step - loss: 1.4388 - mae: 0.9213 - mse: 1.4388 - r2_score: -327.0029\n",
      "Epoch 9/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 230ms/step - loss: 1.4779 - mae: 0.9429 - mse: 1.4779 - r2_score: -314.4048\n",
      "Epoch 10/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.5020 - mae: 0.9362 - mse: 1.5020 - r2_score: -314.8189\n",
      "Epoch 11/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 1.3674 - mae: 0.9101 - mse: 1.3674 - r2_score: -313.6203\n",
      "Epoch 12/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 1.3451 - mae: 0.8984 - mse: 1.3451 - r2_score: -322.2683\n",
      "Epoch 13/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 232ms/step - loss: 1.3632 - mae: 0.9054 - mse: 1.3632 - r2_score: -307.3507\n",
      "Epoch 14/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 1.3442 - mae: 0.8924 - mse: 1.3442 - r2_score: -313.2749\n",
      "Epoch 15/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 1.3013 - mae: 0.8830 - mse: 1.3013 - r2_score: -304.6421\n",
      "Epoch 16/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 1.3962 - mae: 0.9125 - mse: 1.3962 - r2_score: -307.3083\n",
      "Epoch 17/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step - loss: 1.3911 - mae: 0.9029 - mse: 1.3911 - r2_score: -300.7143\n",
      "Epoch 18/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 1.3078 - mae: 0.8942 - mse: 1.3078 - r2_score: -299.1396\n",
      "Epoch 19/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.3143 - mae: 0.8867 - mse: 1.3143 - r2_score: -295.6475\n",
      "Epoch 20/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 1.3603 - mae: 0.9013 - mse: 1.3603 - r2_score: -299.3548\n",
      "Epoch 21/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 1.4481 - mae: 0.9259 - mse: 1.4481 - r2_score: -295.3272\n",
      "Epoch 22/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 232ms/step - loss: 1.3817 - mae: 0.9095 - mse: 1.3817 - r2_score: -292.7554\n",
      "Epoch 23/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 1.3032 - mae: 0.8824 - mse: 1.3032 - r2_score: -293.8137\n",
      "Epoch 24/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 1.3293 - mae: 0.8868 - mse: 1.3293 - r2_score: -292.3503\n",
      "Epoch 25/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 1.2682 - mae: 0.8774 - mse: 1.2682 - r2_score: -294.4897\n",
      "Epoch 26/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step - loss: 1.3121 - mae: 0.8911 - mse: 1.3121 - r2_score: -288.6911\n",
      "Epoch 27/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 1.3230 - mae: 0.8829 - mse: 1.3230 - r2_score: -288.0729\n",
      "Epoch 28/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 229ms/step - loss: 1.3117 - mae: 0.8894 - mse: 1.3117 - r2_score: -292.4843\n",
      "Epoch 29/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 221ms/step - loss: 1.3415 - mae: 0.8876 - mse: 1.3415 - r2_score: -289.3965\n",
      "Epoch 30/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 221ms/step - loss: 1.2928 - mae: 0.8812 - mse: 1.2928 - r2_score: -291.8185\n",
      "Epoch 31/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 1.2664 - mae: 0.8782 - mse: 1.2664 - r2_score: -288.6715\n",
      "Epoch 32/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 237ms/step - loss: 1.2825 - mae: 0.8712 - mse: 1.2825 - r2_score: -287.3437\n",
      "Epoch 33/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 240ms/step - loss: 1.2797 - mae: 0.8764 - mse: 1.2797 - r2_score: -287.0681\n",
      "Epoch 34/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 243ms/step - loss: 1.2645 - mae: 0.8731 - mse: 1.2645 - r2_score: -290.4836\n",
      "Epoch 35/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 231ms/step - loss: 1.2301 - mae: 0.8683 - mse: 1.2301 - r2_score: -282.7289\n",
      "Epoch 36/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.2524 - mae: 0.8610 - mse: 1.2524 - r2_score: -287.2178\n",
      "Epoch 37/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 220ms/step - loss: 1.3186 - mae: 0.8825 - mse: 1.3186 - r2_score: -288.2117\n",
      "Epoch 38/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 1.2910 - mae: 0.8780 - mse: 1.2910 - r2_score: -288.4502\n",
      "Epoch 39/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 1.2747 - mae: 0.8670 - mse: 1.2747 - r2_score: -281.8673\n",
      "Epoch 40/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 221ms/step - loss: 1.2638 - mae: 0.8683 - mse: 1.2638 - r2_score: -286.3060\n",
      "Epoch 41/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 1.2791 - mae: 0.8747 - mse: 1.2791 - r2_score: -284.9011\n",
      "Epoch 42/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 1.2784 - mae: 0.8702 - mse: 1.2784 - r2_score: -285.9112\n",
      "Epoch 43/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step - loss: 1.2654 - mae: 0.8716 - mse: 1.2654 - r2_score: -281.7268\n",
      "Epoch 44/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.3119 - mae: 0.8808 - mse: 1.3119 - r2_score: -284.2271\n",
      "Epoch 45/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 1.2544 - mae: 0.8663 - mse: 1.2544 - r2_score: -280.6479\n",
      "Epoch 46/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step - loss: 1.2906 - mae: 0.8748 - mse: 1.2906 - r2_score: -279.1639\n",
      "Epoch 47/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 1.2314 - mae: 0.8625 - mse: 1.2314 - r2_score: -279.5802\n",
      "Epoch 48/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 1.2755 - mae: 0.8713 - mse: 1.2755 - r2_score: -281.1031\n",
      "Epoch 49/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 1.2549 - mae: 0.8706 - mse: 1.2549 - r2_score: -280.2636\n",
      "Epoch 50/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 240ms/step - loss: 1.2311 - mae: 0.8682 - mse: 1.2311 - r2_score: -280.7864\n",
      "Epoch 51/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 1.2301 - mae: 0.8588 - mse: 1.2301 - r2_score: -282.4189\n",
      "Epoch 52/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 232ms/step - loss: 1.3340 - mae: 0.8765 - mse: 1.3340 - r2_score: -275.3062\n",
      "Epoch 53/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 1.2585 - mae: 0.8651 - mse: 1.2585 - r2_score: -282.5393\n",
      "Epoch 54/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 1.2476 - mae: 0.8626 - mse: 1.2476 - r2_score: -280.9791\n",
      "Epoch 55/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 1.2453 - mae: 0.8722 - mse: 1.2453 - r2_score: -283.9317\n",
      "Epoch 56/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.2387 - mae: 0.8530 - mse: 1.2387 - r2_score: -278.0099\n",
      "Epoch 57/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 1.2563 - mae: 0.8586 - mse: 1.2563 - r2_score: -282.9254\n",
      "Epoch 58/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 1.2205 - mae: 0.8531 - mse: 1.2205 - r2_score: -275.5742\n",
      "Epoch 59/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.2731 - mae: 0.8806 - mse: 1.2731 - r2_score: -281.5658\n",
      "Epoch 60/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 1.2384 - mae: 0.8653 - mse: 1.2384 - r2_score: -278.3569\n",
      "Epoch 61/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.2064 - mae: 0.8542 - mse: 1.2064 - r2_score: -280.8325\n",
      "Epoch 62/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step - loss: 1.2198 - mae: 0.8463 - mse: 1.2198 - r2_score: -277.8053\n",
      "Epoch 63/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 229ms/step - loss: 1.2305 - mae: 0.8615 - mse: 1.2305 - r2_score: -276.8271\n",
      "Epoch 64/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 1.2312 - mae: 0.8579 - mse: 1.2312 - r2_score: -276.4801\n",
      "Epoch 65/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.2473 - mae: 0.8665 - mse: 1.2473 - r2_score: -277.2955\n",
      "Epoch 66/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 232ms/step - loss: 1.2262 - mae: 0.8523 - mse: 1.2262 - r2_score: -278.4760\n",
      "Epoch 67/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 1.2037 - mae: 0.8547 - mse: 1.2037 - r2_score: -278.0141\n",
      "Epoch 68/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 230ms/step - loss: 1.1855 - mae: 0.8441 - mse: 1.1855 - r2_score: -276.8786\n",
      "Epoch 69/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 229ms/step - loss: 1.2351 - mae: 0.8671 - mse: 1.2351 - r2_score: -278.1449\n",
      "Epoch 70/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step - loss: 1.2258 - mae: 0.8484 - mse: 1.2258 - r2_score: -274.4479\n",
      "Epoch 71/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 1.2306 - mae: 0.8635 - mse: 1.2306 - r2_score: -274.9703\n",
      "Epoch 72/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 1.2953 - mae: 0.8759 - mse: 1.2953 - r2_score: -276.5361\n",
      "Epoch 73/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 1.2839 - mae: 0.8718 - mse: 1.2839 - r2_score: -274.1278\n",
      "Epoch 74/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.2165 - mae: 0.8562 - mse: 1.2165 - r2_score: -277.4681\n",
      "Epoch 75/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 1.2273 - mae: 0.8611 - mse: 1.2273 - r2_score: -275.2969\n",
      "Epoch 76/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.2489 - mae: 0.8676 - mse: 1.2489 - r2_score: -273.9422\n",
      "Epoch 77/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 1.2014 - mae: 0.8484 - mse: 1.2014 - r2_score: -274.6609\n",
      "Epoch 78/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 221ms/step - loss: 1.2344 - mae: 0.8617 - mse: 1.2344 - r2_score: -274.3565\n",
      "Epoch 79/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 229ms/step - loss: 1.2488 - mae: 0.8561 - mse: 1.2488 - r2_score: -273.8766\n",
      "Epoch 80/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 1.2194 - mae: 0.8619 - mse: 1.2194 - r2_score: -275.1316\n",
      "Epoch 81/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 1.2834 - mae: 0.8756 - mse: 1.2834 - r2_score: -273.7179\n",
      "Epoch 82/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 1.1950 - mae: 0.8457 - mse: 1.1950 - r2_score: -280.8792\n",
      "Epoch 83/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.3048 - mae: 0.8751 - mse: 1.3048 - r2_score: -271.9848\n",
      "Epoch 84/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 1.2322 - mae: 0.8568 - mse: 1.2322 - r2_score: -273.2885\n",
      "Epoch 85/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 1.2235 - mae: 0.8513 - mse: 1.2235 - r2_score: -275.4830\n",
      "Epoch 86/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 1.2071 - mae: 0.8608 - mse: 1.2071 - r2_score: -274.9109\n",
      "Epoch 87/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 1.2131 - mae: 0.8560 - mse: 1.2131 - r2_score: -275.4465\n",
      "Epoch 88/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 234ms/step - loss: 1.2362 - mae: 0.8514 - mse: 1.2362 - r2_score: -273.6894\n",
      "Epoch 89/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 1.2019 - mae: 0.8471 - mse: 1.2019 - r2_score: -273.3879\n",
      "Epoch 90/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.1843 - mae: 0.8488 - mse: 1.1843 - r2_score: -271.6952\n",
      "Epoch 91/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 1.2409 - mae: 0.8658 - mse: 1.2409 - r2_score: -274.4047\n",
      "Epoch 92/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 1.2362 - mae: 0.8549 - mse: 1.2362 - r2_score: -273.5976\n",
      "Epoch 93/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 1.2262 - mae: 0.8530 - mse: 1.2262 - r2_score: -274.7452\n",
      "Epoch 94/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 220ms/step - loss: 1.2269 - mae: 0.8551 - mse: 1.2269 - r2_score: -272.5508\n",
      "Epoch 95/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 229ms/step - loss: 1.2485 - mae: 0.8618 - mse: 1.2485 - r2_score: -270.9981\n",
      "Epoch 96/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step - loss: 1.2601 - mae: 0.8594 - mse: 1.2601 - r2_score: -275.8152\n",
      "Epoch 97/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 1.1623 - mae: 0.8377 - mse: 1.1623 - r2_score: -271.7903\n",
      "Epoch 98/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 230ms/step - loss: 1.2079 - mae: 0.8569 - mse: 1.2079 - r2_score: -271.4850\n",
      "Epoch 99/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 235ms/step - loss: 1.2276 - mae: 0.8567 - mse: 1.2276 - r2_score: -273.6379\n",
      "Epoch 100/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 238ms/step - loss: 1.2267 - mae: 0.8609 - mse: 1.2267 - r2_score: -272.8579\n",
      "Restoring model weights from the end of the best epoch: 91.\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 13.8564 - mae: 3.4945 - mse: 13.8564 - r2_score: -469.1530\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 10.7984 - mae: 3.0248 - mse: 10.7984 - r2_score: -282.3827\n",
      "\n",
      "=== Phase 1: Test Evaluation Results ===\n",
      "  loss    mae    mse  r2_score\n",
      "1.4730 0.9057 1.4730  -43.4798\n",
      "\n",
      "=== Phase 3: Train Evaluation Results ===\n",
      "   loss    mae     mse  r2_score\n",
      "13.2063 3.3981 13.2063 -434.2722\n",
      "\n",
      "=== Phase 3: Test Evaluation Results ===\n",
      "   loss    mae     mse  r2_score\n",
      "10.7487 3.0348 10.7487 -273.5351\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "XD_np = np.array(XD)\n",
    "labels_np = np.array(labels)\n",
    "\n",
    "# Remove samples that don't have labels\n",
    "index = ~np.isnan(labels_np)\n",
    "labels_np = labels_np[index]\n",
    "XD_np = XD_np[index]\n",
    "\n",
    "# Filter the corresponding SMILES\n",
    "filtered_smiles = smiles.iloc[index].values\n",
    "\n",
    "# Create a Dataset using SMILES as identifiers\n",
    "dataset = NumpyDataset(X=XD_np, y=labels_np, ids=filtered_smiles)\n",
    "\n",
    "# Create a ScaffoldSplitter\n",
    "splitter = ScaffoldSplitter()\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(\n",
    "    dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1\n",
    ")\n",
    "\n",
    "# Extract training and test indices\n",
    "train_indices = train_dataset.ids\n",
    "test_indices = test_dataset.ids\n",
    "smiles_to_index = {smile: idx for idx, smile in enumerate(filtered_smiles)}\n",
    "\n",
    "# Convert SMILES to indices\n",
    "train_indices = np.array([smiles_to_index[smile] for smile in train_indices])\n",
    "test_indices = np.array([smiles_to_index[smile] for smile in test_indices])\n",
    "\n",
    "# Split the data based on the indices\n",
    "X_train, X_test = XD_np[train_indices], XD_np[test_indices]\n",
    "y_train, y_test = labels_np[train_indices], labels_np[test_indices]\n",
    "\n",
    "print(f\"Number of training samples: {X_train.shape[0]}\")\n",
    "print(f\"Number of test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# === Define Optimizer for Interaction Model ===\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "interactionModel.compile(optimizer=opt, loss='mse', metrics=METRICS_REGRESSION)\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=30, verbose=1, mode='min', restore_best_weights=True)\n",
    "reduce_lr = LearningRateScheduler(lambda epoch: 1e-3 * 0.9 ** epoch)\n",
    "\n",
    "# === Phase 1: Train Interaction Model ===\n",
    "interactionModel.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=256, epochs=100,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Phase 1 on test data\n",
    "test_eval_phase1 = interactionModel.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# === Feature Extraction ===\n",
    "feature_train = model_feature.predict(X_train)\n",
    "feature_test = model_feature.predict(X_test)\n",
    "\n",
    "# === Phase 2: Train Model Phase 2 ===\n",
    "opt_phaz2 = optimizers.Adam(learning_rate=0.001)\n",
    "model_phaz2.compile(optimizer=opt_phaz2, loss='mse', metrics=METRICS_REGRESSION)\n",
    "model_phaz2.fit(\n",
    "    feature_train, y_train,\n",
    "    batch_size=256, epochs=100,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    ")\n",
    "\n",
    "# === Freeze Transformer Blocks ===\n",
    "new_transformer_block.set_weights(transformer_block1.get_weights())\n",
    "new_transformer_block2.set_weights(transformer_block2.get_weights())\n",
    "\n",
    "# === Phase 3: Train Model Phase 3 ===\n",
    "opt_phaz3 = optimizers.Adam(learning_rate=0.0001)\n",
    "model_phaz3.compile(optimizer=opt_phaz3, loss='mse', metrics=METRICS_REGRESSION)\n",
    "\n",
    "model_phaz3.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=256, epochs=100,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Phase 3 on both training and testing data\n",
    "train_eval_phase3 = model_phaz3.evaluate(X_train, y_train, verbose=1)\n",
    "test_eval_phase3 = model_phaz3.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# === Function to Format Results ===\n",
    "def format_results(phase1_result, phase3_train_result, phase3_test_result):\n",
    "    # Define Metric Names\n",
    "    phase1_metrics = ['loss', 'mae', 'mse', 'r2_score']\n",
    "    phase3_metrics = ['loss', 'mae', 'mse', 'r2_score']\n",
    "\n",
    "    # Create DataFrames\n",
    "    phase1_test_df = pd.DataFrame([phase1_result], columns=phase1_metrics)\n",
    "    phase3_train_df = pd.DataFrame([phase3_train_result], columns=phase3_metrics)\n",
    "    phase3_test_df = pd.DataFrame([test_eval_phase3], columns=phase3_metrics)\n",
    "\n",
    "    # Display the Results\n",
    "    print('\\n=== Phase 1: Test Evaluation Results ===')\n",
    "    print(phase1_test_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "\n",
    "    print('\\n=== Phase 3: Train Evaluation Results ===')\n",
    "    print(phase3_train_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "\n",
    "    print('\\n=== Phase 3: Test Evaluation Results ===')\n",
    "    print(phase3_test_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "\n",
    "# === Display Results ===\n",
    "format_results(test_eval_phase1, train_eval_phase3, test_eval_phase3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test R2 Score: -7.0134\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "import sklearn.metrics\n",
    "\n",
    "# === Scatter Plot of Predictions vs True Values ===\n",
    "y_pred_test = model_phaz3.predict(X_test).flatten()\n",
    "r2_val = sklearn.metrics.r2_score(y_test.flatten(), y_pred_test)\n",
    "print(f\"Test R2 Score: {r2_val:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=y_test.flatten(), y=y_pred_test)\n",
    "min_val = min(y_test.min(), y_pred_test.min())\n",
    "max_val = max(y_test.max(), y_pred_test.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('True vs Predicted')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
