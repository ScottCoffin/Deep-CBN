{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w71ntxmiP5Cs"
   },
   "source": [
    "#Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2mc3-OTP210",
    "outputId": "1bcf264f-343b-4610-b609-3c89f2b742f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "from tensorflow.keras import optimizers, layers, regularizers, metrics\n",
    "from tensorflow.keras.layers import (\n",
    "    Lambda, Input, Reshape, Activation, Concatenate, Dense, Dropout,\n",
    "    BatchNormalization, Conv1D, GlobalMaxPooling1D, LayerNormalization, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "from deepchem.data import NumpyDataset\n",
    "from deepchem.splits import ScaffoldSplitter\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWrFzNjSQCtg"
   },
   "source": [
    "### Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "kXFLqk2vja5M",
    "outputId": "3e31a23d-130b-48c7-cfd4-bc76cd6c75bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:17:04] SMILES Parse Error: syntax error while parsing: O|[Al](|O)(|O)|[Al](|O)(|O)Cl\n",
      "[20:17:04] SMILES Parse Error: Failed parsing SMILES 'O|[Al](|O)(|O)|[Al](|O)(|O)Cl' for input: 'O|[Al](|O)(|O)|[Al](|O)(|O)Cl'\n",
      "[20:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:04] Explicit valence for atom # 3 Si, 8, is greater than permitted\n",
      "[20:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:17:06] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows before drop: 4,657\n",
      "Rows after NA drop:  4,222\n",
      "Rows after invalid SMILES drop:  4,218\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CAS</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>log_RfD_mg_kg_d</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100-00-5</td>\n",
       "      <td>C1=CC(=CC=C1[N+](=O)[O-])Cl</td>\n",
       "      <td>-3.060745</td>\n",
       "      <td>-3.060745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>100-01-6</td>\n",
       "      <td>C1=CC(=CC=C1N)[N+](=O)[O-]</td>\n",
       "      <td>-2.193802</td>\n",
       "      <td>-2.193802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10004-44-1</td>\n",
       "      <td>CC1=CC(=O)NO1</td>\n",
       "      <td>-2.015260</td>\n",
       "      <td>-2.015260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1001354-72-8</td>\n",
       "      <td>CCCCC(C(CC)N)O</td>\n",
       "      <td>-1.769892</td>\n",
       "      <td>-1.769892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>10016-20-3</td>\n",
       "      <td>C(C1C2C(C(C(O1)OC3C(OC(C(C3O)O)OC4C(OC(C(C4O)O...</td>\n",
       "      <td>1.032814</td>\n",
       "      <td>1.032814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           CAS  \\\n",
       "0           1      100-00-5   \n",
       "1           2      100-01-6   \n",
       "2           3    10004-44-1   \n",
       "3           4  1001354-72-8   \n",
       "4           6    10016-20-3   \n",
       "\n",
       "                                              SMILES  log_RfD_mg_kg_d  \\\n",
       "0                        C1=CC(=CC=C1[N+](=O)[O-])Cl        -3.060745   \n",
       "1                         C1=CC(=CC=C1N)[N+](=O)[O-]        -2.193802   \n",
       "2                                      CC1=CC(=O)NO1        -2.015260   \n",
       "3                                     CCCCC(C(CC)N)O        -1.769892   \n",
       "4  C(C1C2C(C(C(O1)OC3C(OC(C(C3O)O)OC4C(OC(C(C4O)O...         1.032814   \n",
       "\n",
       "     target  \n",
       "0 -3.060745  \n",
       "1 -2.193802  \n",
       "2 -2.015260  \n",
       "3 -1.769892  \n",
       "4  1.032814  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Loading and Preparing Data\n",
    "data = pd.read_csv('../Data/SMILES_RfD.csv')\n",
    "\n",
    "# Drop any rows where the 'SMILES' column is missing or empty\n",
    "data_clean = data.dropna(subset=['SMILES'])\n",
    "data_clean = data_clean[data_clean['SMILES'].str.strip() != \"\"]\n",
    "\n",
    "data_clean = data_clean.reset_index(drop=True)\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "# Drop rows where the SMILES column is NaN or invalid\n",
    "valid_mask = data_clean['SMILES'].apply(\n",
    "    lambda s: isinstance(s, str) and Chem.MolFromSmiles(s) is not None\n",
    ")\n",
    "data_clean_valid = data_clean[valid_mask]\n",
    "\n",
    "print(f\"\\nRows before drop: {len(data):,}\")\n",
    "print(f\"Rows after NA drop:  {len(data_clean):,}\")\n",
    "print(f\"Rows after invalid SMILES drop:  {len(data_clean_valid):,}\")\n",
    "\n",
    "\n",
    "# Preview the cleaned data\n",
    "data_clean_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AFxos0tgQC9s"
   },
   "outputs": [],
   "source": [
    "smiles = data_clean_valid['SMILES']\n",
    "labels = data_clean_valid['target']\n",
    "\n",
    "# Dictionary to convert SMILES characters into numeric values\n",
    "smiles_dict = {\n",
    "    \"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n",
    "    \"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n",
    "    \"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n",
    "    \"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n",
    "    \"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50, \"T\": 17, \"W\": 51,\n",
    "    \"V\": 18, \"Y\": 52, \"[\": 53, \"Z\": 19, \"]\": 54, \"\\\\\": 20, \"a\": 55, \"c\": 56,\n",
    "    \"b\": 21, \"e\": 57, \"d\": 22, \"g\": 58, \"f\": 23, \"i\": 59, \"h\": 24, \"m\": 60,\n",
    "    \"l\": 25, \"o\": 61, \"n\": 26, \"s\": 62, \"r\": 27, \"u\": 63, \"t\": 28, \"y\": 64,\n",
    "    \" \": 65, \":\": 66, \",\": 67, \"p\": 68, \"j\": 69, \"*\": 70\n",
    "}\n",
    "\n",
    "def label_smiles(line, MAX_SMI_LEN, smi_ch_ind):\n",
    "    X = np.zeros(MAX_SMI_LEN, dtype=int)\n",
    "    for i, ch in enumerate(line[:MAX_SMI_LEN]):\n",
    "        if ch in smi_ch_ind:\n",
    "            X[i] = smi_ch_ind[ch]\n",
    "    return X\n",
    "\n",
    "MAX_SMI_LEN = 100  # You can adjust this based on your needs\n",
    "XD = np.array([label_smiles(str(smi), MAX_SMI_LEN, smiles_dict) for smi in smiles])\n",
    "labels = labels.values\n",
    "\n",
    "# Convert to categorical\n",
    "XD = to_categorical(XD, num_classes=71)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qmjyj91JeXsG"
   },
   "source": [
    "# phaze1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A2VgoLaMQN0X",
    "outputId": "d183f5ed-45d0-496a-8aa7-9347ac07ec00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_feature\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"model_feature\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_39 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m9,152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_40 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_41 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_23\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_23\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_23 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_92 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_40 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_93 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_41 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_94 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_95 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,961</span> (847.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m216,961\u001b[0m (847.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">215,425</span> (841.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m215,425\u001b[0m (841.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"interactionModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"interactionModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ model_feature (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">216,961</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_11     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ model_feature (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m58,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_23 (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │       \u001b[38;5;34m216,961\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_11     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">275,457</span> (1.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m275,457\u001b[0m (1.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">273,921</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m273,921\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature model definition \n",
    "XDinput = Input(shape=(100, 71), name='XDinput')\n",
    "encode_smiles = Conv1D(filters=64, kernel_size=2, activation='relu', padding='valid', strides=1)(XDinput)  # (99,64)\n",
    "encode_smiles = Conv1D(filters=64, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)  # (96,64)\n",
    "encode_smiles = Conv1D(filters=128, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)  # (93,128)\n",
    "model_feature = Model(inputs=XDinput, outputs=encode_smiles, name='model_feature')\n",
    "model_feature.summary()\n",
    "\n",
    "# Prediction model definition\n",
    "input_extracted_feature = Input(shape=(93, 128))\n",
    "FC1 = Dense(512, activation='relu')(input_extracted_feature)\n",
    "FC1 = BatchNormalization()(FC1)\n",
    "FC2 = Dropout(0.1)(FC1)\n",
    "FC3 = Dense(256, activation='relu')(FC2)\n",
    "FC3 = BatchNormalization()(FC3)\n",
    "FC4 = Dropout(0.1)(FC3)\n",
    "FC5 = Dense(64, activation='relu')(FC4)\n",
    "predictions = Dense(1, activation='linear')(FC5)\n",
    "model_pred = Model(inputs=input_extracted_feature, outputs=predictions)\n",
    "model_pred.summary()\n",
    "\n",
    "# Full model definition with added Pooling layer\n",
    "interaction_input = XDinput\n",
     "encoded_features = model_feature(interaction_input)  # Output: (None, 93, 128)\n",
     "predicted_output = model_pred(encoded_features)      # Output: (None, 93, 1)\n",
    "\n",
    "# Adding Pooling layer to reduce dimensionality\n",
     "pooled_output = GlobalAveragePooling1D()(predicted_output)  # Output: (None, 1)\n",
    "\n",
    "# Final model definition\n",
    "interactionModel = Model(inputs=interaction_input, outputs=pooled_output, name='interactionModel')\n",
    "interactionModel.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW4Dc7MfQWbn"
   },
   "source": [
    "### BiFormer Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "C5EAZalFQWpH"
   },
   "outputs": [],
   "source": [
    "# Define custom layers for 1D processing\n",
    "class TopkRouting(layers.Layer):\n",
    "    def __init__(self, qk_dim, topk=16, qk_scale=None, param_routing=False, diff_routing=False):\n",
    "        super().__init__()\n",
    "        self.topk = topk\n",
    "        self.qk_dim = qk_dim\n",
    "        self.scale = qk_scale if qk_scale is not None else qk_dim**-0.5\n",
    "        self.diff_routing = diff_routing\n",
    "        if param_routing:\n",
    "            self.emb = layers.Dense(qk_dim)\n",
    "        else:\n",
    "            self.emb = lambda x: x\n",
    "        self.routing_act = lambda x, axis: tf.nn.softmax(x, axis=axis)\n",
    "\n",
    "    def call(self, query, key, training=None):\n",
    "        if not self.diff_routing:\n",
    "            query = tf.stop_gradient(query)\n",
    "            key = tf.stop_gradient(key)\n",
    "        query_hat = self.emb(query)\n",
    "        key_hat = self.emb(key)\n",
    "\n",
    "        attn_logit = tf.einsum('bnc,bqc->bnq', query_hat*self.scale, key_hat)  # Adjusted for 1D\n",
    "        topk_attn_logit, topk_index = tf.math.top_k(attn_logit, k=self.topk, sorted=True)\n",
    "        r_weight = self.routing_act(topk_attn_logit, axis=-1)\n",
    "        return r_weight, topk_index\n",
    "\n",
    "def tf_gather_kv(kv, r_idx):\n",
    "    n = tf.shape(kv)[0]\n",
    "    p2 = tf.shape(kv)[1]\n",
    "    c_kv = tf.shape(kv)[2]\n",
    "    topk = tf.shape(r_idx)[2]\n",
    "\n",
    "    batch_idx = tf.reshape(tf.range(n), [n, 1, 1])\n",
    "    batch_idx = tf.tile(batch_idx, [1, p2, topk])\n",
    "    p2_idx = tf.reshape(tf.range(p2), [1, p2, 1])\n",
    "    p2_idx = tf.tile(p2_idx, [n, 1, topk])\n",
    "\n",
    "    gather_indices = tf.stack([batch_idx, p2_idx, r_idx], axis=-1)\n",
    "    gathered = tf.gather_nd(kv, gather_indices)\n",
    "    return gathered  # Shape: (n, p2, topk, c_kv)\n",
    "\n",
    "class KVGather(layers.Layer):\n",
    "    def __init__(self, mul_weight='none'):\n",
    "        super().__init__()\n",
    "        assert mul_weight in ['none', 'soft', 'hard']\n",
    "        self.mul_weight = mul_weight\n",
    "\n",
    "    def call(self, r_idx, r_weight, kv, training=None):\n",
    "        topk_kv = tf_gather_kv(kv, r_idx)\n",
    "        if self.mul_weight == 'soft':\n",
    "            r_weight_exp = tf.expand_dims(tf.expand_dims(r_weight, -1), -1)\n",
    "            topk_kv = topk_kv * r_weight_exp\n",
    "        return topk_kv\n",
    "\n",
    "class QKVLinear(layers.Layer):\n",
    "    def __init__(self, dim, qk_dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.qk_dim = qk_dim\n",
    "        self.qkv = layers.Dense(qk_dim + qk_dim + dim, use_bias=bias)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        qkv = self.qkv(x)\n",
    "        q, kv = tf.split(qkv, [self.qk_dim, self.qk_dim + self.dim], axis=-1)\n",
    "        return q, kv\n",
    "\n",
    "class BiLevelRoutingAttention(layers.Layer):\n",
    "    def __init__(self, dim, n_win=7, num_heads=8, qk_dim=None, qk_scale=None,\n",
    "                 kv_per_win=4, kv_downsample_ratio=4, kv_downsample_mode='identity',\n",
    "                 topk=4, param_attention=\"qkvo\", param_routing=False, diff_routing=False, soft_routing=False,\n",
    "                 side_dwconv=3, auto_pad=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_win = n_win\n",
    "        self.num_heads = num_heads\n",
    "        self.qk_dim = qk_dim if qk_dim is not None else dim\n",
    "        self.scale = qk_scale if qk_scale is not None else self.qk_dim**-0.5\n",
    "        self.topk = topk\n",
    "        self.param_routing = param_routing\n",
    "        self.diff_routing = diff_routing\n",
    "        self.soft_routing = soft_routing\n",
    "        self.auto_pad = auto_pad\n",
    "\n",
    "        # For 1D, we use Conv1D\n",
    "        if side_dwconv > 0:\n",
    "            self.lepe = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n",
    "        else:\n",
    "            self.lepe = lambda x: tf.zeros_like(x)\n",
    "\n",
    "        self.router = TopkRouting(qk_dim=self.qk_dim, topk=self.topk, qk_scale=self.scale,\n",
    "                                  param_routing=self.param_routing, diff_routing=self.diff_routing)\n",
    "\n",
    "        mul_weight = 'none'\n",
    "        if self.soft_routing:\n",
    "            mul_weight = 'soft'\n",
    "        self.kv_gather = KVGather(mul_weight=mul_weight)\n",
    "\n",
    "        if param_attention in ['qkvo', 'qkv']:\n",
    "            self.qkv = QKVLinear(self.dim, self.qk_dim)\n",
    "            if param_attention == 'qkvo':\n",
    "                self.wo = layers.Dense(self.dim)\n",
    "            else:\n",
    "                self.wo = lambda x: x\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported param_attention mode\")\n",
    "\n",
    "        self.attn_act = lambda x: tf.nn.softmax(x, axis=-1)\n",
    "        self.kv_down = lambda x: x  # identity for simplicity\n",
    "\n",
    "    def call(self, x, training=None, ret_attn_mask=False):\n",
    "        # Implementing full attention can be complex, so here we only keep the general structure\n",
    "        if ret_attn_mask:\n",
    "            return x, None, None, None\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class Attention(layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n",
    "        self.qkv = layers.Dense(dim*3, use_bias=qkv_bias)\n",
    "        self.attn_drop = layers.Dropout(attn_drop)\n",
    "        self.proj = layers.Dense(dim)\n",
    "        self.proj_drop = layers.Dropout(proj_drop)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        batch_size, seq_length, dim = tf.unstack(tf.shape(x))\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "\n",
    "        attn = tf.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn, training=training)\n",
    "        out = tf.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out, training=training)\n",
    "        return out\n",
    "\n",
    "class AttentionLePE(layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., side_dwconv=5):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n",
    "        self.qkv = layers.Dense(dim*3, use_bias=qkv_bias)\n",
    "        self.attn_drop = layers.Dropout(attn_drop)\n",
    "        self.proj = layers.Dense(dim)\n",
    "        self.proj_drop = layers.Dropout(proj_drop)\n",
    "        if side_dwconv > 0:\n",
    "            self.lepe = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n",
    "        else:\n",
    "            self.lepe = lambda x: tf.zeros_like(x)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        lepe_out = self.lepe(x)\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "\n",
    "        attn = tf.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn, training=training)\n",
    "        out = tf.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')\n",
    "        out = out + lepe_out\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out, training=training)\n",
    "        return out\n",
    "\n",
    "class PreNorm(layers.Layer):\n",
    "    def __init__(self, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.fn(self.norm(x), training=training)\n",
    "\n",
    "class MLP(layers.Layer):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = keras.Sequential([\n",
    "            layers.Dense(hidden_dim),\n",
    "            layers.Activation('gelu'),\n",
    "            layers.Dropout(rate=dropout),\n",
    "            layers.Dense(dim),\n",
    "            layers.Dropout(rate=dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.net(x, training=training)\n",
    "\n",
    "class DropPath(layers.Layer):\n",
    "    # Placeholder for DropPath, currently no-op\n",
    "    def __init__(self, drop_prob=0.):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if (not training) or self.drop_prob == 0.:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += tf.random.uniform(tf.shape(x), dtype=x.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        return tf.divide(x, keep_prob) * binary_tensor\n",
    "\n",
    "class Block(layers.Layer):\n",
    "    def __init__(self, dim, drop_path=0.1, layer_scale_init_value=-1,\n",
    "                 num_heads=8, n_win=7, qk_dim=128, qk_scale=None,\n",
    "                 kv_per_win=8, kv_downsample_ratio=1, kv_downsample_mode='identity',\n",
    "                 topk=8, param_attention=\"qkvo\", param_routing=True, diff_routing=False, soft_routing=True,\n",
    "                 mlp_ratio=4, mlp_dwconv=False, side_dwconv=5, before_attn_dwconv=3, pre_norm=True, auto_pad=True):\n",
    "        super().__init__()\n",
    "        qk_dim = qk_dim or dim\n",
    "\n",
    "        # For 1D, we use Conv1D\n",
    "        if before_attn_dwconv > 0:\n",
    "            self.pos_embed = layers.Conv1D(dim, kernel_size=before_attn_dwconv, padding='same', activation='relu')\n",
    "        else:\n",
    "            self.pos_embed = lambda x: tf.zeros_like(x)\n",
    "\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        if topk > 0:\n",
    "            self.attn = BiLevelRoutingAttention(dim=dim, num_heads=num_heads, n_win=n_win, qk_dim=qk_dim,\n",
    "                                               qk_scale=qk_scale, kv_per_win=kv_per_win, kv_downsample_ratio=kv_downsample_ratio,\n",
    "                                               kv_downsample_mode=kv_downsample_mode, topk=topk, param_attention=param_attention,\n",
    "                                               param_routing=param_routing, diff_routing=diff_routing, soft_routing=soft_routing,\n",
    "                                               side_dwconv=side_dwconv, auto_pad=auto_pad)\n",
    "        elif topk == -1:\n",
    "            self.attn = Attention(dim=dim, num_heads=num_heads, qk_scale=qk_scale)\n",
    "        elif topk == -2:\n",
    "            self.attn = AttentionLePE(dim=dim, num_heads=num_heads, qk_scale=qk_scale, side_dwconv=side_dwconv)\n",
    "        elif topk == 0:\n",
    "            # Pseudo attention\n",
    "            self.attn = keras.Sequential([\n",
    "                layers.Dense(dim),\n",
    "                layers.Conv1D(dim, kernel_size=5, padding='same', activation='relu'),\n",
    "                layers.Dense(dim)\n",
    "            ])\n",
    "\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        mlp_hidden_dim = int(mlp_ratio * dim)\n",
    "        mlp_layers = [layers.Dense(mlp_hidden_dim)]\n",
    "        if mlp_dwconv:\n",
    "            mlp_layers.append(layers.Conv1D(mlp_hidden_dim, kernel_size=3, padding='same', activation='relu'))\n",
    "        mlp_layers.append(layers.Activation('gelu'))\n",
    "        mlp_layers.append(layers.Dense(dim))\n",
    "        mlp_layers.insert(1, layers.Dropout(0.2)) # Add dropout after first Dense\n",
    "        mlp_layers.append(layers.Dropout(0.2))\n",
    "        self.mlp = keras.Sequential(mlp_layers)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else layers.Lambda(lambda x: x)\n",
    "\n",
    "        if layer_scale_init_value > 0:\n",
    "            self.use_layer_scale = True\n",
    "            self.gamma1 = self.add_weight(shape=(dim,),\n",
    "                                          initializer=tf.keras.initializers.Constant(layer_scale_init_value),\n",
    "                                          trainable=True)\n",
    "            self.gamma2 = self.add_weight(shape=(dim,),\n",
    "                                          initializer=tf.keras.initializers.Constant(layer_scale_init_value),\n",
    "                                          trainable=True)\n",
    "        else:\n",
    "            self.use_layer_scale = False\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        x = x + self.pos_embed(x)  # Add positional embedding\n",
    "\n",
    "        if self.pre_norm:\n",
    "            if self.use_layer_scale:\n",
    "                x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x), training=training), training=training)\n",
    "                x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x), training=training), training=training)\n",
    "            else:\n",
    "                x = x + self.drop_path(self.attn(self.norm1(x), training=training), training=training)\n",
    "                x = x + self.drop_path(self.mlp(self.norm2(x), training=training), training=training)\n",
    "        else:\n",
    "            if self.use_layer_scale:\n",
    "                tmp = x + self.drop_path(self.gamma1 * self.attn(x, training=training), training=training)\n",
    "                x = self.norm1(tmp)\n",
    "                tmp = x + self.drop_path(self.gamma2 * self.mlp(x, training=training), training=training)\n",
    "                x = self.norm2(tmp)\n",
    "            else:\n",
    "                tmp = x + self.drop_path(self.attn(x, training=training), training=training)\n",
    "                x = self.norm1(tmp)\n",
    "                tmp = x + self.drop_path(self.mlp(x, training=training), training=training)\n",
    "                x = self.norm2(tmp)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k41fqbmrQxbV"
   },
   "source": [
    "# phaze2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "qFxfd5e4QdV3",
    "outputId": "3ffefc83-bb87-45ff-cb54-e4535be8d055"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_12', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_13', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_26\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_26\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ input_layer_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ input_layer_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ difference (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ difference[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_12 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ input_layer_24[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_13 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ input_layer_24[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_12 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_13 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ difference (\u001b[38;5;33mLambda\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ difference[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">297,216</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m297,216\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">297,216</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m297,216\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameter settings for the block\n",
    "dim = 128   \n",
    "num_heads = 8\n",
    "mlp_dim = 128 * 3  \n",
    "depth = 2\n",
    "dim_head = 48\n",
    "\n",
    "# Create Phase 2 model\n",
    "input_phaz2 = Input(shape=(93, 128))  \n",
    "\n",
    "processed_input = input_phaz2\n",
    "\n",
    "# Define transformer blocks\n",
    "transformer_block1 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "\n",
    "# Apply the first transformer block\n",
    "transformer_output1 = transformer_block1(processed_input)\n",
    "# Compute the norm of the output along the last axis (preserving dimensions)\n",
    "transformer_output1 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1)\n",
    "\n",
    "# Apply the second transformer block\n",
    "transformer_output2 = transformer_block2(processed_input)\n",
    "# Compute the norm of the output along the last axis (preserving dimensions)\n",
    "transformer_output2 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2)\n",
    "\n",
    "# Combine the outputs of both transformer blocks along the last axis\n",
    "combined_outputs = Concatenate(axis=-1)([transformer_output1, transformer_output2])\n",
    "\n",
    "# Apply global average pooling to reduce the output to (batch_size, 2)\n",
    "pooled_outputs = GlobalAveragePooling1D()(combined_outputs)\n",
    "\n",
    "# Compute the difference between the two pooled outputs and reshape to (batch_size, 1)\n",
    "difference = Lambda(lambda x: tf.expand_dims(x[:, 0] - x[:, 1], axis=-1), name='difference')(pooled_outputs)\n",
    "\n",
    "# Apply sigmoid activation to normalize the output to the range [0, 1]\n",
    "condition_2 = Activation('sigmoid', name='activation_11')(difference)\n",
    "\n",
    "# Freeze the `model_feature` layers to prevent them from being trained\n",
    "model_feature.trainable = False\n",
    "\n",
    "# Define the complete model\n",
    "model_phaz2 = Model(inputs=input_phaz2, outputs=condition_2)\n",
    "model_phaz2.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcH6Y5x1e5OS"
   },
   "source": [
    "# phaze3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F6DTjT17Q1W5",
    "outputId": "6bcfc3f8-eeee-418e-cddd-31ff826c0ff5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_14', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_15', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer_27 True\n",
      "model_feature False\n",
      "block_14 False\n",
      "block_15 False\n",
      "lambda_14 True\n",
      "lambda_15 True\n",
      "concatenate_7 True\n",
      "global_average_pooling1d_13 True\n",
      "dense_116 True\n",
      "batch_normalization_18 True\n",
      "dropout_50 True\n",
      "dense_117 True\n",
      "batch_normalization_19 True\n",
      "dropout_51 True\n",
      "dense_118 True\n",
      "dense_119 True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_29\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_29\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_27      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ model_feature       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> │ input_layer_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_116 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_116[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_50          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_117 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_117[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_51          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_118 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ dropout_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_119 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │ dense_118[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_27      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ model_feature       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m58,496\u001b[0m │ input_layer_27[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_14 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ model_feature[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_15 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ model_feature[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_14 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_15 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_116 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m1,536\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ dense_116[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_50          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_117 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_117[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_51          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_118 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m16,448\u001b[0m │ dropout_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_119 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m130\u001b[0m │ dense_118[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">508,226</span> (1.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m508,226\u001b[0m (1.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,978</span> (589.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,978\u001b[0m (589.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">357,248</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m357,248\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define new transformer blocks\n",
    "new_transformer_block = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "new_transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "\n",
    "# Input for Phase 3\n",
    "XDinput_phaz3 = Input(shape=(100, 71))\n",
    "# Pass input through the feature extraction model\n",
    "model_feature_output = model_feature(XDinput_phaz3)  # Output shape: (93, 192)\n",
    "\n",
    "# Transformer blocks processing\n",
    "transformer_output1_phaz3 = new_transformer_block(model_feature_output)  # Output shape: (93, 192)\n",
    "# Compute the norm along the last axis, retaining dimensions\n",
    "transformer_output1_phaz3 = layers.Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1_phaz3)  # Shape: (93, 1)\n",
    "\n",
    "transformer_output2_phaz3 = new_transformer_block2(model_feature_output)  # Output shape: (93, 192)\n",
    "# Compute the norm along the last axis, retaining dimensions\n",
    "transformer_output2_phaz3 = layers.Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2_phaz3)  # Shape: (93, 1)\n",
    "\n",
    "# Combine the outputs of both transformer blocks\n",
    "concatenated_phaz3 = Concatenate(axis=-1)([transformer_output1_phaz3, transformer_output2_phaz3])  # Shape: (93, 2)\n",
    "\n",
    "# Freeze the weights of feature extractor and transformer blocks\n",
    "model_feature.trainable = False\n",
    "new_transformer_block.trainable = False\n",
    "new_transformer_block2.trainable = False\n",
    "\n",
    "# Apply global average pooling to reduce the dimensions to (batch_size, 2)\n",
    "pooled_phaz3 = GlobalAveragePooling1D()(concatenated_phaz3)  # Shape: (None, 2)\n",
    "\n",
    "# Fully connected layers\n",
    "FC1_phaz3 = Dense(512, activation='relu')(pooled_phaz3)\n",
    "FC1_phaz3 = BatchNormalization()(FC1_phaz3)\n",
    "FC2_phaz3 = Dropout(0.1)(FC1_phaz3)  # Increased dropout rate\n",
    "FC3_phaz3 = Dense(256, activation='relu')(FC2_phaz3)\n",
    "FC3_phaz3 = BatchNormalization()(FC3_phaz3)\n",
    "FC4_phaz3 = Dropout(0.1)(FC3_phaz3)  # Increased dropout rate\n",
    "FC5_phaz3 = Dense(64, activation='relu')(FC4_phaz3)\n",
    "\n",
     "# Final prediction layer for regression\n",
     "predictions_phaz3 = Dense(1, activation='linear')(FC5_phaz3)\n",
    "\n",
    "# Define the complete model for Phase 3\n",
    "model_phaz3 = Model(inputs=XDinput_phaz3, outputs=predictions_phaz3)\n",
    "\n",
    "# Print the name and trainable status of each layer\n",
    "for layer in model_phaz3.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "\n",
    "# Model summary\n",
    "model_phaz3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "jF60_7QXQ4lB"
   },
   "outputs": [],
   "source": [
    "# Metrics for regression tasks\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"TensorFlow implementation of the coefficient of determination.\"\"\"\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + tf.keras.backend.epsilon())\n",
    "\n",
    "METRICS_REGRESSION = [\n",
    "    metrics.MeanAbsoluteError(name='mae'),\n",
    "    metrics.MeanSquaredError(name='mse'),\n",
    "    r2_score,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBNS2iz6f9xP"
   },
   "source": [
    "# Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcS3ex1TgCRK",
    "outputId": "7a3df7f6-88ae-459e-8e9d-fde383fd7d9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:23:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:23:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3374\n",
      "Number of test samples: 422\n",
      "Epoch 1/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - loss: 3.4608 - mae: 1.5009 - mse: 3.4608 - r2_score: -705.7711\n",
      "Epoch 2/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 837ms/step - loss: 1.2074 - mae: 0.8382 - mse: 1.2074 - r2_score: -262.6396\n",
      "Epoch 3/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 982ms/step - loss: 1.1323 - mae: 0.8261 - mse: 1.1323 - r2_score: -280.7585\n",
      "Epoch 4/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 973ms/step - loss: 1.0957 - mae: 0.8013 - mse: 1.0957 - r2_score: -271.5926\n",
      "Epoch 5/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 953ms/step - loss: 1.0495 - mae: 0.7880 - mse: 1.0495 - r2_score: -268.1882\n",
      "Epoch 6/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 910ms/step - loss: 1.0710 - mae: 0.7805 - mse: 1.0710 - r2_score: -276.0538\n",
      "Epoch 7/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - loss: 1.0494 - mae: 0.7858 - mse: 1.0494 - r2_score: -274.2872\n",
      "Epoch 8/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 652ms/step - loss: 1.0293 - mae: 0.7926 - mse: 1.0293 - r2_score: -275.0028\n",
      "Epoch 9/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 622ms/step - loss: 1.0557 - mae: 0.7928 - mse: 1.0557 - r2_score: -273.6519\n",
      "Epoch 10/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 629ms/step - loss: 1.0437 - mae: 0.7724 - mse: 1.0437 - r2_score: -276.3913\n",
      "Epoch 11/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 817ms/step - loss: 1.0230 - mae: 0.7693 - mse: 1.0230 - r2_score: -277.3341\n",
      "Epoch 12/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 875ms/step - loss: 0.9898 - mae: 0.7585 - mse: 0.9898 - r2_score: -281.9400\n",
      "Epoch 13/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 672ms/step - loss: 0.9869 - mae: 0.7674 - mse: 0.9869 - r2_score: -278.9803\n",
      "Epoch 14/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 864ms/step - loss: 0.9827 - mae: 0.7634 - mse: 0.9827 - r2_score: -278.7934\n",
      "Epoch 15/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 854ms/step - loss: 0.9853 - mae: 0.7628 - mse: 0.9853 - r2_score: -278.8678\n",
      "Epoch 16/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 883ms/step - loss: 0.9432 - mae: 0.7499 - mse: 0.9432 - r2_score: -283.1440\n",
      "Epoch 17/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 818ms/step - loss: 0.9885 - mae: 0.7561 - mse: 0.9885 - r2_score: -283.8681\n",
      "Epoch 18/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 708ms/step - loss: 0.9693 - mae: 0.7604 - mse: 0.9693 - r2_score: -282.4840\n",
      "Epoch 19/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 882ms/step - loss: 0.9485 - mae: 0.7463 - mse: 0.9485 - r2_score: -285.3940\n",
      "Epoch 20/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 739ms/step - loss: 0.9696 - mae: 0.7636 - mse: 0.9696 - r2_score: -285.4673\n",
      "Epoch 21/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 822ms/step - loss: 1.0258 - mae: 0.7747 - mse: 1.0258 - r2_score: -288.0397\n",
      "Epoch 22/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 782ms/step - loss: 0.9641 - mae: 0.7547 - mse: 0.9641 - r2_score: -289.0865\n",
      "Epoch 23/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 875ms/step - loss: 0.8932 - mae: 0.7306 - mse: 0.8932 - r2_score: -289.0086\n",
      "Epoch 24/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 879ms/step - loss: 0.9205 - mae: 0.7343 - mse: 0.9205 - r2_score: -292.1608\n",
      "Epoch 25/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 756ms/step - loss: 0.8991 - mae: 0.7320 - mse: 0.8991 - r2_score: -291.1677\n",
      "Epoch 26/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 869ms/step - loss: 0.9597 - mae: 0.7560 - mse: 0.9597 - r2_score: -289.6737\n",
      "Epoch 27/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 875ms/step - loss: 0.9298 - mae: 0.7433 - mse: 0.9298 - r2_score: -287.9765\n",
      "Epoch 28/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 898ms/step - loss: 0.9183 - mae: 0.7440 - mse: 0.9183 - r2_score: -296.2313\n",
      "Epoch 29/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 881ms/step - loss: 0.9105 - mae: 0.7325 - mse: 0.9105 - r2_score: -293.6644\n",
      "Epoch 30/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 890ms/step - loss: 0.8976 - mae: 0.7266 - mse: 0.8976 - r2_score: -293.1479\n",
      "Epoch 31/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 874ms/step - loss: 0.9244 - mae: 0.7371 - mse: 0.9244 - r2_score: -293.6056\n",
      "Epoch 32/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 866ms/step - loss: 0.8875 - mae: 0.7262 - mse: 0.8875 - r2_score: -295.6156\n",
      "Epoch 33/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 918ms/step - loss: 0.8917 - mae: 0.7308 - mse: 0.8917 - r2_score: -296.2150\n",
      "Epoch 34/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 847ms/step - loss: 0.9123 - mae: 0.7330 - mse: 0.9123 - r2_score: -298.9361\n",
      "Epoch 35/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 703ms/step - loss: 0.8803 - mae: 0.7237 - mse: 0.8803 - r2_score: -298.7829\n",
      "Epoch 36/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 388ms/step - loss: 0.8586 - mae: 0.7182 - mse: 0.8586 - r2_score: -301.7372\n",
      "Epoch 37/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 703ms/step - loss: 0.8633 - mae: 0.7175 - mse: 0.8633 - r2_score: -301.3952\n",
      "Epoch 38/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 744ms/step - loss: 0.8297 - mae: 0.7015 - mse: 0.8297 - r2_score: -301.7308\n",
      "Epoch 39/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 860ms/step - loss: 0.8984 - mae: 0.7244 - mse: 0.8984 - r2_score: -297.5887\n",
      "Epoch 40/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 810ms/step - loss: 0.8870 - mae: 0.7290 - mse: 0.8870 - r2_score: -298.3194\n",
      "Epoch 41/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 687ms/step - loss: 0.8543 - mae: 0.7131 - mse: 0.8543 - r2_score: -298.4524\n",
      "Epoch 42/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 870ms/step - loss: 0.9196 - mae: 0.7344 - mse: 0.9196 - r2_score: -303.2710\n",
      "Epoch 43/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 864ms/step - loss: 0.8744 - mae: 0.7278 - mse: 0.8744 - r2_score: -300.1776\n",
      "Epoch 44/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 877ms/step - loss: 0.8794 - mae: 0.7147 - mse: 0.8794 - r2_score: -304.0407\n",
      "Epoch 45/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 889ms/step - loss: 0.8473 - mae: 0.7069 - mse: 0.8473 - r2_score: -303.6453\n",
      "Epoch 46/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 894ms/step - loss: 0.8279 - mae: 0.7026 - mse: 0.8279 - r2_score: -309.8816\n",
      "Epoch 47/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 886ms/step - loss: 0.8314 - mae: 0.7088 - mse: 0.8314 - r2_score: -305.6054\n",
      "Epoch 48/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 862ms/step - loss: 0.8092 - mae: 0.6920 - mse: 0.8092 - r2_score: -306.7743\n",
      "Epoch 49/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 857ms/step - loss: 0.8197 - mae: 0.6958 - mse: 0.8197 - r2_score: -305.9088\n",
      "Epoch 50/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 820ms/step - loss: 0.8230 - mae: 0.6994 - mse: 0.8230 - r2_score: -306.3448\n",
      "Epoch 51/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 855ms/step - loss: 0.8051 - mae: 0.6927 - mse: 0.8051 - r2_score: -306.6085\n",
      "Epoch 52/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 863ms/step - loss: 0.8111 - mae: 0.6954 - mse: 0.8111 - r2_score: -310.2076\n",
      "Epoch 53/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 849ms/step - loss: 0.8049 - mae: 0.6953 - mse: 0.8049 - r2_score: -312.2764\n",
      "Epoch 54/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 863ms/step - loss: 0.8376 - mae: 0.7015 - mse: 0.8376 - r2_score: -311.6663\n",
      "Epoch 55/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 634ms/step - loss: 0.7643 - mae: 0.6793 - mse: 0.7643 - r2_score: -312.3587\n",
      "Epoch 56/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 900ms/step - loss: 0.8102 - mae: 0.6987 - mse: 0.8102 - r2_score: -315.4694\n",
      "Epoch 57/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 907ms/step - loss: 0.8090 - mae: 0.6983 - mse: 0.8090 - r2_score: -310.7832\n",
      "Epoch 58/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 875ms/step - loss: 0.8249 - mae: 0.7066 - mse: 0.8249 - r2_score: -320.2496\n",
      "Epoch 59/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 853ms/step - loss: 0.8111 - mae: 0.6950 - mse: 0.8111 - r2_score: -313.2469\n",
      "Epoch 60/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 838ms/step - loss: 0.7968 - mae: 0.6849 - mse: 0.7968 - r2_score: -310.4001\n",
      "Epoch 61/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 827ms/step - loss: 0.7840 - mae: 0.6826 - mse: 0.7840 - r2_score: -319.1846\n",
      "Epoch 62/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 871ms/step - loss: 0.7949 - mae: 0.6884 - mse: 0.7949 - r2_score: -319.3557\n",
      "Epoch 63/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 839ms/step - loss: 0.7815 - mae: 0.6733 - mse: 0.7815 - r2_score: -320.7764\n",
      "Epoch 64/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 763ms/step - loss: 0.7815 - mae: 0.6739 - mse: 0.7815 - r2_score: -316.8315\n",
      "Epoch 65/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 726ms/step - loss: 0.7234 - mae: 0.6584 - mse: 0.7234 - r2_score: -323.0613\n",
      "Epoch 66/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 875ms/step - loss: 0.7793 - mae: 0.6832 - mse: 0.7793 - r2_score: -319.6875\n",
      "Epoch 67/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 654ms/step - loss: 0.7838 - mae: 0.6787 - mse: 0.7838 - r2_score: -324.0176\n",
      "Epoch 68/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 871ms/step - loss: 0.8215 - mae: 0.6973 - mse: 0.8215 - r2_score: -323.1091\n",
      "Epoch 69/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 898ms/step - loss: 0.8004 - mae: 0.6837 - mse: 0.8004 - r2_score: -327.9405\n",
      "Epoch 70/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 901ms/step - loss: 0.7833 - mae: 0.6809 - mse: 0.7833 - r2_score: -320.5465\n",
      "Epoch 71/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 641ms/step - loss: 0.7636 - mae: 0.6652 - mse: 0.7636 - r2_score: -320.9546\n",
      "Epoch 72/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 887ms/step - loss: 0.7373 - mae: 0.6606 - mse: 0.7373 - r2_score: -326.7551\n",
      "Epoch 73/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 900ms/step - loss: 0.7741 - mae: 0.6729 - mse: 0.7741 - r2_score: -331.7924\n",
      "Epoch 74/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 650ms/step - loss: 0.7141 - mae: 0.6469 - mse: 0.7141 - r2_score: -324.7583\n",
      "Epoch 75/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 881ms/step - loss: 0.7518 - mae: 0.6685 - mse: 0.7518 - r2_score: -325.0063\n",
      "Epoch 76/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 840ms/step - loss: 0.7291 - mae: 0.6581 - mse: 0.7291 - r2_score: -325.1157\n",
      "Epoch 77/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 804ms/step - loss: 0.7431 - mae: 0.6671 - mse: 0.7431 - r2_score: -329.0079\n",
      "Epoch 78/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 845ms/step - loss: 0.7374 - mae: 0.6629 - mse: 0.7374 - r2_score: -329.3301\n",
      "Epoch 79/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 889ms/step - loss: 0.7756 - mae: 0.6762 - mse: 0.7756 - r2_score: -330.8187\n",
      "Epoch 80/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 907ms/step - loss: 0.7286 - mae: 0.6570 - mse: 0.7286 - r2_score: -331.3231\n",
      "Epoch 81/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 721ms/step - loss: 0.7461 - mae: 0.6680 - mse: 0.7461 - r2_score: -323.0836\n",
      "Epoch 82/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 873ms/step - loss: 0.7096 - mae: 0.6466 - mse: 0.7096 - r2_score: -335.9869\n",
      "Epoch 83/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 851ms/step - loss: 0.7524 - mae: 0.6706 - mse: 0.7524 - r2_score: -332.4932\n",
      "Epoch 84/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 811ms/step - loss: 0.7120 - mae: 0.6461 - mse: 0.7120 - r2_score: -331.2153\n",
      "Epoch 85/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 841ms/step - loss: 0.7087 - mae: 0.6522 - mse: 0.7087 - r2_score: -336.3782\n",
      "Epoch 86/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 871ms/step - loss: 0.7326 - mae: 0.6529 - mse: 0.7326 - r2_score: -327.2464\n",
      "Epoch 87/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 876ms/step - loss: 0.7505 - mae: 0.6667 - mse: 0.7505 - r2_score: -340.0821\n",
      "Epoch 88/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 867ms/step - loss: 0.7403 - mae: 0.6661 - mse: 0.7403 - r2_score: -332.6702\n",
      "Epoch 89/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 651ms/step - loss: 0.7617 - mae: 0.6678 - mse: 0.7617 - r2_score: -329.3371\n",
      "Epoch 90/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 909ms/step - loss: 0.7571 - mae: 0.6623 - mse: 0.7571 - r2_score: -337.5638\n",
      "Epoch 91/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 614ms/step - loss: 0.7088 - mae: 0.6469 - mse: 0.7088 - r2_score: -336.9849\n",
      "Epoch 92/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 901ms/step - loss: 0.7461 - mae: 0.6681 - mse: 0.7461 - r2_score: -333.2249\n",
      "Epoch 93/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 923ms/step - loss: 0.7200 - mae: 0.6443 - mse: 0.7200 - r2_score: -338.6558\n",
      "Epoch 94/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 701ms/step - loss: 0.7057 - mae: 0.6465 - mse: 0.7057 - r2_score: -335.4736\n",
      "Epoch 95/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 823ms/step - loss: 0.6881 - mae: 0.6361 - mse: 0.6881 - r2_score: -339.4605\n",
      "Epoch 96/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 803ms/step - loss: 0.6888 - mae: 0.6423 - mse: 0.6888 - r2_score: -341.6500\n",
      "Epoch 97/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 591ms/step - loss: 0.6975 - mae: 0.6404 - mse: 0.6975 - r2_score: -333.6667\n",
      "Epoch 98/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 605ms/step - loss: 0.7160 - mae: 0.6436 - mse: 0.7160 - r2_score: -336.0848\n",
      "Epoch 99/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 596ms/step - loss: 0.6779 - mae: 0.6306 - mse: 0.6779 - r2_score: -339.9638\n",
      "Epoch 100/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 605ms/step - loss: 0.6814 - mae: 0.6307 - mse: 0.6814 - r2_score: -344.2087\n",
      "Epoch 101/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 607ms/step - loss: 0.6753 - mae: 0.6400 - mse: 0.6753 - r2_score: -346.7786\n",
      "Epoch 102/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 599ms/step - loss: 0.7029 - mae: 0.6481 - mse: 0.7029 - r2_score: -339.9530\n",
      "Epoch 103/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 608ms/step - loss: 0.7072 - mae: 0.6450 - mse: 0.7072 - r2_score: -340.1428\n",
      "Epoch 104/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 535ms/step - loss: 0.6827 - mae: 0.6375 - mse: 0.6827 - r2_score: -343.2745\n",
      "Epoch 105/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 599ms/step - loss: 0.6600 - mae: 0.6302 - mse: 0.6600 - r2_score: -343.4145\n",
      "Epoch 106/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 599ms/step - loss: 0.6412 - mae: 0.6201 - mse: 0.6412 - r2_score: -352.1045\n",
      "Epoch 107/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 588ms/step - loss: 0.7319 - mae: 0.6555 - mse: 0.7319 - r2_score: -341.3773\n",
      "Epoch 108/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 593ms/step - loss: 0.6531 - mae: 0.6206 - mse: 0.6531 - r2_score: -344.8997\n",
      "Epoch 109/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 609ms/step - loss: 0.6842 - mae: 0.6431 - mse: 0.6842 - r2_score: -356.9337\n",
      "Epoch 110/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 448ms/step - loss: 0.6637 - mae: 0.6355 - mse: 0.6637 - r2_score: -345.7135\n",
      "Epoch 111/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 586ms/step - loss: 0.7037 - mae: 0.6381 - mse: 0.7037 - r2_score: -341.5329\n",
      "Epoch 112/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 583ms/step - loss: 0.6877 - mae: 0.6350 - mse: 0.6877 - r2_score: -342.9548\n",
      "Epoch 113/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.6635 - mae: 0.6278 - mse: 0.6635 - r2_score: -355.3843\n",
      "Epoch 114/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 610ms/step - loss: 0.6386 - mae: 0.6142 - mse: 0.6386 - r2_score: -344.9266\n",
      "Epoch 115/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 598ms/step - loss: 0.6798 - mae: 0.6292 - mse: 0.6798 - r2_score: -344.0282\n",
      "Epoch 116/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 606ms/step - loss: 0.6337 - mae: 0.6165 - mse: 0.6337 - r2_score: -351.5584\n",
      "Epoch 117/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 594ms/step - loss: 0.6440 - mae: 0.6115 - mse: 0.6440 - r2_score: -353.0404\n",
      "Epoch 118/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 608ms/step - loss: 0.6820 - mae: 0.6350 - mse: 0.6820 - r2_score: -349.0828\n",
      "Epoch 119/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.6798 - mae: 0.6389 - mse: 0.6798 - r2_score: -347.9189\n",
      "Epoch 120/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 598ms/step - loss: 0.6686 - mae: 0.6254 - mse: 0.6686 - r2_score: -356.7320\n",
      "Epoch 121/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.6836 - mae: 0.6406 - mse: 0.6836 - r2_score: -355.2161\n",
      "Epoch 122/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 613ms/step - loss: 0.6613 - mae: 0.6178 - mse: 0.6613 - r2_score: -353.4497\n",
      "Epoch 123/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 613ms/step - loss: 0.7015 - mae: 0.6363 - mse: 0.7015 - r2_score: -346.1997\n",
      "Epoch 124/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 594ms/step - loss: 0.6275 - mae: 0.6086 - mse: 0.6275 - r2_score: -344.1858\n",
      "Epoch 125/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 579ms/step - loss: 0.6556 - mae: 0.6187 - mse: 0.6556 - r2_score: -346.5774\n",
      "Epoch 126/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 602ms/step - loss: 0.6332 - mae: 0.6143 - mse: 0.6332 - r2_score: -351.7768\n",
      "Epoch 127/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 600ms/step - loss: 0.6470 - mae: 0.6161 - mse: 0.6470 - r2_score: -351.1270\n",
      "Epoch 128/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 605ms/step - loss: 0.6083 - mae: 0.6032 - mse: 0.6083 - r2_score: -354.6038\n",
      "Epoch 129/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 613ms/step - loss: 0.6639 - mae: 0.6222 - mse: 0.6639 - r2_score: -347.6808\n",
      "Epoch 130/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 591ms/step - loss: 0.6222 - mae: 0.6076 - mse: 0.6222 - r2_score: -365.2309\n",
      "Epoch 131/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 602ms/step - loss: 0.6514 - mae: 0.6193 - mse: 0.6514 - r2_score: -351.4367\n",
      "Epoch 132/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - loss: 0.6636 - mae: 0.6224 - mse: 0.6636 - r2_score: -358.3438\n",
      "Epoch 133/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 602ms/step - loss: 0.6339 - mae: 0.6076 - mse: 0.6339 - r2_score: -360.4915\n",
      "Epoch 134/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.6131 - mae: 0.6069 - mse: 0.6131 - r2_score: -354.4668\n",
      "Epoch 135/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 582ms/step - loss: 0.6153 - mae: 0.6011 - mse: 0.6153 - r2_score: -359.4674\n",
      "Epoch 136/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 587ms/step - loss: 0.6205 - mae: 0.6120 - mse: 0.6205 - r2_score: -354.5870\n",
      "Epoch 137/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 517ms/step - loss: 0.6268 - mae: 0.6048 - mse: 0.6268 - r2_score: -356.1596\n",
      "Epoch 138/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 606ms/step - loss: 0.6470 - mae: 0.6167 - mse: 0.6470 - r2_score: -348.8973\n",
      "Epoch 139/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 604ms/step - loss: 0.6534 - mae: 0.6084 - mse: 0.6534 - r2_score: -358.1210\n",
      "Epoch 140/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 555ms/step - loss: 0.6258 - mae: 0.6091 - mse: 0.6258 - r2_score: -356.7788\n",
      "Epoch 141/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 595ms/step - loss: 0.6418 - mae: 0.6142 - mse: 0.6418 - r2_score: -352.4162\n",
      "Epoch 142/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 599ms/step - loss: 0.6546 - mae: 0.6201 - mse: 0.6546 - r2_score: -363.2264\n",
      "Epoch 143/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 601ms/step - loss: 0.5999 - mae: 0.5935 - mse: 0.5999 - r2_score: -361.3524\n",
      "Epoch 144/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 594ms/step - loss: 0.6024 - mae: 0.5961 - mse: 0.6024 - r2_score: -358.0506\n",
      "Epoch 145/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 594ms/step - loss: 0.6313 - mae: 0.6091 - mse: 0.6313 - r2_score: -359.3916\n",
      "Epoch 146/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 591ms/step - loss: 0.6114 - mae: 0.5971 - mse: 0.6114 - r2_score: -353.6768\n",
      "Epoch 147/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 585ms/step - loss: 0.6337 - mae: 0.5990 - mse: 0.6337 - r2_score: -356.4383\n",
      "Epoch 148/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 598ms/step - loss: 0.5951 - mae: 0.5895 - mse: 0.5951 - r2_score: -361.3705\n",
      "Epoch 149/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 599ms/step - loss: 0.5761 - mae: 0.5819 - mse: 0.5761 - r2_score: -367.0920\n",
      "Epoch 150/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 626ms/step - loss: 0.6078 - mae: 0.5991 - mse: 0.6078 - r2_score: -365.2068\n",
      "Epoch 151/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 599ms/step - loss: 0.5786 - mae: 0.5884 - mse: 0.5786 - r2_score: -369.7052\n",
      "Epoch 152/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - loss: 0.6125 - mae: 0.5969 - mse: 0.6125 - r2_score: -370.2485\n",
      "Epoch 153/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 592ms/step - loss: 0.6274 - mae: 0.6142 - mse: 0.6274 - r2_score: -366.3486\n",
      "Epoch 154/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 624ms/step - loss: 0.5948 - mae: 0.5930 - mse: 0.5948 - r2_score: -363.1350\n",
      "Epoch 155/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 606ms/step - loss: 0.5793 - mae: 0.5810 - mse: 0.5793 - r2_score: -365.6145\n",
      "Epoch 156/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 606ms/step - loss: 0.5815 - mae: 0.5840 - mse: 0.5815 - r2_score: -356.7524\n",
      "Epoch 157/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 635ms/step - loss: 0.5970 - mae: 0.5923 - mse: 0.5970 - r2_score: -356.7002\n",
      "Epoch 158/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 707ms/step - loss: 0.5901 - mae: 0.5866 - mse: 0.5901 - r2_score: -364.8341\n",
      "Epoch 159/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 685ms/step - loss: 0.5764 - mae: 0.5816 - mse: 0.5764 - r2_score: -365.5255\n",
      "Epoch 160/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 642ms/step - loss: 0.6095 - mae: 0.6089 - mse: 0.6095 - r2_score: -374.7166\n",
      "Epoch 161/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 627ms/step - loss: 0.6122 - mae: 0.5989 - mse: 0.6122 - r2_score: -368.7620\n",
      "Epoch 162/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 613ms/step - loss: 0.5710 - mae: 0.5775 - mse: 0.5710 - r2_score: -364.6176\n",
      "Epoch 163/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 617ms/step - loss: 0.5844 - mae: 0.5877 - mse: 0.5844 - r2_score: -362.5828\n",
      "Epoch 164/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 630ms/step - loss: 0.5874 - mae: 0.5876 - mse: 0.5874 - r2_score: -369.0173\n",
      "Epoch 165/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 541ms/step - loss: 0.6042 - mae: 0.5861 - mse: 0.6042 - r2_score: -365.3918\n",
      "Epoch 166/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.5892 - mae: 0.5939 - mse: 0.5892 - r2_score: -359.9664\n",
      "Epoch 167/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 595ms/step - loss: 0.5874 - mae: 0.5884 - mse: 0.5874 - r2_score: -363.2801\n",
      "Epoch 168/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 618ms/step - loss: 0.6115 - mae: 0.6001 - mse: 0.6115 - r2_score: -361.5392\n",
      "Epoch 169/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 603ms/step - loss: 0.5940 - mae: 0.5816 - mse: 0.5940 - r2_score: -364.8152\n",
      "Epoch 170/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 614ms/step - loss: 0.5837 - mae: 0.5868 - mse: 0.5837 - r2_score: -366.6300\n",
      "Epoch 171/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 624ms/step - loss: 0.5802 - mae: 0.5895 - mse: 0.5802 - r2_score: -366.1683\n",
      "Epoch 172/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 598ms/step - loss: 0.5745 - mae: 0.5715 - mse: 0.5745 - r2_score: -371.6134\n",
      "Epoch 173/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 601ms/step - loss: 0.5833 - mae: 0.5852 - mse: 0.5833 - r2_score: -366.6632\n",
      "Epoch 174/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 525ms/step - loss: 0.5674 - mae: 0.5801 - mse: 0.5674 - r2_score: -374.3982\n",
      "Epoch 175/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 709ms/step - loss: 0.5660 - mae: 0.5797 - mse: 0.5660 - r2_score: -373.4626\n",
      "Epoch 176/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.5849 - mae: 0.5833 - mse: 0.5849 - r2_score: -365.6947\n",
      "Epoch 177/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 582ms/step - loss: 0.6048 - mae: 0.5894 - mse: 0.6048 - r2_score: -363.2091\n",
      "Epoch 178/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 594ms/step - loss: 0.5885 - mae: 0.5811 - mse: 0.5885 - r2_score: -368.6967\n",
      "Epoch 179/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 579ms/step - loss: 0.5554 - mae: 0.5665 - mse: 0.5554 - r2_score: -376.3326\n",
      "Epoch 180/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 478ms/step - loss: 0.5949 - mae: 0.5920 - mse: 0.5949 - r2_score: -373.7954\n",
      "Epoch 181/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 561ms/step - loss: 0.5951 - mae: 0.5881 - mse: 0.5951 - r2_score: -367.4408\n",
      "Epoch 182/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 578ms/step - loss: 0.5727 - mae: 0.5780 - mse: 0.5727 - r2_score: -372.0433\n",
      "Epoch 183/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.5903 - mae: 0.5847 - mse: 0.5903 - r2_score: -369.0403\n",
      "Epoch 184/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 592ms/step - loss: 0.5779 - mae: 0.5756 - mse: 0.5779 - r2_score: -369.8189\n",
      "Epoch 185/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 595ms/step - loss: 0.5672 - mae: 0.5731 - mse: 0.5672 - r2_score: -366.5081\n",
      "Epoch 186/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 589ms/step - loss: 0.5489 - mae: 0.5597 - mse: 0.5489 - r2_score: -369.5904\n",
      "Epoch 187/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 589ms/step - loss: 0.5317 - mae: 0.5551 - mse: 0.5317 - r2_score: -372.9421\n",
      "Epoch 188/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 611ms/step - loss: 0.5519 - mae: 0.5694 - mse: 0.5519 - r2_score: -368.6323\n",
      "Epoch 189/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 623ms/step - loss: 0.5470 - mae: 0.5570 - mse: 0.5470 - r2_score: -366.9860\n",
      "Epoch 190/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 592ms/step - loss: 0.5324 - mae: 0.5643 - mse: 0.5324 - r2_score: -374.2079\n",
      "Epoch 191/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 593ms/step - loss: 0.5493 - mae: 0.5711 - mse: 0.5493 - r2_score: -372.5669\n",
      "Epoch 192/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 608ms/step - loss: 0.5155 - mae: 0.5503 - mse: 0.5155 - r2_score: -377.1373\n",
      "Epoch 193/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.5242 - mae: 0.5547 - mse: 0.5242 - r2_score: -373.5263\n",
      "Epoch 194/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.5575 - mae: 0.5678 - mse: 0.5575 - r2_score: -371.3798\n",
      "Epoch 195/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 616ms/step - loss: 0.5588 - mae: 0.5626 - mse: 0.5588 - r2_score: -378.1483\n",
      "Epoch 196/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 611ms/step - loss: 0.5624 - mae: 0.5678 - mse: 0.5624 - r2_score: -375.5080\n",
      "Epoch 197/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 606ms/step - loss: 0.5823 - mae: 0.5775 - mse: 0.5823 - r2_score: -373.0605\n",
      "Epoch 198/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 593ms/step - loss: 0.5445 - mae: 0.5619 - mse: 0.5445 - r2_score: -379.9563\n",
      "Epoch 199/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 596ms/step - loss: 0.5343 - mae: 0.5605 - mse: 0.5343 - r2_score: -379.6948\n",
      "Epoch 200/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 535ms/step - loss: 0.5442 - mae: 0.5586 - mse: 0.5442 - r2_score: -372.5149\n",
      "Epoch 201/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 603ms/step - loss: 0.5208 - mae: 0.5579 - mse: 0.5208 - r2_score: -378.8841\n",
      "Epoch 202/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 599ms/step - loss: 0.5684 - mae: 0.5711 - mse: 0.5684 - r2_score: -372.7918\n",
      "Epoch 203/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 533ms/step - loss: 0.5365 - mae: 0.5581 - mse: 0.5365 - r2_score: -376.7279\n",
      "Epoch 204/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 542ms/step - loss: 0.5311 - mae: 0.5585 - mse: 0.5311 - r2_score: -376.8173\n",
      "Epoch 205/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 606ms/step - loss: 0.5334 - mae: 0.5605 - mse: 0.5334 - r2_score: -376.7699\n",
      "Epoch 206/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 596ms/step - loss: 0.5490 - mae: 0.5718 - mse: 0.5490 - r2_score: -377.0709\n",
      "Epoch 207/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 624ms/step - loss: 0.5729 - mae: 0.5785 - mse: 0.5729 - r2_score: -373.7053\n",
      "Epoch 208/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 600ms/step - loss: 0.5431 - mae: 0.5653 - mse: 0.5431 - r2_score: -385.6982\n",
      "Epoch 209/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 594ms/step - loss: 0.5395 - mae: 0.5623 - mse: 0.5395 - r2_score: -377.1089\n",
      "Epoch 210/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 593ms/step - loss: 0.5144 - mae: 0.5492 - mse: 0.5144 - r2_score: -376.8520\n",
      "Epoch 211/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 604ms/step - loss: 0.5144 - mae: 0.5419 - mse: 0.5144 - r2_score: -372.9848\n",
      "Epoch 212/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 585ms/step - loss: 0.5218 - mae: 0.5548 - mse: 0.5218 - r2_score: -375.7266\n",
      "Epoch 213/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 556ms/step - loss: 0.5367 - mae: 0.5540 - mse: 0.5367 - r2_score: -374.6742\n",
      "Epoch 214/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 633ms/step - loss: 0.5551 - mae: 0.5656 - mse: 0.5551 - r2_score: -371.7932\n",
      "Epoch 215/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.5193 - mae: 0.5495 - mse: 0.5193 - r2_score: -378.7881\n",
      "Epoch 216/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 596ms/step - loss: 0.5142 - mae: 0.5494 - mse: 0.5142 - r2_score: -381.4533\n",
      "Epoch 217/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 617ms/step - loss: 0.5486 - mae: 0.5624 - mse: 0.5486 - r2_score: -373.5016\n",
      "Epoch 218/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 628ms/step - loss: 0.5378 - mae: 0.5574 - mse: 0.5378 - r2_score: -383.8968\n",
      "Epoch 219/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 625ms/step - loss: 0.4981 - mae: 0.5439 - mse: 0.4981 - r2_score: -392.2673\n",
      "Epoch 220/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 608ms/step - loss: 0.5181 - mae: 0.5533 - mse: 0.5181 - r2_score: -377.0851\n",
      "Epoch 221/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 607ms/step - loss: 0.5308 - mae: 0.5539 - mse: 0.5308 - r2_score: -381.6560\n",
      "Epoch 222/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 520ms/step - loss: 0.5344 - mae: 0.5559 - mse: 0.5344 - r2_score: -383.6922\n",
      "Epoch 223/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 589ms/step - loss: 0.5281 - mae: 0.5544 - mse: 0.5281 - r2_score: -372.3758\n",
      "Epoch 224/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 589ms/step - loss: 0.5097 - mae: 0.5412 - mse: 0.5097 - r2_score: -382.0202\n",
      "Epoch 225/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 594ms/step - loss: 0.5263 - mae: 0.5586 - mse: 0.5263 - r2_score: -378.6917\n",
      "Epoch 226/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 584ms/step - loss: 0.5377 - mae: 0.5571 - mse: 0.5377 - r2_score: -381.0073\n",
      "Epoch 227/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 602ms/step - loss: 0.5236 - mae: 0.5518 - mse: 0.5236 - r2_score: -385.2796\n",
      "Epoch 228/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 615ms/step - loss: 0.5506 - mae: 0.5691 - mse: 0.5506 - r2_score: -376.4116\n",
      "Epoch 229/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 584ms/step - loss: 0.5099 - mae: 0.5433 - mse: 0.5099 - r2_score: -380.1694\n",
      "Epoch 230/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 598ms/step - loss: 0.4971 - mae: 0.5381 - mse: 0.4971 - r2_score: -385.0356\n",
      "Epoch 231/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 599ms/step - loss: 0.5287 - mae: 0.5583 - mse: 0.5287 - r2_score: -377.0187\n",
      "Epoch 232/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 609ms/step - loss: 0.5724 - mae: 0.5805 - mse: 0.5724 - r2_score: -400.4136\n",
      "Epoch 233/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 595ms/step - loss: 0.5178 - mae: 0.5550 - mse: 0.5178 - r2_score: -387.4279\n",
      "Epoch 234/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 611ms/step - loss: 0.5087 - mae: 0.5471 - mse: 0.5087 - r2_score: -381.9990\n",
      "Epoch 235/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 627ms/step - loss: 0.5228 - mae: 0.5585 - mse: 0.5228 - r2_score: -385.0061\n",
      "Epoch 236/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 603ms/step - loss: 0.5214 - mae: 0.5481 - mse: 0.5214 - r2_score: -385.3643\n",
      "Epoch 237/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 610ms/step - loss: 0.5059 - mae: 0.5439 - mse: 0.5059 - r2_score: -384.1370\n",
      "Epoch 238/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 440ms/step - loss: 0.5111 - mae: 0.5505 - mse: 0.5111 - r2_score: -379.4232\n",
      "Epoch 239/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 606ms/step - loss: 0.5051 - mae: 0.5431 - mse: 0.5051 - r2_score: -384.2605\n",
      "Epoch 240/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 534ms/step - loss: 0.5083 - mae: 0.5419 - mse: 0.5083 - r2_score: -382.3702\n",
      "Epoch 241/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 597ms/step - loss: 0.4961 - mae: 0.5408 - mse: 0.4961 - r2_score: -382.7923\n",
      "Epoch 242/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 587ms/step - loss: 0.5015 - mae: 0.5342 - mse: 0.5015 - r2_score: -388.6747\n",
      "Epoch 243/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 584ms/step - loss: 0.5030 - mae: 0.5393 - mse: 0.5030 - r2_score: -385.0074\n",
      "Epoch 244/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 520ms/step - loss: 0.5009 - mae: 0.5421 - mse: 0.5009 - r2_score: -384.9617\n",
      "Epoch 245/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - loss: 0.5035 - mae: 0.5412 - mse: 0.5035 - r2_score: -383.2610\n",
      "Epoch 246/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 569ms/step - loss: 0.5182 - mae: 0.5449 - mse: 0.5182 - r2_score: -392.2982\n",
      "Epoch 247/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 606ms/step - loss: 0.5117 - mae: 0.5463 - mse: 0.5117 - r2_score: -390.6914\n",
      "Epoch 248/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 585ms/step - loss: 0.4845 - mae: 0.5390 - mse: 0.4845 - r2_score: -385.3933\n",
      "Epoch 249/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - loss: 0.4886 - mae: 0.5343 - mse: 0.4886 - r2_score: -394.4383\n",
      "Epoch 250/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 595ms/step - loss: 0.4965 - mae: 0.5380 - mse: 0.4965 - r2_score: -383.9412\n",
      "Epoch 251/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 512ms/step - loss: 0.5097 - mae: 0.5475 - mse: 0.5097 - r2_score: -384.4908\n",
      "Epoch 252/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 595ms/step - loss: 0.4700 - mae: 0.5281 - mse: 0.4700 - r2_score: -396.4051\n",
      "Epoch 253/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - loss: 0.4918 - mae: 0.5364 - mse: 0.4918 - r2_score: -389.4517\n",
      "Epoch 254/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 591ms/step - loss: 0.4813 - mae: 0.5298 - mse: 0.4813 - r2_score: -389.4079\n",
      "Epoch 255/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 598ms/step - loss: 0.4823 - mae: 0.5347 - mse: 0.4823 - r2_score: -392.7702\n",
      "Epoch 256/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 561ms/step - loss: 0.4803 - mae: 0.5301 - mse: 0.4803 - r2_score: -386.4258\n",
      "Epoch 257/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 561ms/step - loss: 0.4782 - mae: 0.5257 - mse: 0.4782 - r2_score: -389.8225\n",
      "Epoch 258/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 612ms/step - loss: 0.4888 - mae: 0.5327 - mse: 0.4888 - r2_score: -391.8776\n",
      "Epoch 259/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 611ms/step - loss: 0.4707 - mae: 0.5209 - mse: 0.4707 - r2_score: -385.3654\n",
      "Epoch 260/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 593ms/step - loss: 0.4813 - mae: 0.5288 - mse: 0.4813 - r2_score: -390.2432\n",
      "Epoch 261/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 539ms/step - loss: 0.4864 - mae: 0.5338 - mse: 0.4864 - r2_score: -387.5479\n",
      "Epoch 262/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 421ms/step - loss: 0.4846 - mae: 0.5279 - mse: 0.4846 - r2_score: -389.2891\n",
      "Epoch 263/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 368ms/step - loss: 0.5069 - mae: 0.5423 - mse: 0.5069 - r2_score: -394.8011\n",
      "Epoch 264/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 370ms/step - loss: 0.4829 - mae: 0.5329 - mse: 0.4829 - r2_score: -388.2888\n",
      "Epoch 265/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 358ms/step - loss: 0.4786 - mae: 0.5269 - mse: 0.4786 - r2_score: -393.5778\n",
      "Epoch 266/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 460ms/step - loss: 0.5045 - mae: 0.5349 - mse: 0.5045 - r2_score: -390.5379\n",
      "Epoch 267/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 685ms/step - loss: 0.4672 - mae: 0.5160 - mse: 0.4672 - r2_score: -397.0997\n",
      "Epoch 268/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 703ms/step - loss: 0.4997 - mae: 0.5372 - mse: 0.4997 - r2_score: -390.5915\n",
      "Epoch 269/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 703ms/step - loss: 0.4794 - mae: 0.5199 - mse: 0.4794 - r2_score: -389.9286\n",
      "Epoch 270/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 724ms/step - loss: 0.4656 - mae: 0.5186 - mse: 0.4656 - r2_score: -396.6378\n",
      "Epoch 271/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 732ms/step - loss: 0.4511 - mae: 0.5133 - mse: 0.4511 - r2_score: -388.1871\n",
      "Epoch 272/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 355ms/step - loss: 0.4629 - mae: 0.5254 - mse: 0.4629 - r2_score: -400.9289\n",
      "Epoch 273/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 541ms/step - loss: 0.4454 - mae: 0.5093 - mse: 0.4454 - r2_score: -396.9364\n",
      "Epoch 274/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 739ms/step - loss: 0.4636 - mae: 0.5183 - mse: 0.4636 - r2_score: -401.4189\n",
      "Epoch 275/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 742ms/step - loss: 0.4716 - mae: 0.5251 - mse: 0.4716 - r2_score: -391.0620\n",
      "Epoch 276/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 703ms/step - loss: 0.4827 - mae: 0.5297 - mse: 0.4827 - r2_score: -392.1029\n",
      "Epoch 277/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 708ms/step - loss: 0.4693 - mae: 0.5189 - mse: 0.4693 - r2_score: -391.3810\n",
      "Epoch 278/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 711ms/step - loss: 0.4731 - mae: 0.5173 - mse: 0.4731 - r2_score: -387.5643\n",
      "Epoch 279/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 588ms/step - loss: 0.4767 - mae: 0.5223 - mse: 0.4767 - r2_score: -394.9459\n",
      "Epoch 280/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 328ms/step - loss: 0.4760 - mae: 0.5236 - mse: 0.4760 - r2_score: -401.8829\n",
      "Epoch 281/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 698ms/step - loss: 0.4992 - mae: 0.5411 - mse: 0.4992 - r2_score: -391.7617\n",
      "Epoch 282/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 679ms/step - loss: 0.4648 - mae: 0.5158 - mse: 0.4648 - r2_score: -392.6364\n",
      "Epoch 283/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 706ms/step - loss: 0.4780 - mae: 0.5242 - mse: 0.4780 - r2_score: -392.7318\n",
      "Epoch 284/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 713ms/step - loss: 0.4804 - mae: 0.5206 - mse: 0.4804 - r2_score: -395.5237\n",
      "Epoch 285/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 707ms/step - loss: 0.4508 - mae: 0.5124 - mse: 0.4508 - r2_score: -398.1792\n",
      "Epoch 286/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 721ms/step - loss: 0.4825 - mae: 0.5304 - mse: 0.4825 - r2_score: -399.4842\n",
      "Epoch 287/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 717ms/step - loss: 0.4642 - mae: 0.5147 - mse: 0.4642 - r2_score: -397.5967\n",
      "Epoch 288/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 744ms/step - loss: 0.4908 - mae: 0.5257 - mse: 0.4908 - r2_score: -394.9835\n",
      "Epoch 289/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 702ms/step - loss: 0.4744 - mae: 0.5295 - mse: 0.4744 - r2_score: -391.8444\n",
      "Epoch 290/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 698ms/step - loss: 0.4515 - mae: 0.5115 - mse: 0.4515 - r2_score: -395.7397\n",
      "Epoch 291/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 725ms/step - loss: 0.4472 - mae: 0.5093 - mse: 0.4472 - r2_score: -398.1986\n",
      "Epoch 292/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 698ms/step - loss: 0.4499 - mae: 0.5051 - mse: 0.4499 - r2_score: -393.2230\n",
      "Epoch 293/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 723ms/step - loss: 0.4494 - mae: 0.5179 - mse: 0.4494 - r2_score: -401.4303\n",
      "Epoch 294/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 728ms/step - loss: 0.4663 - mae: 0.5193 - mse: 0.4663 - r2_score: -399.4861\n",
      "Epoch 295/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 718ms/step - loss: 0.4524 - mae: 0.5081 - mse: 0.4524 - r2_score: -399.1314\n",
      "Epoch 296/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 704ms/step - loss: 0.4674 - mae: 0.5081 - mse: 0.4674 - r2_score: -397.1021\n",
      "Epoch 297/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 704ms/step - loss: 0.4743 - mae: 0.5210 - mse: 0.4743 - r2_score: -399.4056\n",
      "Epoch 298/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 708ms/step - loss: 0.4506 - mae: 0.5071 - mse: 0.4506 - r2_score: -397.5447\n",
      "Epoch 299/1000\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 684ms/step - loss: 0.4901 - mae: 0.5338 - mse: 0.4901 - r2_score: -403.2048\n",
      "Epoch 300/1000\n",
      "\u001b[1m 3/14\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 833ms/step - loss: 0.4830 - mae: 0.5297 - mse: 0.4830 - r2_score: -404.2089"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "XD_np = np.array(XD)\n",
    "labels_np = np.array(labels)\n",
    "\n",
    "# Remove samples that don't have labels\n",
    "index = ~np.isnan(labels_np)\n",
    "labels_np = labels_np[index]\n",
    "XD_np = XD_np[index]\n",
    "\n",
    "# Filter the corresponding SMILES\n",
    "filtered_smiles = smiles.iloc[index].values\n",
    "\n",
    "# Create a Dataset using SMILES as identifiers\n",
    "dataset = NumpyDataset(X=XD_np, y=labels_np, ids=filtered_smiles)\n",
    "\n",
    "# Create a ScaffoldSplitter\n",
    "splitter = ScaffoldSplitter()\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(\n",
    "    dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1\n",
    ")\n",
    "\n",
    "# Extract training and test indices\n",
    "train_indices = train_dataset.ids\n",
    "test_indices = test_dataset.ids\n",
    "smiles_to_index = {smile: idx for idx, smile in enumerate(filtered_smiles)}\n",
    "\n",
    "# Convert SMILES to indices\n",
    "train_indices = np.array([smiles_to_index[smile] for smile in train_indices])\n",
    "test_indices = np.array([smiles_to_index[smile] for smile in test_indices])\n",
    "\n",
    "# Split the data based on the indices\n",
    "X_train, X_test = XD_np[train_indices], XD_np[test_indices]\n",
    "y_train, y_test = labels_np[train_indices], labels_np[test_indices]\n",
    "\n",
    "print(f\"Number of training samples: {X_train.shape[0]}\")\n",
    "print(f\"Number of test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# === Define Optimizer for Interaction Model ===\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "interactionModel.compile(optimizer=opt, loss='mse', metrics=METRICS_REGRESSION)\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=30, verbose=1, mode='min', restore_best_weights=True)\n",
    "reduce_lr = LearningRateScheduler(lambda epoch: 1e-3 * 0.9 ** epoch)\n",
    "\n",
    "# === Phase 1: Train Interaction Model ===\n",
    "interactionModel.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=256, epochs=1000,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Phase 1 on test data\n",
    "test_eval_phase1 = interactionModel.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# === Feature Extraction ===\n",
    "feature_train = model_feature.predict(X_train)\n",
    "feature_test = model_feature.predict(X_test)\n",
    "\n",
    "# === Phase 2: Train Model Phase 2 ===\n",
    "opt_phaz2 = optimizers.Adam(learning_rate=0.001)\n",
    "model_phaz2.compile(optimizer=opt_phaz2, loss='mse', metrics=METRICS_REGRESSION)\n",
    "model_phaz2.fit(\n",
    "    feature_train, y_train,\n",
    "    batch_size=256, epochs=1000,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    ")\n",
    "\n",
    "# === Freeze Transformer Blocks ===\n",
    "new_transformer_block.set_weights(transformer_block1.get_weights())\n",
    "new_transformer_block2.set_weights(transformer_block2.get_weights())\n",
    "\n",
    "# === Phase 3: Train Model Phase 3 ===\n",
    "opt_phaz3 = optimizers.Adam(learning_rate=0.0001)\n",
    "model_phaz3.compile(optimizer=opt_phaz3, loss='mse', metrics=METRICS_REGRESSION)\n",
    "\n",
    "model_phaz3.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=256, epochs=1000,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Phase 3 on both training and testing data\n",
    "train_eval_phase3 = model_phaz3.evaluate(X_train, y_train, verbose=1)\n",
    "test_eval_phase3 = model_phaz3.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# === Function to Format Results ===\n",
    "def format_results(phase1_result, phase3_train_result, phase3_test_result):\n",
    "    # Define Metric Names\n",
    "    phase1_metrics = ['loss', 'mae', 'mse', 'r2_score']\n",
    "    phase3_metrics = ['loss', 'mae', 'mse', 'r2_score']\n",
    "\n",
    "    # Create DataFrames\n",
    "    phase1_test_df = pd.DataFrame([phase1_result], columns=phase1_metrics)\n",
    "    phase3_train_df = pd.DataFrame([phase3_train_result], columns=phase3_metrics)\n",
    "    phase3_test_df = pd.DataFrame([test_eval_phase3], columns=phase3_metrics)\n",
    "\n",
    "    # Display the Results\n",
    "    print('\\n=== Phase 1: Test Evaluation Results ===')\n",
    "    print(phase1_test_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "\n",
    "    print('\\n=== Phase 3: Train Evaluation Results ===')\n",
    "    print(phase3_train_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "\n",
    "    print('\\n=== Phase 3: Test Evaluation Results ===')\n",
    "    print(phase3_test_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "\n",
    "# === Display Results ===\n",
    "format_results(test_eval_phase1, train_eval_phase3, test_eval_phase3)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
