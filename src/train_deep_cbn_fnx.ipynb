{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0c992de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for converting SMILES characters to numbers\n",
    "smiles_dict = {\n",
    "    \"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n",
    "    \"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n",
    "    \"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n",
    "    \"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n",
    "    \"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50, \"T\": 17, \"W\": 51,\n",
    "    \"V\": 18, \"Y\": 52, \"[\": 53, \"Z\": 19, \"]\": 54, \"\\\\\": 20, \"a\": 55, \"c\": 56,\n",
    "    \"b\": 21, \"e\": 57, \"d\": 22, \"g\": 58, \"f\": 23, \"i\": 59, \"h\": 24, \"m\": 60,\n",
    "    \"l\": 25, \"o\": 61, \"n\": 26, \"s\": 62, \"r\": 27, \"u\": 63, \"t\": 28, \"y\": 64,\n",
    "    \" \": 65, \":\": 66, \",\": 67, \"p\": 68, \"j\": 69, \"*\": 70\n",
    "    }\n",
    "\n",
    "def label_smiles(line, MAX_SMI_LEN, smi_ch_ind):\n",
    "    X = np.zeros(MAX_SMI_LEN, dtype=int)\n",
    "    for i, ch in enumerate(line[:MAX_SMI_LEN]):\n",
    "        if ch in smi_ch_ind:\n",
    "            X[i] = smi_ch_ind[ch]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70906076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_cbn(dataset_path, target_col, smiles_col, n_epochs = 100):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    from tensorflow.keras import optimizers, layers, metrics\n",
    "    from tensorflow.keras.layers import (\n",
    "        Input, Dense, Dropout, BatchNormalization, Conv1D,\n",
    "        GlobalAveragePooling1D, Lambda, Activation, Concatenate\n",
    "    )\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "    # === Load Dataset ===\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    smiles = data[smiles_col]\n",
    "    labels = data[target_col]\n",
    "    \n",
    "    # Dictionary for converting SMILES characters to numbers\n",
    "    smiles_dict = {\n",
    "    \"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n",
    "    \"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n",
    "    \"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n",
    "    \"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n",
    "    \"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50, \"T\": 17, \"W\": 51,\n",
    "    \"V\": 18, \"Y\": 52, \"[\": 53, \"Z\": 19, \"]\": 54, \"\\\\\": 20, \"a\": 55, \"c\": 56,\n",
    "    \"b\": 21, \"e\": 57, \"d\": 22, \"g\": 58, \"f\": 23, \"i\": 59, \"h\": 24, \"m\": 60,\n",
    "    \"l\": 25, \"o\": 61, \"n\": 26, \"s\": 62, \"r\": 27, \"u\": 63, \"t\": 28, \"y\": 64,\n",
    "    \" \": 65, \":\": 66, \",\": 67, \"p\": 68, \"j\": 69, \"*\": 70\n",
    "    }\n",
    "\n",
    "    def label_smiles(line, MAX_SMI_LEN, smi_ch_ind):\n",
    "        X = np.zeros(MAX_SMI_LEN, dtype=int)\n",
    "        for i, ch in enumerate(line[:MAX_SMI_LEN]):\n",
    "            if ch != '\\n' and ch in smi_ch_ind:\n",
    "                X[i] = smi_ch_ind[ch]\n",
    "        return X\n",
    "\n",
    "    XD = np.array([label_smiles(str(smi), 100, smiles_dict) for smi in smiles])\n",
    "    labels = labels.values\n",
    "\n",
    "    # Convert to categorical\n",
    "    XD = to_categorical(XD, num_classes=71)\n",
    "\n",
    "    #### Phase 1 ####\n",
    "\n",
    "    # Feature model definition (unchanged)\n",
    "    XDinput = Input(shape=(100, 71), name='XDinput')\n",
    "    encode_smiles = Conv1D(filters=64, kernel_size=2, activation='relu', padding='valid', strides=1)(XDinput)  # (99,64)\n",
    "    encode_smiles = Conv1D(filters=64, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)  # (96,64)\n",
    "    encode_smiles = Conv1D(filters=128, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)  # (93,128)\n",
    "    model_feature = Model(inputs=XDinput, outputs=encode_smiles, name='model_feature')\n",
    "    model_feature.summary()\n",
    "\n",
    "\n",
    "    # Prediction model definition\n",
    "    input_extracted_feature = Input(shape=(93, 128))\n",
    "    FC1 = Dense(512, activation='relu')(input_extracted_feature)\n",
    "    FC1 = BatchNormalization()(FC1)\n",
    "    FC2 = Dropout(0.1)(FC1)\n",
    "    FC3 = Dense(256, activation='relu')(FC2)\n",
    "    FC3 = BatchNormalization()(FC3)\n",
    "    FC4 = Dropout(0.1)(FC3)\n",
    "    FC5 = Dense(64, activation='relu')(FC4)\n",
    "    predictions = Dense(2, activation='softmax')(FC5)\n",
    "    model_pred = Model(inputs=input_extracted_feature, outputs=predictions)\n",
    "    model_pred.summary()\n",
    "\n",
    "    # Full model definition with added Pooling layer\n",
    "    interaction_input = XDinput\n",
    "    encoded_features = model_feature(interaction_input) # Output: (None, 93, 128)\n",
    "    predicted_output = model_pred(encoded_features)  # Output: (None, 93, 2)\n",
    "\n",
    "    # Adding Pooling layer to reduce dimensionality\n",
    "    pooled_output = GlobalAveragePooling1D()(predicted_output) # Output: (None, 2)\n",
    "    interactionModel = Model(inputs=interaction_input, outputs=pooled_output, name='interactionModel')\n",
    "    interactionModel.summary()\n",
    "\n",
    "    ### BiFormer Block ###\n",
    "    class DropPath(layers.Layer):\n",
    "         # Placeholder for DropPath, currently no-op\n",
    "        def __init__(self, drop_prob=0.):\n",
    "            super().__init__()\n",
    "            self.drop_prob = drop_prob\n",
    "        def call(self, x, training=None):\n",
    "            if (not training) or self.drop_prob == 0.:\n",
    "                return x\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            random_tensor = keep_prob\n",
    "            random_tensor += tf.random.uniform(tf.shape(x), dtype=x.dtype)\n",
    "            binary_tensor = tf.floor(random_tensor)\n",
    "            return tf.divide(x, keep_prob) * binary_tensor\n",
    "        \n",
    "    # Define custom layers for 1D processing\n",
    "    class TopkRouting(layers.Layer):\n",
    "        def __init__(self, qk_dim, topk=16, qk_scale=None, param_routing=False, diff_routing=False):\n",
    "            super().__init__()\n",
    "            self.topk = topk\n",
    "            self.qk_dim = qk_dim\n",
    "            self.scale = qk_scale if qk_scale is not None else qk_dim**-0.5\n",
    "            self.diff_routing = diff_routing\n",
    "            if param_routing:\n",
    "                self.emb = layers.Dense(qk_dim)\n",
    "            else:\n",
    "                self.emb = lambda x: x\n",
    "            self.routing_act = lambda x, axis: tf.nn.softmax(x, axis=axis)\n",
    "        def call(self, query, key, training=None):\n",
    "            if not self.diff_routing:\n",
    "                query = tf.stop_gradient(query)\n",
    "                key = tf.stop_gradient(key)\n",
    "            query_hat = self.emb(query)\n",
    "            key_hat = self.emb(key)\n",
    "            attn_logit = tf.einsum('bnc,bqc->bnq', query_hat*self.scale, key_hat)\n",
    "            topk_attn_logit, topk_index = tf.math.top_k(attn_logit, k=self.topk, sorted=True)\n",
    "            r_weight = self.routing_act(topk_attn_logit, axis=-1)\n",
    "            return r_weight, topk_index\n",
    "\n",
    "    def tf_gather_kv(kv, r_idx):\n",
    "        n = tf.shape(kv)[0]\n",
    "        p2 = tf.shape(kv)[1]\n",
    "        c_kv = tf.shape(kv)[2]\n",
    "        topk = tf.shape(r_idx)[2]\n",
    "        batch_idx = tf.reshape(tf.range(n), [n, 1, 1])\n",
    "        batch_idx = tf.tile(batch_idx, [1, p2, topk])\n",
    "        p2_idx = tf.reshape(tf.range(p2), [1, p2, 1])\n",
    "        p2_idx = tf.tile(p2_idx, [n, 1, topk])\n",
    "        gather_indices = tf.stack([batch_idx, p2_idx, r_idx], axis=-1)\n",
    "        gathered = tf.gather_nd(kv, gather_indices)\n",
    "        return gathered\n",
    "\n",
    "    class KVGather(layers.Layer):\n",
    "        def __init__(self, mul_weight='none'):\n",
    "            super().__init__()\n",
    "            assert mul_weight in ['none', 'soft', 'hard']\n",
    "            self.mul_weight = mul_weight\n",
    "        def call(self, r_idx, r_weight, kv, training=None):\n",
    "            topk_kv = tf_gather_kv(kv, r_idx)\n",
    "            if self.mul_weight == 'soft':\n",
    "                r_weight_exp = tf.expand_dims(tf.expand_dims(r_weight, -1), -1)\n",
    "                topk_kv = topk_kv * r_weight_exp\n",
    "            return topk_kv\n",
    "\n",
    "    class QKVLinear(layers.Layer):\n",
    "        def __init__(self, dim, qk_dim, bias=True):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.qk_dim = qk_dim\n",
    "            self.qkv = layers.Dense(qk_dim + qk_dim + dim, use_bias=bias)\n",
    "        def call(self, x, training=None):\n",
    "            qkv = self.qkv(x)\n",
    "            q, kv = tf.split(qkv, [self.qk_dim, self.qk_dim + self.dim], axis=-1)\n",
    "            return q, kv\n",
    "\n",
    "    class BiLevelRoutingAttention(layers.Layer):\n",
    "        def __init__(self, dim, n_win=7, num_heads=8, qk_dim=None, qk_scale=None,\n",
    "                     kv_per_win=4, kv_downsample_ratio=4, kv_downsample_mode='identity',\n",
    "                     topk=4, param_attention=\"qkvo\", param_routing=False, diff_routing=False, soft_routing=False,\n",
    "                     side_dwconv=3, auto_pad=True):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.n_win = n_win\n",
    "            self.num_heads = num_heads\n",
    "            self.qk_dim = qk_dim if qk_dim is not None else dim\n",
    "            self.scale = qk_scale if qk_scale is not None else self.qk_dim**-0.5\n",
    "            self.topk = topk\n",
    "            self.param_routing = param_routing\n",
    "            self.diff_routing = diff_routing\n",
    "            self.soft_routing = soft_routing\n",
    "            self.auto_pad = auto_pad\n",
    "\n",
    "             # For 1D, we use Conv1D\n",
    "            if side_dwconv > 0:\n",
    "                self.lepe = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n",
    "            else:\n",
    "                self.lepe = lambda x: tf.zeros_like(x)\n",
    "            self.router = TopkRouting(qk_dim=self.qk_dim, topk=self.topk, qk_scale=self.scale,\n",
    "                                      param_routing=self.param_routing, diff_routing=self.diff_routing)\n",
    "            mul_weight = 'none'\n",
    "            if self.soft_routing:\n",
    "                mul_weight = 'soft'\n",
    "            self.kv_gather = KVGather(mul_weight=mul_weight)\n",
    "            if param_attention in ['qkvo', 'qkv']:\n",
    "                self.qkv = QKVLinear(self.dim, self.qk_dim)\n",
    "                if param_attention == 'qkvo':\n",
    "                    self.wo = layers.Dense(self.dim)\n",
    "                else:\n",
    "                    self.wo = lambda x: x\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported param_attention mode\")\n",
    "            self.attn_act = lambda x: tf.nn.softmax(x, axis=-1)\n",
    "            self.kv_down = lambda x: x # identity for simplicity\n",
    "\n",
    "        def call(self, x, training=None, ret_attn_mask=False):\n",
    "            # Implementing full attention can be complex, so here we only keep the general structure\n",
    "            if ret_attn_mask:\n",
    "                return x, None, None, None\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "    class Attention(layers.Layer):\n",
    "        def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "            super().__init__()\n",
    "            self.num_heads = num_heads\n",
    "            head_dim = dim // num_heads\n",
    "            self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n",
    "        def call(self, x, training=None):\n",
    "            return x\n",
    "\n",
    "    class AttentionLePE(layers.Layer):\n",
    "        def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., side_dwconv=3):\n",
    "            super().__init__()\n",
    "            self.num_heads = num_heads\n",
    "            head_dim = dim // num_heads\n",
    "            self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n",
    "            self.side_dwconv = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n",
    "        def call(self, x, training=None):\n",
    "            return x\n",
    "\n",
    "    class Mlp(layers.Layer):\n",
    "        def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
    "            super().__init__()\n",
    "            out_features = out_features or in_features\n",
    "            hidden_features = hidden_features or in_features\n",
    "            self.net = keras.Sequential([\n",
    "                layers.Dense(hidden_features),\n",
    "                layers.Activation('gelu'),\n",
    "                layers.Dropout(rate=drop),\n",
    "                layers.Dense(out_features),\n",
    "                layers.Dropout(rate=drop)\n",
    "            ])\n",
    "        def call(self, x, training=True):\n",
    "            return self.net(x, training=training)\n",
    "\n",
    "    class Block(layers.Layer):\n",
    "        def __init__(self, dim, drop_path=0.1, layer_scale_init_value=-1,\n",
    "                     num_heads=8, n_win=7, qk_dim=128, qk_scale=None,\n",
    "                     kv_per_win=8, kv_downsample_ratio=1, kv_downsample_mode='identity',\n",
    "                     topk=8, param_attention=\"qkvo\", param_routing=True, diff_routing=False, soft_routing=True,\n",
    "                     mlp_ratio=4, mlp_dwconv=False, side_dwconv=5, before_attn_dwconv=3, pre_norm=True, auto_pad=True):\n",
    "            super().__init__()\n",
    "            qk_dim = qk_dim or dim\n",
    "\n",
    "            # For 1D, we use Conv1D\n",
    "            if before_attn_dwconv > 0:\n",
    "                self.pos_embed = layers.Conv1D(dim, kernel_size=before_attn_dwconv, padding='same', activation='relu')\n",
    "            else:\n",
    "                self.pos_embed = lambda x: tf.zeros_like(x)\n",
    "            self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            if topk > 0:\n",
    "                self.attn = BiLevelRoutingAttention(dim=dim, num_heads=num_heads, n_win=n_win, qk_dim=qk_dim,\n",
    "                                                   qk_scale=qk_scale, kv_per_win=kv_per_win, kv_downsample_ratio=kv_downsample_ratio,\n",
    "                                                   kv_downsample_mode=kv_downsample_mode, topk=topk, param_attention=param_attention,\n",
    "                                                   param_routing=param_routing, diff_routing=diff_routing, soft_routing=soft_routing,\n",
    "                                                   side_dwconv=side_dwconv, auto_pad=auto_pad)\n",
    "            elif topk == -1:\n",
    "                self.attn = Attention(dim=dim, num_heads=num_heads, qk_scale=qk_scale)\n",
    "            elif topk == -2:\n",
    "                self.attn = AttentionLePE(dim=dim, num_heads=num_heads, qk_scale=qk_scale, side_dwconv=side_dwconv)\n",
    "            else:\n",
    "                 # Pseudo attention\n",
    "                self.attn = keras.Sequential([\n",
    "                    layers.Dense(dim),\n",
    "                    layers.Conv1D(dim, kernel_size=5, padding='same', activation='relu'),\n",
    "                    layers.Dense(dim)\n",
    "                ])\n",
    "            self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            mlp_hidden_dim = int(mlp_ratio * dim)\n",
    "            mlp_layers = [layers.Dense(mlp_hidden_dim)]\n",
    "            if mlp_dwconv:\n",
    "                mlp_layers.append(layers.Conv1D(mlp_hidden_dim, kernel_size=3, padding='same', activation='relu'))\n",
    "            mlp_layers.append(layers.Activation('gelu'))\n",
    "            mlp_layers.append(layers.Dense(dim))\n",
    "            mlp_layers.insert(1, layers.Dropout(0.2))\n",
    "            mlp_layers.append(layers.Dropout(0.2))\n",
    "            self.mlp = keras.Sequential(mlp_layers)\n",
    "            self.drop_path = DropPath(drop_path) if drop_path > 0. else layers.Lambda(lambda x: x)\n",
    "            if layer_scale_init_value > 0:\n",
    "                self.use_layer_scale = True\n",
    "                self.gamma1 = self.add_weight(shape=(dim,), initializer=tf.keras.initializers.Constant(layer_scale_init_value), trainable=True)\n",
    "                self.gamma2 = self.add_weight(shape=(dim,), initializer=tf.keras.initializers.Constant(layer_scale_init_value), trainable=True)\n",
    "            else:\n",
    "                self.use_layer_scale = False\n",
    "\n",
    "        def call(self, x, training=None):\n",
    "            input_x = x\n",
    "            x = self.pos_embed(x) # Add positional embedding\n",
    "            x = self.norm1(x)\n",
    "            attn = self.attn(x, training=training)\n",
    "            if self.use_layer_scale:\n",
    "                attn = self.gamma1 * attn\n",
    "            x = input_x + self.drop_path(attn, training=training)\n",
    "            input_x = x\n",
    "            x = self.norm2(x)\n",
    "            mlp_output = self.mlp(x, training=training)\n",
    "            if self.use_layer_scale:\n",
    "                mlp_output = self.gamma2 * mlp_output\n",
    "            x = input_x + self.drop_path(mlp_output, training=training)\n",
    "            return x\n",
    "\n",
    "    ### Phase 2 ###\n",
    "    # Parameter settings for the block\n",
    "    dim = 128 # Adjusted from 192 to 128\n",
    "    num_heads = 8\n",
    "\n",
    "    # Create Phase 2 model\n",
    "    input_phaz2 = Input(shape=(93, 128)) # Input shape is (batch_size, sequence_length, embedding_dim)\n",
    "    \n",
    "    # processed_input = model_feature(input_phaz2)\n",
    "    processed_input = input_phaz2\n",
    "\n",
    "    # Define transformer blocks\n",
    "    transformer_block1 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "    transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "\n",
    "    # Apply the first transformer block\n",
    "    transformer_output1 = transformer_block1(processed_input)\n",
    "\n",
    "    # Compute the norm of the output along the last axis (preserving dimensions)\n",
    "    transformer_output1 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1)\n",
    "\n",
    "    # Apply the second transformer block\n",
    "    transformer_output2 = transformer_block2(processed_input)\n",
    "\n",
    "    # Compute the norm of the output along the last axis (preserving dimensions)\n",
    "    transformer_output2 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2)\n",
    "\n",
    "    # Combine the outputs of both transformer blocks along the last axis\n",
    "    combined_outputs = Concatenate(axis=-1)([transformer_output1, transformer_output2])\n",
    "\n",
    "    # Apply global average pooling to reduce the output to (batch_size, 2)\n",
    "    pooled_outputs = GlobalAveragePooling1D()(combined_outputs)\n",
    "\n",
    "    # Compute the difference between the two pooled outputs and reshape to (batch_size, 1)\n",
    "    difference = Lambda(lambda x: tf.expand_dims(x[:, 0] - x[:, 1], axis=-1), name='difference')(pooled_outputs)\n",
    "\n",
    "    # Apply sigmoid activation to normalize the output to the range [0, 1]\n",
    "    condition_2 = Activation('sigmoid', name='activation_11')(difference)\n",
    "\n",
    "    # Freeze the `model_feature` layers to prevent them from being trained\n",
    "    model_feature.trainable = False\n",
    "\n",
    "    # Define the complete model\n",
    "    model_phaz2 = Model(inputs=input_phaz2, outputs=condition_2)\n",
    "    model_phaz2.summary()\n",
    "\n",
    "    ### Phase 3 ####\n",
    "    # Define new transformer blocks\n",
    "    new_transformer_block = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "    new_transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "\n",
    "    # Input for Phase 3\n",
    "    XDinput_phaz3 = Input(shape=(100, 71))\n",
    "\n",
    "    # Pass input through the feature extraction model\n",
    "    model_feature_output = model_feature(XDinput_phaz3)\n",
    "\n",
    "    # Transformer blocks processing\n",
    "    transformer_output1_phaz3 = new_transformer_block(model_feature_output)\n",
    "    # Compute the norm along the last axis, retaining dimensions\n",
    "    transformer_output1_phaz3 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1_phaz3)\n",
    "\n",
    "    # Compute the norm along the last axis, retaining dimensions\n",
    "    transformer_output2_phaz3 = new_transformer_block2(model_feature_output)\n",
    "\n",
    "    # Compute the norm along the last axis, retaining dimensions\n",
    "    transformer_output2_phaz3 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2_phaz3)\n",
    "\n",
    "    # Combine the outputs of both transformer blocks\n",
    "    concatenated_phaz3 = Concatenate(axis=-1)([transformer_output1_phaz3, transformer_output2_phaz3])\n",
    "\n",
    "    # Freeze the weights of feature extractor and transformer blocks\n",
    "    model_feature.trainable = False\n",
    "    new_transformer_block.trainable = False\n",
    "    new_transformer_block2.trainable = False\n",
    "\n",
    "    # Apply global average pooling to reduce the dimensions to (batch_size, 2)\n",
    "    pooled_phaz3 = GlobalAveragePooling1D()(concatenated_phaz3)\n",
    "\n",
    "    # Fully connected layers\n",
    "    FC1_phaz3 = Dense(512, activation='relu')(pooled_phaz3)\n",
    "    FC1_phaz3 = BatchNormalization()(FC1_phaz3)\n",
    "    FC2_phaz3 = Dropout(0.1)(FC1_phaz3)\n",
    "    FC3_phaz3 = Dense(256, activation='relu')(FC2_phaz3)\n",
    "    FC3_phaz3 = BatchNormalization()(FC3_phaz3)\n",
    "    FC4_phaz3 = Dropout(0.1)(FC3_phaz3)\n",
    "    FC5_phaz3 = Dense(64, activation='relu')(FC4_phaz3)\n",
    "\n",
    "    # Final prediction layer with softmax activation\n",
    "    predictions_phaz3 = Dense(2, activation='softmax')(FC5_phaz3)\n",
    "\n",
    "    # Define the complete model for Phase 3\n",
    "    model_phaz3 = Model(inputs=XDinput_phaz3, outputs=predictions_phaz3)\n",
    "\n",
    "    # Model summary\n",
    "    model_phaz3.summary()\n",
    "    \n",
    "    # Metrics for binary classification tasks\n",
    "    METRICS_BINARY = [\n",
    "        metrics.BinaryAccuracy(name='accuracy'),\n",
    "        metrics.AUC(name='auc'),\n",
    "    ]\n",
    "\n",
    "    # Metrics for categorical (multi-class) classification tasks\n",
    "    METRICS_CATEGORICAL = [\n",
    "        metrics.CategoricalAccuracy(name='accuracy'),\n",
    "        metrics.Precision(name='precision'),\n",
    "        metrics.Recall(name='recall'),\n",
    "        metrics.AUC(name='auc'),\n",
    "        metrics.F1Score(average='macro', name='f1_score')\n",
    "    ]\n",
    "\n",
    "\n",
    "    ###### Train and Validation ##### \n",
    "    # === Data Preparation ===\n",
    "    XD_np = np.array(XD)\n",
    "    labels_np = np.array(labels)\n",
    "    index = np.where(~np.isnan(labels_np))\n",
    "    labels_np, XD_np = labels_np[index[0]], XD_np[index[0], :]\n",
    "\n",
    "    # === Train-Test Split ===\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        XD_np, labels_np, test_size=0.2, stratify=labels_np, random_state=9\n",
    "    )\n",
    "\n",
    "    # Convert labels to categorical format\n",
    "    y_train_cat = to_categorical(y_train, num_classes=2)\n",
    "    y_test_cat = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    # Compute class weights to handle class imbalance\n",
    "    class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train.ravel()\n",
    "    )\n",
    "    class_weight = {\n",
    "    0: class_weights_array[0],\n",
    "    1: class_weights_array[1]\n",
    "    }\n",
    "\n",
    "    # === Define Optimizer for Interaction Model ===\n",
    "    opt = optimizers.Adam(learning_rate=0.0001)\n",
    "    interactionModel.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=METRICS_CATEGORICAL)\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=30, verbose=1, mode='min', restore_best_weights=True)\n",
    "    reduce_lr = LearningRateScheduler(lambda epoch: 1e-3 * 0.9 ** epoch)\n",
    "\n",
    "    # === Phase 1: Train Interaction Model ===\n",
    "    interactionModel.fit(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=256, epochs=n_epochs, class_weight=class_weight,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate Phase 1 on test data\n",
    "    test_eval_phase1 = interactionModel.evaluate(X_test, y_test_cat, verbose=1)\n",
    "\n",
    "    # === Feature Extraction ===\n",
    "    feature_train = model_feature.predict(X_train)\n",
    "    feature_test = model_feature.predict(X_test)\n",
    "\n",
    "    # === Phase 2: Train Model Phase 2 ===\n",
    "    opt_phaz2 = optimizers.Adam(learning_rate=0.001)\n",
    "    model_phaz2.compile(optimizer=opt_phaz2, loss=\"binary_crossentropy\", metrics=METRICS_BINARY)\n",
    "    model_phaz2.fit(\n",
    "    feature_train, y_train,\n",
    "    batch_size=256, epochs=n_epochs,class_weight=class_weight,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    "    )\n",
    "\n",
    "    # === Freeze Transformer Blocks ===\n",
    "    new_transformer_block.set_weights(transformer_block1.get_weights())\n",
    "    new_transformer_block2.set_weights(transformer_block2.get_weights())  # Corrected as per your comment\n",
    "\n",
    "    # === Phase 3: Train Model Phase 3 ===\n",
    "    opt_phaz3 = optimizers.Adam(learning_rate=0.0001)\n",
    "    model_phaz3.compile(optimizer=opt_phaz3, loss=\"categorical_crossentropy\", metrics=METRICS_CATEGORICAL)\n",
    "\n",
    "    model_phaz3.fit(\n",
    "        X_train, y_train_cat,\n",
    "        batch_size=256, epochs=n_epochs,class_weight=class_weight,\n",
    "        callbacks=[early_stopping], verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate Phase 3 on both training and testing data\n",
    "    train_eval_phase3 = model_phaz3.evaluate(X_train, y_train_cat, verbose=1)\n",
    "    test_eval_phase3 = model_phaz3.evaluate(X_test, y_test_cat, verbose=1)\n",
    "\n",
    "    # === Function to Format Results ===\n",
    "    def format_results(phase1_result, phase3_train_result, phase3_test_result):\n",
    "            # Define Metric Names\n",
    "        phase1_metrics = ['loss', 'accuracy', 'precision', 'recall', 'auc', 'f1_score']\n",
    "        phase3_metrics = ['loss', 'accuracy', 'precision', 'recall', 'auc', 'f1_score']\n",
    "\n",
    "        # Create DataFrames\n",
    "        phase1_test_df = pd.DataFrame([phase1_result], columns=phase1_metrics)\n",
    "        phase3_train_df = pd.DataFrame([phase3_train_result], columns=phase3_metrics)\n",
    "        phase3_test_df = pd.DataFrame([test_eval_phase3], columns=phase3_metrics)\n",
    "\n",
    "        # Display the Results\n",
    "        print(\"\\n=== Phase 1: Test Evaluation Results ===\")\n",
    "        print(phase1_test_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "        print(\"\\n=== Phase 3: Train Evaluation Results ===\")\n",
    "        print(phase3_train_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "        print(\"\\n=== Phase 3: Test Evaluation Results ===\")\n",
    "        print(phase3_test_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "\n",
    "    # === Display Results ===\n",
    "    format_results(test_eval_phase1, train_eval_phase3, test_eval_phase3)\n",
    "\n",
    "    return model_phaz3, train_eval_phase3, test_eval_phase3, X_test, y_test_cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f78c42f",
   "metadata": {},
   "source": [
    "# Run on Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e8e8600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_feature\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"model_feature\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_55 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m9,152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_56 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_57 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_35\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_35\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_140 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_20          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_141 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_142 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_143 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_35 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_140 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_20          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_60 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_141 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_61 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_142 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_143 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">217,026</span> (847.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m217,026\u001b[0m (847.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">215,490</span> (841.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m215,490\u001b[0m (841.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_20', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_21', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_22', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_23', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m15/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 463ms/step - accuracy: 0.7355 - auc: 0.7990 - f1_score: 0.4312 - loss: 0.7345 - precision: 0.7355 - recall: 0.7355"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_phaz3, train_eval_phase3, test_eval_phase3, X_test, y_test_cat = \u001b[43mtrain_deep_cbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../Data/tox21.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNR-PPAR-gamma\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmiles_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msmiles\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 375\u001b[39m, in \u001b[36mtrain_deep_cbn\u001b[39m\u001b[34m(dataset_path, target_col, smiles_col, n_epochs)\u001b[39m\n\u001b[32m    372\u001b[39m reduce_lr = LearningRateScheduler(\u001b[38;5;28;01mlambda\u001b[39;00m epoch: \u001b[32m1e-3\u001b[39m * \u001b[32m0.9\u001b[39m ** epoch)\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# === Phase 1: Train Interaction Model ===\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[43minteractionModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_cat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    379\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[38;5;66;03m# Evaluate Phase 1 on test data\u001b[39;00m\n\u001b[32m    382\u001b[39m test_eval_phase1 = interactionModel.evaluate(X_test, y_test_cat, verbose=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_phaz3, train_eval_phase3, test_eval_phase3, X_test, y_test_cat = train_deep_cbn(dataset_path = '../Data/tox21.csv', target_col = 'NR-PPAR-gamma', smiles_col= 'smiles', n_epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a5db0",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21a17958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.7820\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#function to calculate ROC_AUC\n",
    "\n",
    "def calculate_roc_auc(model, X_test, y_test_cat):\n",
    "    y_test = np.argmax(y_test_cat, axis=1)  # Convert one-hot-encoded labels to single-label format\n",
    "\n",
    "    # Get predicted probabilities for the positive class\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "\n",
    "    # Check if there are multiple output classes\n",
    "    if y_pred_proba.shape[1] > 1:\n",
    "        # Use probabilities of the positive class only\n",
    "        y_pred_proba = y_pred_proba[:, 1]\n",
    "\n",
    "    # Compute ROC-AUC score\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Compute ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "# Example usage:\n",
    "roc_auc = calculate_roc_auc(model_phaz3, X_test, y_test_cat)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b745fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(model, X_test, y_test_cat, class_names=None):\n",
    "    # Convert one-hot encoded labels to single-label format\n",
    "    y_test = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "    # Predict the class probabilities\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "\n",
    "    # Get the predicted class labels\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Normalize the confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Create a plot for the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=class_names)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='.2f')  # Use '.2f' for floats\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print the raw confusion matrix (optional)\n",
    "    print(\"Raw Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Print the normalized confusion matrix (optional)\n",
    "    print(\"\\nNormalized Confusion Matrix:\")\n",
    "    print(cm_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ef1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Confusion Matrix:\n",
      "[[1220   33]\n",
      " [  24   13]]\n",
      "\n",
      "Normalized Confusion Matrix:\n",
      "[[0.97366321 0.02633679]\n",
      " [0.64864865 0.35135135]]\n"
     ]
    }
   ],
   "source": [
    "class_names = ['Negative', 'Positive']\n",
    "\n",
    "# Call the function\n",
    "plot_confusion_matrix(model_phaz3, X_test, y_test_cat, class_names=class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b42b9",
   "metadata": {},
   "source": [
    "# Batch mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9545dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def process_multiple_targets(dataset_path, target_cols, smiles_col, n_epochs):\n",
    "    \"\"\"\n",
    "    Train a model for each target column in a dataset, generate confusion matrices,\n",
    "    and save the results into a unified dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to the dataset (CSV file).\n",
    "        target_cols (list): List of target columns to train and evaluate models on.\n",
    "        smiles_col (str): Name of the column containing SMILES strings.\n",
    "        n_epochs (int): Number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe containing results for all target columns.\n",
    "        dict: Dictionary containing the trained models for each target column.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store results for each target column\n",
    "    all_results = []\n",
    "    \n",
    "    # Initialize a dictionary to store models for each target column\n",
    "    models_dict = {}\n",
    "\n",
    "    for target_col in target_cols:\n",
    "        print(f\"Processing target column: {target_col}\")\n",
    "\n",
    "        # Train the model for the current target column\n",
    "        model, train_eval, test_eval, X_test, y_test_cat = train_deep_cbn(\n",
    "            dataset_path=dataset_path,\n",
    "            target_col=target_col,\n",
    "            smiles_col=smiles_col,\n",
    "            n_epochs=n_epochs\n",
    "        )\n",
    "\n",
    "        # Save the trained model into the dictionary\n",
    "        models_dict[target_col] = model\n",
    "\n",
    "        # Convert one-hot encoded labels back to single-label format\n",
    "        y_test = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "        # Predict probabilities for the test set\n",
    "        y_pred_proba = model.predict(X_test)\n",
    "\n",
    "        # Get predicted class labels\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "        # Compute confusion matrix for the current target column\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Normalize the confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        # Create a dictionary to store results for the current target column\n",
    "        results_dict = {\n",
    "            'target_col': target_col,\n",
    "            'train_loss': train_eval[0],\n",
    "            'train_accuracy': train_eval[1],\n",
    "            'test_loss': test_eval[0],\n",
    "            'test_accuracy': test_eval[1],\n",
    "            'confusion_matrix': cm.tolist(),  # Save as a list for serialization\n",
    "            'confusion_matrix_normalized': cm_normalized.tolist()  # Save normalized CM\n",
    "        }\n",
    "\n",
    "        # Append the results to the list\n",
    "        all_results.append(results_dict)\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "    return results_df, models_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97d2ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target column: NR-PPAR-gamma\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_feature\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"model_feature\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_154 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_155 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_156 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_154 (\u001b[38;5;33mConv1D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m9,152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_155 (\u001b[38;5;33mConv1D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_156 (\u001b[38;5;33mConv1D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_98\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_98\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_392 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_56          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_168 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_393 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_57          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_169 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_394 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_395 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_98 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_392 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_56          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_168 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_393 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_57          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_169 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_394 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_395 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">217,026</span> (847.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m217,026\u001b[0m (847.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">215,490</span> (841.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m215,490\u001b[0m (841.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"interactionModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"interactionModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ model_feature (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">217,026</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_42     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ model_feature (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m58,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_98 (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │       \u001b[38;5;34m217,026\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_42     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">275,522</span> (1.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m275,522\u001b[0m (1.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">273,986</span> (1.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m273,986\u001b[0m (1.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_56', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_57', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_101\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_101\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_99      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ input_layer_99[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ input_layer_99[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_28      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ difference (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ difference[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_99      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_56 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ input_layer_99[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_57 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ input_layer_99[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_56 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_57 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_28      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate_28[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ difference (\u001b[38;5;33mLambda\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ difference[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">297,216</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m297,216\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">297,216</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m297,216\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_58', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_59', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_104\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_104\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_102     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ model_feature       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> │ input_layer_102[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_29      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_416 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_416[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_178         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_417 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_178[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_417[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_179         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_418 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ dropout_179[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_419 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │ dense_418[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_102     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ model_feature       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m58,496\u001b[0m │ input_layer_102[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_58 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ model_feature[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_59 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ model_feature[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_58 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_59 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_29      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate_29[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_416 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m1,536\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ dense_416[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_178         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_417 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_178[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_417[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_179         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_418 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m16,448\u001b[0m │ dropout_179[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_419 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m130\u001b[0m │ dense_418[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">508,226</span> (1.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m508,226\u001b[0m (1.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,978</span> (589.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,978\u001b[0m (589.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">357,248</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m357,248\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 416ms/step - accuracy: 0.5595 - auc: 0.5495 - f1_score: 0.3804 - loss: 0.6682 - precision: 0.5595 - recall: 0.5595\n",
      "Epoch 2/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 463ms/step - accuracy: 0.5176 - auc: 0.5138 - f1_score: 0.3720 - loss: 0.6819 - precision: 0.5176 - recall: 0.5176\n",
      "Epoch 3/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 398ms/step - accuracy: 0.7025 - auc: 0.7408 - f1_score: 0.4721 - loss: 0.6799 - precision: 0.7025 - recall: 0.7025\n",
      "Epoch 4/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 364ms/step - accuracy: 0.7672 - auc: 0.8392 - f1_score: 0.4878 - loss: 0.6590 - precision: 0.7672 - recall: 0.7672\n",
      "Epoch 5/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 355ms/step - accuracy: 0.8262 - auc: 0.9015 - f1_score: 0.5176 - loss: 0.6376 - precision: 0.8262 - recall: 0.8262\n",
      "Epoch 6/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 352ms/step - accuracy: 0.6603 - auc: 0.6941 - f1_score: 0.4491 - loss: 0.6226 - precision: 0.6603 - recall: 0.6603\n",
      "Epoch 7/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 352ms/step - accuracy: 0.7328 - auc: 0.7850 - f1_score: 0.4794 - loss: 0.6197 - precision: 0.7328 - recall: 0.7328\n",
      "Epoch 8/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 363ms/step - accuracy: 0.7833 - auc: 0.8522 - f1_score: 0.5089 - loss: 0.6631 - precision: 0.7833 - recall: 0.7833\n",
      "Epoch 9/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 408ms/step - accuracy: 0.7016 - auc: 0.7506 - f1_score: 0.4708 - loss: 0.6445 - precision: 0.7016 - recall: 0.7016\n",
      "Epoch 10/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 369ms/step - accuracy: 0.8297 - auc: 0.9016 - f1_score: 0.5231 - loss: 0.5949 - precision: 0.8297 - recall: 0.8297\n",
      "Epoch 11/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 366ms/step - accuracy: 0.6878 - auc: 0.7452 - f1_score: 0.4611 - loss: 0.6671 - precision: 0.6878 - recall: 0.6878\n",
      "Epoch 12/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 393ms/step - accuracy: 0.8192 - auc: 0.8892 - f1_score: 0.5149 - loss: 0.6271 - precision: 0.8192 - recall: 0.8192\n",
      "Epoch 13/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 412ms/step - accuracy: 0.7117 - auc: 0.7736 - f1_score: 0.4780 - loss: 0.6386 - precision: 0.7117 - recall: 0.7117\n",
      "Epoch 14/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 364ms/step - accuracy: 0.8075 - auc: 0.8848 - f1_score: 0.5292 - loss: 0.6593 - precision: 0.8075 - recall: 0.8075\n",
      "Epoch 15/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 396ms/step - accuracy: 0.8572 - auc: 0.9271 - f1_score: 0.5584 - loss: 0.6358 - precision: 0.8572 - recall: 0.8572\n",
      "Epoch 16/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 436ms/step - accuracy: 0.8574 - auc: 0.9235 - f1_score: 0.5452 - loss: 0.6007 - precision: 0.8574 - recall: 0.8574\n",
      "Epoch 17/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 361ms/step - accuracy: 0.7183 - auc: 0.7849 - f1_score: 0.4821 - loss: 0.6246 - precision: 0.7183 - recall: 0.7183\n",
      "Epoch 18/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 352ms/step - accuracy: 0.8214 - auc: 0.8900 - f1_score: 0.5400 - loss: 0.6437 - precision: 0.8214 - recall: 0.8214\n",
      "Epoch 19/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 349ms/step - accuracy: 0.8525 - auc: 0.9269 - f1_score: 0.5489 - loss: 0.6181 - precision: 0.8525 - recall: 0.8525\n",
      "Epoch 20/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 345ms/step - accuracy: 0.7420 - auc: 0.8132 - f1_score: 0.5081 - loss: 0.6604 - precision: 0.7420 - recall: 0.7420\n",
      "Epoch 21/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 351ms/step - accuracy: 0.8481 - auc: 0.9281 - f1_score: 0.5353 - loss: 0.5604 - precision: 0.8481 - recall: 0.8481\n",
      "Epoch 22/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 341ms/step - accuracy: 0.6931 - auc: 0.7457 - f1_score: 0.4719 - loss: 0.6230 - precision: 0.6931 - recall: 0.6931\n",
      "Epoch 23/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 343ms/step - accuracy: 0.7888 - auc: 0.8646 - f1_score: 0.5219 - loss: 0.6149 - precision: 0.7888 - recall: 0.7888\n",
      "Epoch 24/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 340ms/step - accuracy: 0.8065 - auc: 0.8837 - f1_score: 0.5238 - loss: 0.6008 - precision: 0.8065 - recall: 0.8065\n",
      "Epoch 25/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 354ms/step - accuracy: 0.7998 - auc: 0.8790 - f1_score: 0.5342 - loss: 0.6590 - precision: 0.7998 - recall: 0.7998\n",
      "Epoch 26/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 352ms/step - accuracy: 0.7869 - auc: 0.8640 - f1_score: 0.5172 - loss: 0.5945 - precision: 0.7869 - recall: 0.7869\n",
      "Epoch 27/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 342ms/step - accuracy: 0.7651 - auc: 0.8444 - f1_score: 0.5029 - loss: 0.5837 - precision: 0.7651 - recall: 0.7651\n",
      "Epoch 28/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 353ms/step - accuracy: 0.8415 - auc: 0.9154 - f1_score: 0.5460 - loss: 0.5877 - precision: 0.8415 - recall: 0.8415\n",
      "Epoch 29/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 341ms/step - accuracy: 0.7054 - auc: 0.7722 - f1_score: 0.4798 - loss: 0.6060 - precision: 0.7054 - recall: 0.7054\n",
      "Epoch 30/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 350ms/step - accuracy: 0.7684 - auc: 0.8455 - f1_score: 0.5163 - loss: 0.6469 - precision: 0.7684 - recall: 0.7684\n",
      "Epoch 31/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 341ms/step - accuracy: 0.8001 - auc: 0.8742 - f1_score: 0.5276 - loss: 0.5914 - precision: 0.8001 - recall: 0.8001\n",
      "Epoch 32/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 344ms/step - accuracy: 0.7891 - auc: 0.8751 - f1_score: 0.5365 - loss: 0.6375 - precision: 0.7891 - recall: 0.7891\n",
      "Epoch 33/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 366ms/step - accuracy: 0.8861 - auc: 0.9545 - f1_score: 0.5816 - loss: 0.5745 - precision: 0.8861 - recall: 0.8861\n",
      "Epoch 34/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 347ms/step - accuracy: 0.8074 - auc: 0.8875 - f1_score: 0.5406 - loss: 0.6130 - precision: 0.8074 - recall: 0.8074\n",
      "Epoch 35/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 359ms/step - accuracy: 0.7968 - auc: 0.8797 - f1_score: 0.5350 - loss: 0.6081 - precision: 0.7968 - recall: 0.7968\n",
      "Epoch 36/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 341ms/step - accuracy: 0.8157 - auc: 0.8925 - f1_score: 0.5389 - loss: 0.6238 - precision: 0.8157 - recall: 0.8157\n",
      "Epoch 37/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 344ms/step - accuracy: 0.8693 - auc: 0.9397 - f1_score: 0.5751 - loss: 0.6082 - precision: 0.8693 - recall: 0.8693\n",
      "Epoch 38/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 345ms/step - accuracy: 0.8676 - auc: 0.9412 - f1_score: 0.5793 - loss: 0.6256 - precision: 0.8676 - recall: 0.8676\n",
      "Epoch 39/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 342ms/step - accuracy: 0.8433 - auc: 0.9208 - f1_score: 0.5581 - loss: 0.6089 - precision: 0.8433 - recall: 0.8433\n",
      "Epoch 40/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 347ms/step - accuracy: 0.8476 - auc: 0.9308 - f1_score: 0.5601 - loss: 0.5934 - precision: 0.8476 - recall: 0.8476\n",
      "Epoch 41/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 345ms/step - accuracy: 0.8667 - auc: 0.9428 - f1_score: 0.5825 - loss: 0.6289 - precision: 0.8667 - recall: 0.8667\n",
      "Epoch 42/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 357ms/step - accuracy: 0.7775 - auc: 0.8603 - f1_score: 0.5219 - loss: 0.5881 - precision: 0.7775 - recall: 0.7775\n",
      "Epoch 43/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 348ms/step - accuracy: 0.7795 - auc: 0.8517 - f1_score: 0.5278 - loss: 0.5818 - precision: 0.7795 - recall: 0.7795\n",
      "Epoch 44/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 358ms/step - accuracy: 0.8695 - auc: 0.9400 - f1_score: 0.5856 - loss: 0.5891 - precision: 0.8695 - recall: 0.8695\n",
      "Epoch 45/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 348ms/step - accuracy: 0.8908 - auc: 0.9581 - f1_score: 0.5962 - loss: 0.5743 - precision: 0.8908 - recall: 0.8908\n",
      "Epoch 46/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 350ms/step - accuracy: 0.8372 - auc: 0.9166 - f1_score: 0.5668 - loss: 0.6219 - precision: 0.8372 - recall: 0.8372\n",
      "Epoch 47/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 341ms/step - accuracy: 0.8819 - auc: 0.9516 - f1_score: 0.5792 - loss: 0.5615 - precision: 0.8819 - recall: 0.8819\n",
      "Epoch 48/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 362ms/step - accuracy: 0.8395 - auc: 0.9131 - f1_score: 0.5604 - loss: 0.6218 - precision: 0.8395 - recall: 0.8395\n",
      "Epoch 49/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 343ms/step - accuracy: 0.8652 - auc: 0.9396 - f1_score: 0.5721 - loss: 0.5715 - precision: 0.8652 - recall: 0.8652\n",
      "Epoch 50/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 341ms/step - accuracy: 0.8875 - auc: 0.9535 - f1_score: 0.5859 - loss: 0.5350 - precision: 0.8875 - recall: 0.8875\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8719 - auc: 0.9437 - f1_score: 0.5427 - loss: 0.5438 - precision: 0.8719 - recall: 0.8719\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Epoch 1/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 513ms/step - accuracy: 0.4348 - auc: 0.4914 - loss: 1.0918\n",
      "Epoch 2/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 523ms/step - accuracy: 0.4610 - auc: 0.6011 - loss: 0.6806\n",
      "Epoch 3/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 501ms/step - accuracy: 0.6834 - auc: 0.5591 - loss: 0.6697\n",
      "Epoch 4/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 520ms/step - accuracy: 0.5664 - auc: 0.6484 - loss: 0.6544\n",
      "Epoch 5/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 552ms/step - accuracy: 0.7813 - auc: 0.6509 - loss: 0.6315\n",
      "Epoch 6/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 515ms/step - accuracy: 0.4182 - auc: 0.6227 - loss: 0.6787\n",
      "Epoch 7/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 512ms/step - accuracy: 0.5574 - auc: 0.6666 - loss: 0.6699\n",
      "Epoch 8/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 517ms/step - accuracy: 0.7231 - auc: 0.6495 - loss: 0.6867\n",
      "Epoch 9/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 522ms/step - accuracy: 0.6942 - auc: 0.7111 - loss: 0.6251\n",
      "Epoch 10/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 513ms/step - accuracy: 0.7500 - auc: 0.7289 - loss: 0.6284\n",
      "Epoch 11/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 517ms/step - accuracy: 0.8032 - auc: 0.7254 - loss: 0.6281\n",
      "Epoch 12/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 503ms/step - accuracy: 0.6979 - auc: 0.7490 - loss: 0.6039\n",
      "Epoch 13/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 502ms/step - accuracy: 0.7150 - auc: 0.7524 - loss: 0.5930\n",
      "Epoch 14/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 513ms/step - accuracy: 0.7690 - auc: 0.8271 - loss: 0.5090\n",
      "Epoch 15/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 522ms/step - accuracy: 0.7137 - auc: 0.8202 - loss: 0.5362\n",
      "Epoch 16/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 503ms/step - accuracy: 0.7941 - auc: 0.7569 - loss: 0.6394\n",
      "Epoch 17/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 511ms/step - accuracy: 0.6974 - auc: 0.7829 - loss: 0.5779\n",
      "Epoch 18/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 502ms/step - accuracy: 0.7603 - auc: 0.8406 - loss: 0.5355\n",
      "Epoch 19/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 504ms/step - accuracy: 0.8428 - auc: 0.8511 - loss: 0.4614\n",
      "Epoch 20/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 504ms/step - accuracy: 0.7794 - auc: 0.8675 - loss: 0.4698\n",
      "Epoch 21/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 523ms/step - accuracy: 0.8079 - auc: 0.8627 - loss: 0.4792\n",
      "Epoch 22/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 569ms/step - accuracy: 0.8450 - auc: 0.8844 - loss: 0.4278\n",
      "Epoch 23/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 506ms/step - accuracy: 0.8545 - auc: 0.9330 - loss: 0.3709\n",
      "Epoch 24/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 504ms/step - accuracy: 0.8753 - auc: 0.9302 - loss: 0.3566\n",
      "Epoch 25/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 513ms/step - accuracy: 0.8957 - auc: 0.9391 - loss: 0.3425\n",
      "Epoch 26/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 504ms/step - accuracy: 0.8881 - auc: 0.9476 - loss: 0.3179\n",
      "Epoch 27/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 512ms/step - accuracy: 0.8758 - auc: 0.9612 - loss: 0.2868\n",
      "Epoch 28/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 522ms/step - accuracy: 0.8659 - auc: 0.9569 - loss: 0.3015\n",
      "Epoch 29/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 517ms/step - accuracy: 0.9015 - auc: 0.9515 - loss: 0.2944\n",
      "Epoch 30/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 520ms/step - accuracy: 0.9268 - auc: 0.9806 - loss: 0.2021\n",
      "Epoch 31/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 509ms/step - accuracy: 0.9132 - auc: 0.9729 - loss: 0.2358\n",
      "Epoch 32/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 514ms/step - accuracy: 0.9182 - auc: 0.9612 - loss: 0.2714\n",
      "Epoch 33/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 514ms/step - accuracy: 0.9351 - auc: 0.9756 - loss: 0.2179\n",
      "Epoch 34/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 517ms/step - accuracy: 0.9025 - auc: 0.9535 - loss: 0.2970\n",
      "Epoch 35/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 523ms/step - accuracy: 0.8947 - auc: 0.9731 - loss: 0.2249\n",
      "Epoch 36/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 506ms/step - accuracy: 0.8755 - auc: 0.9619 - loss: 0.2752\n",
      "Epoch 37/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 506ms/step - accuracy: 0.9454 - auc: 0.9851 - loss: 0.1659\n",
      "Epoch 38/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 512ms/step - accuracy: 0.9347 - auc: 0.9860 - loss: 0.1559\n",
      "Epoch 39/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 532ms/step - accuracy: 0.9579 - auc: 0.9928 - loss: 0.1176\n",
      "Epoch 40/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 521ms/step - accuracy: 0.9591 - auc: 0.9918 - loss: 0.1134\n",
      "Epoch 41/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 525ms/step - accuracy: 0.9111 - auc: 0.9780 - loss: 0.1941\n",
      "Epoch 42/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 515ms/step - accuracy: 0.9522 - auc: 0.9920 - loss: 0.1208\n",
      "Epoch 43/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 511ms/step - accuracy: 0.9544 - auc: 0.9901 - loss: 0.1274\n",
      "Epoch 44/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 508ms/step - accuracy: 0.9490 - auc: 0.9924 - loss: 0.1056\n",
      "Epoch 45/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 509ms/step - accuracy: 0.9285 - auc: 0.9756 - loss: 0.2050\n",
      "Epoch 46/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 513ms/step - accuracy: 0.9536 - auc: 0.9883 - loss: 0.1296\n",
      "Epoch 47/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 533ms/step - accuracy: 0.9445 - auc: 0.9864 - loss: 0.1491\n",
      "Epoch 48/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 525ms/step - accuracy: 0.9142 - auc: 0.9698 - loss: 0.2423\n",
      "Epoch 49/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 514ms/step - accuracy: 0.9061 - auc: 0.9614 - loss: 0.2776\n",
      "Epoch 50/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 529ms/step - accuracy: 0.9328 - auc: 0.9840 - loss: 0.1636\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Epoch 1/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 264ms/step - accuracy: 0.7236 - auc: 0.7467 - f1_score: 0.4778 - loss: 0.7817 - precision: 0.7236 - recall: 0.7236\n",
      "Epoch 2/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 266ms/step - accuracy: 0.8856 - auc: 0.9177 - f1_score: 0.6381 - loss: 0.2343 - precision: 0.8856 - recall: 0.8856\n",
      "Epoch 3/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 268ms/step - accuracy: 0.9566 - auc: 0.9636 - f1_score: 0.7578 - loss: 0.1585 - precision: 0.9566 - recall: 0.9566\n",
      "Epoch 4/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9604 - auc: 0.9708 - f1_score: 0.7550 - loss: 0.1442 - precision: 0.9604 - recall: 0.9604\n",
      "Epoch 5/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9558 - auc: 0.9713 - f1_score: 0.7545 - loss: 0.1227 - precision: 0.9558 - recall: 0.9558\n",
      "Epoch 6/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 268ms/step - accuracy: 0.9614 - auc: 0.9742 - f1_score: 0.7869 - loss: 0.1223 - precision: 0.9614 - recall: 0.9614\n",
      "Epoch 7/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 277ms/step - accuracy: 0.9632 - auc: 0.9801 - f1_score: 0.7850 - loss: 0.1201 - precision: 0.9632 - recall: 0.9632\n",
      "Epoch 8/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 265ms/step - accuracy: 0.9609 - auc: 0.9778 - f1_score: 0.7834 - loss: 0.1297 - precision: 0.9609 - recall: 0.9609\n",
      "Epoch 9/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 256ms/step - accuracy: 0.9577 - auc: 0.9744 - f1_score: 0.7817 - loss: 0.1142 - precision: 0.9577 - recall: 0.9577\n",
      "Epoch 10/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 286ms/step - accuracy: 0.9552 - auc: 0.9795 - f1_score: 0.7624 - loss: 0.1028 - precision: 0.9552 - recall: 0.9552\n",
      "Epoch 11/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 264ms/step - accuracy: 0.9525 - auc: 0.9762 - f1_score: 0.7462 - loss: 0.1423 - precision: 0.9525 - recall: 0.9525\n",
      "Epoch 12/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 261ms/step - accuracy: 0.9519 - auc: 0.9803 - f1_score: 0.7692 - loss: 0.1063 - precision: 0.9519 - recall: 0.9519\n",
      "Epoch 13/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 254ms/step - accuracy: 0.9549 - auc: 0.9850 - f1_score: 0.7662 - loss: 0.1230 - precision: 0.9549 - recall: 0.9549\n",
      "Epoch 14/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258ms/step - accuracy: 0.9570 - auc: 0.9843 - f1_score: 0.7694 - loss: 0.0924 - precision: 0.9570 - recall: 0.9570\n",
      "Epoch 15/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.9552 - auc: 0.9859 - f1_score: 0.7585 - loss: 0.1087 - precision: 0.9552 - recall: 0.9552\n",
      "Epoch 16/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.9515 - auc: 0.9836 - f1_score: 0.7627 - loss: 0.1052 - precision: 0.9515 - recall: 0.9515\n",
      "Epoch 17/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 255ms/step - accuracy: 0.9591 - auc: 0.9861 - f1_score: 0.7818 - loss: 0.1047 - precision: 0.9591 - recall: 0.9591\n",
      "Epoch 18/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.9626 - auc: 0.9894 - f1_score: 0.7990 - loss: 0.0839 - precision: 0.9626 - recall: 0.9626\n",
      "Epoch 19/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 269ms/step - accuracy: 0.9612 - auc: 0.9889 - f1_score: 0.7782 - loss: 0.0947 - precision: 0.9612 - recall: 0.9612\n",
      "Epoch 20/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 266ms/step - accuracy: 0.9523 - auc: 0.9852 - f1_score: 0.7556 - loss: 0.1129 - precision: 0.9523 - recall: 0.9523\n",
      "Epoch 21/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 263ms/step - accuracy: 0.9575 - auc: 0.9877 - f1_score: 0.7695 - loss: 0.0832 - precision: 0.9575 - recall: 0.9575\n",
      "Epoch 22/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 260ms/step - accuracy: 0.9602 - auc: 0.9875 - f1_score: 0.7803 - loss: 0.1027 - precision: 0.9602 - recall: 0.9602\n",
      "Epoch 23/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 256ms/step - accuracy: 0.9616 - auc: 0.9887 - f1_score: 0.7818 - loss: 0.0875 - precision: 0.9616 - recall: 0.9616\n",
      "Epoch 24/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 268ms/step - accuracy: 0.9583 - auc: 0.9895 - f1_score: 0.7893 - loss: 0.1000 - precision: 0.9583 - recall: 0.9583\n",
      "Epoch 25/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.9658 - auc: 0.9913 - f1_score: 0.8109 - loss: 0.0769 - precision: 0.9658 - recall: 0.9658\n",
      "Epoch 26/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 254ms/step - accuracy: 0.9620 - auc: 0.9904 - f1_score: 0.7969 - loss: 0.1038 - precision: 0.9620 - recall: 0.9620\n",
      "Epoch 27/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 259ms/step - accuracy: 0.9645 - auc: 0.9903 - f1_score: 0.8048 - loss: 0.1080 - precision: 0.9645 - recall: 0.9645\n",
      "Epoch 28/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 255ms/step - accuracy: 0.9622 - auc: 0.9917 - f1_score: 0.7935 - loss: 0.0855 - precision: 0.9622 - recall: 0.9622\n",
      "Epoch 29/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9551 - auc: 0.9875 - f1_score: 0.7639 - loss: 0.1019 - precision: 0.9551 - recall: 0.9551\n",
      "Epoch 30/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 256ms/step - accuracy: 0.9490 - auc: 0.9907 - f1_score: 0.7548 - loss: 0.1091 - precision: 0.9490 - recall: 0.9490\n",
      "Epoch 31/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 267ms/step - accuracy: 0.9477 - auc: 0.9892 - f1_score: 0.7628 - loss: 0.0933 - precision: 0.9477 - recall: 0.9477\n",
      "Epoch 32/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 266ms/step - accuracy: 0.9603 - auc: 0.9935 - f1_score: 0.7877 - loss: 0.0862 - precision: 0.9603 - recall: 0.9603\n",
      "Epoch 33/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9644 - auc: 0.9932 - f1_score: 0.8071 - loss: 0.0737 - precision: 0.9644 - recall: 0.9644\n",
      "Epoch 34/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 254ms/step - accuracy: 0.9611 - auc: 0.9927 - f1_score: 0.7752 - loss: 0.0918 - precision: 0.9611 - recall: 0.9611\n",
      "Epoch 35/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.9644 - auc: 0.9932 - f1_score: 0.8004 - loss: 0.0739 - precision: 0.9644 - recall: 0.9644\n",
      "Epoch 36/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 254ms/step - accuracy: 0.9619 - auc: 0.9926 - f1_score: 0.7923 - loss: 0.1025 - precision: 0.9619 - recall: 0.9619\n",
      "Epoch 37/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 256ms/step - accuracy: 0.9663 - auc: 0.9918 - f1_score: 0.8090 - loss: 0.0785 - precision: 0.9663 - recall: 0.9663\n",
      "Epoch 38/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9576 - auc: 0.9913 - f1_score: 0.7782 - loss: 0.0823 - precision: 0.9576 - recall: 0.9576\n",
      "Epoch 39/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.9508 - auc: 0.9891 - f1_score: 0.7649 - loss: 0.0874 - precision: 0.9508 - recall: 0.9508\n",
      "Epoch 40/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 255ms/step - accuracy: 0.9521 - auc: 0.9915 - f1_score: 0.7610 - loss: 0.0902 - precision: 0.9521 - recall: 0.9521\n",
      "Epoch 41/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 255ms/step - accuracy: 0.9587 - auc: 0.9925 - f1_score: 0.7702 - loss: 0.0973 - precision: 0.9587 - recall: 0.9587\n",
      "Epoch 42/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 262ms/step - accuracy: 0.9549 - auc: 0.9911 - f1_score: 0.7718 - loss: 0.0785 - precision: 0.9549 - recall: 0.9549\n",
      "Epoch 43/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 275ms/step - accuracy: 0.9612 - auc: 0.9917 - f1_score: 0.8055 - loss: 0.1119 - precision: 0.9612 - recall: 0.9612\n",
      "Epoch 44/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 275ms/step - accuracy: 0.9610 - auc: 0.9920 - f1_score: 0.7870 - loss: 0.0873 - precision: 0.9610 - recall: 0.9610\n",
      "Epoch 45/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 261ms/step - accuracy: 0.9501 - auc: 0.9906 - f1_score: 0.7457 - loss: 0.0846 - precision: 0.9501 - recall: 0.9501\n",
      "Epoch 46/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 256ms/step - accuracy: 0.9517 - auc: 0.9914 - f1_score: 0.7714 - loss: 0.0995 - precision: 0.9517 - recall: 0.9517\n",
      "Epoch 47/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.9561 - auc: 0.9905 - f1_score: 0.7424 - loss: 0.0802 - precision: 0.9561 - recall: 0.9561\n",
      "Epoch 48/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 255ms/step - accuracy: 0.9531 - auc: 0.9889 - f1_score: 0.7647 - loss: 0.0843 - precision: 0.9531 - recall: 0.9531\n",
      "Epoch 49/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 265ms/step - accuracy: 0.9470 - auc: 0.9835 - f1_score: 0.7408 - loss: 0.1123 - precision: 0.9470 - recall: 0.9470\n",
      "Epoch 50/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.9557 - auc: 0.9906 - f1_score: 0.7569 - loss: 0.0971 - precision: 0.9557 - recall: 0.9557\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - accuracy: 0.9843 - auc: 0.9985 - f1_score: 0.8901 - loss: 0.0478 - precision: 0.9843 - recall: 0.9843\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9694 - auc: 0.9864 - f1_score: 0.7022 - loss: 0.1196 - precision: 0.9694 - recall: 0.9694\n",
      "\n",
      "=== Phase 1: Test Evaluation Results ===\n",
      "  loss  accuracy  precision  recall    auc  f1_score\n",
      "0.5465    0.8682     0.8682  0.8682 0.9382    0.5516\n",
      "\n",
      "=== Phase 3: Train Evaluation Results ===\n",
      "  loss  accuracy  precision  recall    auc  f1_score\n",
      "0.0362    0.9880     0.9880  0.9880 0.9993    0.9046\n",
      "\n",
      "=== Phase 3: Test Evaluation Results ===\n",
      "  loss  accuracy  precision  recall    auc  f1_score\n",
      "0.1521    0.9612     0.9612  0.9612 0.9837    0.6695\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
      "Processing target column: NR-AhR\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_feature\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"model_feature\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_165 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_166 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_167 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_165 (\u001b[38;5;33mConv1D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m9,152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_166 (\u001b[38;5;33mConv1D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_167 (\u001b[38;5;33mConv1D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_105\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_105\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_420 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_60          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_180 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_421 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_61          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_181 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_422 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_423 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_105 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_420 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_60          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_180 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_421 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_61          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_181 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_422 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_423 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">217,026</span> (847.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m217,026\u001b[0m (847.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">215,490</span> (841.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m215,490\u001b[0m (841.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"interactionModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"interactionModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ model_feature (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">217,026</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_45     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ model_feature (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m58,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_105 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │       \u001b[38;5;34m217,026\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_45     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">275,522</span> (1.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m275,522\u001b[0m (1.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">273,986</span> (1.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m273,986\u001b[0m (1.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_60', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_61', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_108\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_108\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_106     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ input_layer_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ input_layer_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_30      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ difference (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ difference[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_106     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_60 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ input_layer_106[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_61 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ input_layer_106[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_60 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_61 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_30      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate_30[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ difference (\u001b[38;5;33mLambda\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ difference[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">297,216</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m297,216\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">297,216</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m297,216\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_62', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_63', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_111\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_111\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_109     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ model_feature       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> │ input_layer_109[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_31      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_444 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_444[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_190         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_445 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_190[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_445[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_191         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_446 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ dropout_191[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_447 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │ dense_446[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_109     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ model_feature       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m58,496\u001b[0m │ input_layer_109[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_62 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ model_feature[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_63 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ model_feature[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_62 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_63 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_31      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate_31[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_444 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m1,536\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ dense_444[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_190         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_445 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_190[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_445[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_191         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_446 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m16,448\u001b[0m │ dropout_191[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_447 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m130\u001b[0m │ dense_446[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">508,226</span> (1.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m508,226\u001b[0m (1.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,978</span> (589.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,978\u001b[0m (589.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">357,248</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m357,248\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 357ms/step - accuracy: 0.6018 - auc: 0.6513 - f1_score: 0.4767 - loss: 0.6831 - precision: 0.6018 - recall: 0.6018\n",
      "Epoch 2/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 354ms/step - accuracy: 0.7738 - auc: 0.8570 - f1_score: 0.5905 - loss: 0.6043 - precision: 0.7738 - recall: 0.7738\n",
      "Epoch 3/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 363ms/step - accuracy: 0.7207 - auc: 0.8069 - f1_score: 0.6001 - loss: 0.6133 - precision: 0.7207 - recall: 0.7207\n",
      "Epoch 4/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 357ms/step - accuracy: 0.7521 - auc: 0.8384 - f1_score: 0.6150 - loss: 0.6011 - precision: 0.7521 - recall: 0.7521\n",
      "Epoch 5/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 351ms/step - accuracy: 0.7212 - auc: 0.8116 - f1_score: 0.5900 - loss: 0.5920 - precision: 0.7212 - recall: 0.7212\n",
      "Epoch 6/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 348ms/step - accuracy: 0.6081 - auc: 0.6730 - f1_score: 0.5317 - loss: 0.6134 - precision: 0.6081 - recall: 0.6081\n",
      "Epoch 7/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 345ms/step - accuracy: 0.7489 - auc: 0.8420 - f1_score: 0.6156 - loss: 0.5942 - precision: 0.7489 - recall: 0.7489\n",
      "Epoch 8/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 348ms/step - accuracy: 0.6166 - auc: 0.6846 - f1_score: 0.5391 - loss: 0.6244 - precision: 0.6166 - recall: 0.6166\n",
      "Epoch 9/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 347ms/step - accuracy: 0.6637 - auc: 0.7417 - f1_score: 0.5633 - loss: 0.6103 - precision: 0.6637 - recall: 0.6637\n",
      "Epoch 10/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 365ms/step - accuracy: 0.7676 - auc: 0.8588 - f1_score: 0.6322 - loss: 0.5939 - precision: 0.7676 - recall: 0.7676\n",
      "Epoch 11/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 352ms/step - accuracy: 0.6857 - auc: 0.7838 - f1_score: 0.5798 - loss: 0.5905 - precision: 0.6857 - recall: 0.6857\n",
      "Epoch 12/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 360ms/step - accuracy: 0.6859 - auc: 0.7761 - f1_score: 0.5818 - loss: 0.5992 - precision: 0.6859 - recall: 0.6859\n",
      "Epoch 13/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 371ms/step - accuracy: 0.6675 - auc: 0.7527 - f1_score: 0.5819 - loss: 0.6160 - precision: 0.6675 - recall: 0.6675\n",
      "Epoch 14/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 352ms/step - accuracy: 0.7869 - auc: 0.8713 - f1_score: 0.6399 - loss: 0.5910 - precision: 0.7869 - recall: 0.7869\n",
      "Epoch 15/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 351ms/step - accuracy: 0.6720 - auc: 0.7623 - f1_score: 0.5737 - loss: 0.5966 - precision: 0.6720 - recall: 0.6720\n",
      "Epoch 16/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 354ms/step - accuracy: 0.6575 - auc: 0.7536 - f1_score: 0.5658 - loss: 0.6005 - precision: 0.6575 - recall: 0.6575\n",
      "Epoch 17/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 354ms/step - accuracy: 0.7035 - auc: 0.7995 - f1_score: 0.6019 - loss: 0.6090 - precision: 0.7035 - recall: 0.7035\n",
      "Epoch 18/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 379ms/step - accuracy: 0.7758 - auc: 0.8738 - f1_score: 0.6221 - loss: 0.5751 - precision: 0.7758 - recall: 0.7758\n",
      "Epoch 19/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 355ms/step - accuracy: 0.6309 - auc: 0.6967 - f1_score: 0.5457 - loss: 0.5891 - precision: 0.6309 - recall: 0.6309\n",
      "Epoch 20/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 347ms/step - accuracy: 0.7125 - auc: 0.8177 - f1_score: 0.6053 - loss: 0.5965 - precision: 0.7125 - recall: 0.7125\n",
      "Epoch 21/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 351ms/step - accuracy: 0.6976 - auc: 0.8013 - f1_score: 0.5909 - loss: 0.5805 - precision: 0.6976 - recall: 0.6976\n",
      "Epoch 22/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 359ms/step - accuracy: 0.6542 - auc: 0.7535 - f1_score: 0.5657 - loss: 0.5966 - precision: 0.6542 - recall: 0.6542\n",
      "Epoch 23/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 347ms/step - accuracy: 0.7612 - auc: 0.8621 - f1_score: 0.6242 - loss: 0.5725 - precision: 0.7612 - recall: 0.7612\n",
      "Epoch 24/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 346ms/step - accuracy: 0.6248 - auc: 0.6914 - f1_score: 0.5469 - loss: 0.5973 - precision: 0.6248 - recall: 0.6248\n",
      "Epoch 25/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 347ms/step - accuracy: 0.7374 - auc: 0.8399 - f1_score: 0.6281 - loss: 0.5982 - precision: 0.7374 - recall: 0.7374\n",
      "Epoch 26/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 358ms/step - accuracy: 0.7272 - auc: 0.8305 - f1_score: 0.6063 - loss: 0.5860 - precision: 0.7272 - recall: 0.7272\n",
      "Epoch 27/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 347ms/step - accuracy: 0.6779 - auc: 0.7725 - f1_score: 0.5837 - loss: 0.5988 - precision: 0.6779 - recall: 0.6779\n",
      "Epoch 28/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 346ms/step - accuracy: 0.7332 - auc: 0.8357 - f1_score: 0.6223 - loss: 0.5855 - precision: 0.7332 - recall: 0.7332\n",
      "Epoch 29/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 345ms/step - accuracy: 0.6636 - auc: 0.7532 - f1_score: 0.5770 - loss: 0.6052 - precision: 0.6636 - recall: 0.6636\n",
      "Epoch 30/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 345ms/step - accuracy: 0.7516 - auc: 0.8474 - f1_score: 0.6408 - loss: 0.5996 - precision: 0.7516 - recall: 0.7516\n",
      "Epoch 31/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 368ms/step - accuracy: 0.7341 - auc: 0.8375 - f1_score: 0.6246 - loss: 0.5913 - precision: 0.7341 - recall: 0.7341\n",
      "Epoch 32/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 346ms/step - accuracy: 0.7057 - auc: 0.8127 - f1_score: 0.5990 - loss: 0.5906 - precision: 0.7057 - recall: 0.7057\n",
      "Epoch 33/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 348ms/step - accuracy: 0.7064 - auc: 0.8095 - f1_score: 0.6116 - loss: 0.5992 - precision: 0.7064 - recall: 0.7064\n",
      "Epoch 34/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 346ms/step - accuracy: 0.7431 - auc: 0.8439 - f1_score: 0.6281 - loss: 0.5800 - precision: 0.7431 - recall: 0.7431\n",
      "Epoch 35/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 347ms/step - accuracy: 0.7150 - auc: 0.8195 - f1_score: 0.6113 - loss: 0.5785 - precision: 0.7150 - recall: 0.7150\n",
      "Epoch 36/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 345ms/step - accuracy: 0.7355 - auc: 0.8352 - f1_score: 0.6209 - loss: 0.5744 - precision: 0.7355 - recall: 0.7355\n",
      "Epoch 37/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 345ms/step - accuracy: 0.6922 - auc: 0.7918 - f1_score: 0.5963 - loss: 0.5857 - precision: 0.6922 - recall: 0.6922\n",
      "Epoch 38/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 347ms/step - accuracy: 0.7388 - auc: 0.8408 - f1_score: 0.6261 - loss: 0.5817 - precision: 0.7388 - recall: 0.7388\n",
      "Epoch 39/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 353ms/step - accuracy: 0.7376 - auc: 0.8363 - f1_score: 0.6300 - loss: 0.5940 - precision: 0.7376 - recall: 0.7376\n",
      "Epoch 40/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 360ms/step - accuracy: 0.7358 - auc: 0.8393 - f1_score: 0.6291 - loss: 0.5906 - precision: 0.7358 - recall: 0.7358\n",
      "Epoch 41/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 359ms/step - accuracy: 0.7396 - auc: 0.8361 - f1_score: 0.6299 - loss: 0.5883 - precision: 0.7396 - recall: 0.7396\n",
      "Epoch 42/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 357ms/step - accuracy: 0.7533 - auc: 0.8498 - f1_score: 0.6280 - loss: 0.5746 - precision: 0.7533 - recall: 0.7533\n",
      "Epoch 43/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 374ms/step - accuracy: 0.7095 - auc: 0.8110 - f1_score: 0.6141 - loss: 0.5849 - precision: 0.7095 - recall: 0.7095\n",
      "Epoch 44/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 346ms/step - accuracy: 0.7366 - auc: 0.8427 - f1_score: 0.6259 - loss: 0.5940 - precision: 0.7366 - recall: 0.7366\n",
      "Epoch 45/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 345ms/step - accuracy: 0.7539 - auc: 0.8557 - f1_score: 0.6402 - loss: 0.5746 - precision: 0.7539 - recall: 0.7539\n",
      "Epoch 46/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 348ms/step - accuracy: 0.7276 - auc: 0.8326 - f1_score: 0.6202 - loss: 0.5805 - precision: 0.7276 - recall: 0.7276\n",
      "Epoch 47/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 349ms/step - accuracy: 0.7254 - auc: 0.8345 - f1_score: 0.6175 - loss: 0.5766 - precision: 0.7254 - recall: 0.7254\n",
      "Epoch 48/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 346ms/step - accuracy: 0.7239 - auc: 0.8275 - f1_score: 0.6270 - loss: 0.5905 - precision: 0.7239 - recall: 0.7239\n",
      "Epoch 49/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 360ms/step - accuracy: 0.7450 - auc: 0.8437 - f1_score: 0.6379 - loss: 0.5881 - precision: 0.7450 - recall: 0.7450\n",
      "Epoch 50/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 364ms/step - accuracy: 0.7374 - auc: 0.8377 - f1_score: 0.6327 - loss: 0.5848 - precision: 0.7374 - recall: 0.7374\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7939 - auc: 0.8920 - f1_score: 0.6354 - loss: 0.5354 - precision: 0.7939 - recall: 0.7939\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 511ms/step - accuracy: 0.4959 - auc: 0.5304 - loss: 0.9095\n",
      "Epoch 2/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 510ms/step - accuracy: 0.6047 - auc: 0.7221 - loss: 0.6310\n",
      "Epoch 3/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 510ms/step - accuracy: 0.7031 - auc: 0.7835 - loss: 0.5716\n",
      "Epoch 4/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 504ms/step - accuracy: 0.7348 - auc: 0.7944 - loss: 0.5390\n",
      "Epoch 5/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 521ms/step - accuracy: 0.7243 - auc: 0.8092 - loss: 0.5332\n",
      "Epoch 6/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 522ms/step - accuracy: 0.6875 - auc: 0.8033 - loss: 0.5534\n",
      "Epoch 7/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 507ms/step - accuracy: 0.7277 - auc: 0.8004 - loss: 0.5413\n",
      "Epoch 8/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 506ms/step - accuracy: 0.7345 - auc: 0.8295 - loss: 0.5226\n",
      "Epoch 9/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 505ms/step - accuracy: 0.7701 - auc: 0.8271 - loss: 0.5136\n",
      "Epoch 10/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 508ms/step - accuracy: 0.7378 - auc: 0.8375 - loss: 0.5033\n",
      "Epoch 11/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 512ms/step - accuracy: 0.7616 - auc: 0.8416 - loss: 0.5094\n",
      "Epoch 12/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 553ms/step - accuracy: 0.7735 - auc: 0.8568 - loss: 0.4741\n",
      "Epoch 13/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 520ms/step - accuracy: 0.7179 - auc: 0.8295 - loss: 0.5139\n",
      "Epoch 14/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 524ms/step - accuracy: 0.7939 - auc: 0.8648 - loss: 0.4581\n",
      "Epoch 15/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 516ms/step - accuracy: 0.7872 - auc: 0.8793 - loss: 0.4445\n",
      "Epoch 16/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 520ms/step - accuracy: 0.7958 - auc: 0.8824 - loss: 0.4411\n",
      "Epoch 17/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 524ms/step - accuracy: 0.7889 - auc: 0.8932 - loss: 0.4149\n",
      "Epoch 18/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 524ms/step - accuracy: 0.7981 - auc: 0.8874 - loss: 0.4318\n",
      "Epoch 19/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 518ms/step - accuracy: 0.8400 - auc: 0.9013 - loss: 0.4035\n",
      "Epoch 20/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 536ms/step - accuracy: 0.7953 - auc: 0.8951 - loss: 0.4102\n",
      "Epoch 21/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 538ms/step - accuracy: 0.8449 - auc: 0.9194 - loss: 0.3779\n",
      "Epoch 22/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 524ms/step - accuracy: 0.8022 - auc: 0.9161 - loss: 0.3846\n",
      "Epoch 23/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 513ms/step - accuracy: 0.8533 - auc: 0.9273 - loss: 0.3490\n",
      "Epoch 24/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 512ms/step - accuracy: 0.8362 - auc: 0.9164 - loss: 0.3750\n",
      "Epoch 25/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 528ms/step - accuracy: 0.8597 - auc: 0.9398 - loss: 0.3201\n",
      "Epoch 26/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 510ms/step - accuracy: 0.8815 - auc: 0.9472 - loss: 0.2904\n",
      "Epoch 27/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 520ms/step - accuracy: 0.8483 - auc: 0.9173 - loss: 0.3721\n",
      "Epoch 28/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 531ms/step - accuracy: 0.8651 - auc: 0.9465 - loss: 0.3052\n",
      "Epoch 29/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 522ms/step - accuracy: 0.8851 - auc: 0.9572 - loss: 0.2738\n",
      "Epoch 30/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 510ms/step - accuracy: 0.8744 - auc: 0.9560 - loss: 0.2687\n",
      "Epoch 31/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 529ms/step - accuracy: 0.8940 - auc: 0.9543 - loss: 0.2754\n",
      "Epoch 32/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 517ms/step - accuracy: 0.9077 - auc: 0.9708 - loss: 0.2247\n",
      "Epoch 33/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 516ms/step - accuracy: 0.8672 - auc: 0.9506 - loss: 0.2798\n",
      "Epoch 34/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 513ms/step - accuracy: 0.8899 - auc: 0.9578 - loss: 0.2682\n",
      "Epoch 35/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 511ms/step - accuracy: 0.8666 - auc: 0.9528 - loss: 0.2731\n",
      "Epoch 36/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 517ms/step - accuracy: 0.8958 - auc: 0.9644 - loss: 0.2416\n",
      "Epoch 37/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 516ms/step - accuracy: 0.8916 - auc: 0.9682 - loss: 0.2297\n",
      "Epoch 38/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 536ms/step - accuracy: 0.9081 - auc: 0.9675 - loss: 0.2313\n",
      "Epoch 39/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 515ms/step - accuracy: 0.8942 - auc: 0.9597 - loss: 0.2552\n",
      "Epoch 40/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 513ms/step - accuracy: 0.9244 - auc: 0.9761 - loss: 0.1973\n",
      "Epoch 41/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 516ms/step - accuracy: 0.9025 - auc: 0.9735 - loss: 0.2061\n",
      "Epoch 42/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 512ms/step - accuracy: 0.9210 - auc: 0.9791 - loss: 0.1901\n",
      "Epoch 43/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 513ms/step - accuracy: 0.9167 - auc: 0.9756 - loss: 0.1949\n",
      "Epoch 44/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 534ms/step - accuracy: 0.9065 - auc: 0.9746 - loss: 0.2064\n",
      "Epoch 45/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 558ms/step - accuracy: 0.9113 - auc: 0.9717 - loss: 0.2117\n",
      "Epoch 46/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 517ms/step - accuracy: 0.9142 - auc: 0.9792 - loss: 0.1864\n",
      "Epoch 47/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 517ms/step - accuracy: 0.9217 - auc: 0.9811 - loss: 0.1707\n",
      "Epoch 48/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 515ms/step - accuracy: 0.9206 - auc: 0.9787 - loss: 0.1831\n",
      "Epoch 49/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 522ms/step - accuracy: 0.9278 - auc: 0.9819 - loss: 0.1728\n",
      "Epoch 50/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 533ms/step - accuracy: 0.9365 - auc: 0.9864 - loss: 0.1460\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Epoch 1/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 259ms/step - accuracy: 0.8033 - auc: 0.8675 - f1_score: 0.6860 - loss: 0.4052 - precision: 0.8033 - recall: 0.8033\n",
      "Epoch 2/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 267ms/step - accuracy: 0.9158 - auc: 0.9632 - f1_score: 0.8429 - loss: 0.1837 - precision: 0.9158 - recall: 0.9158\n",
      "Epoch 3/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 274ms/step - accuracy: 0.9380 - auc: 0.9740 - f1_score: 0.8785 - loss: 0.1609 - precision: 0.9380 - recall: 0.9380\n",
      "Epoch 4/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 265ms/step - accuracy: 0.9398 - auc: 0.9786 - f1_score: 0.8705 - loss: 0.1450 - precision: 0.9398 - recall: 0.9398\n",
      "Epoch 5/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 267ms/step - accuracy: 0.9317 - auc: 0.9782 - f1_score: 0.8611 - loss: 0.1490 - precision: 0.9317 - recall: 0.9317\n",
      "Epoch 6/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 259ms/step - accuracy: 0.9364 - auc: 0.9836 - f1_score: 0.8695 - loss: 0.1386 - precision: 0.9364 - recall: 0.9364\n",
      "Epoch 7/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 266ms/step - accuracy: 0.9434 - auc: 0.9817 - f1_score: 0.8833 - loss: 0.1417 - precision: 0.9434 - recall: 0.9434\n",
      "Epoch 8/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 270ms/step - accuracy: 0.9353 - auc: 0.9842 - f1_score: 0.8717 - loss: 0.1397 - precision: 0.9353 - recall: 0.9353\n",
      "Epoch 9/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 267ms/step - accuracy: 0.9422 - auc: 0.9856 - f1_score: 0.8817 - loss: 0.1393 - precision: 0.9422 - recall: 0.9422\n",
      "Epoch 10/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 274ms/step - accuracy: 0.9451 - auc: 0.9872 - f1_score: 0.8856 - loss: 0.1208 - precision: 0.9451 - recall: 0.9451\n",
      "Epoch 11/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 281ms/step - accuracy: 0.9365 - auc: 0.9840 - f1_score: 0.8732 - loss: 0.1405 - precision: 0.9365 - recall: 0.9365\n",
      "Epoch 12/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 285ms/step - accuracy: 0.9337 - auc: 0.9842 - f1_score: 0.8678 - loss: 0.1526 - precision: 0.9337 - recall: 0.9337\n",
      "Epoch 13/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 277ms/step - accuracy: 0.9439 - auc: 0.9861 - f1_score: 0.8872 - loss: 0.1311 - precision: 0.9439 - recall: 0.9439\n",
      "Epoch 14/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 273ms/step - accuracy: 0.9323 - auc: 0.9827 - f1_score: 0.8604 - loss: 0.1452 - precision: 0.9323 - recall: 0.9323\n",
      "Epoch 15/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 264ms/step - accuracy: 0.9350 - auc: 0.9865 - f1_score: 0.8673 - loss: 0.1386 - precision: 0.9350 - recall: 0.9350\n",
      "Epoch 16/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 265ms/step - accuracy: 0.9376 - auc: 0.9866 - f1_score: 0.8716 - loss: 0.1268 - precision: 0.9376 - recall: 0.9376\n",
      "Epoch 17/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 262ms/step - accuracy: 0.9437 - auc: 0.9887 - f1_score: 0.8863 - loss: 0.1287 - precision: 0.9437 - recall: 0.9437\n",
      "Epoch 18/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 266ms/step - accuracy: 0.9336 - auc: 0.9855 - f1_score: 0.8670 - loss: 0.1451 - precision: 0.9336 - recall: 0.9336\n",
      "Epoch 19/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 270ms/step - accuracy: 0.9383 - auc: 0.9875 - f1_score: 0.8767 - loss: 0.1297 - precision: 0.9383 - recall: 0.9383\n",
      "Epoch 20/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 268ms/step - accuracy: 0.9425 - auc: 0.9885 - f1_score: 0.8848 - loss: 0.1295 - precision: 0.9425 - recall: 0.9425\n",
      "Epoch 21/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9414 - auc: 0.9887 - f1_score: 0.8806 - loss: 0.1190 - precision: 0.9414 - recall: 0.9414\n",
      "Epoch 22/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.9407 - auc: 0.9870 - f1_score: 0.8781 - loss: 0.1228 - precision: 0.9407 - recall: 0.9407\n",
      "Epoch 23/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 271ms/step - accuracy: 0.9337 - auc: 0.9859 - f1_score: 0.8652 - loss: 0.1283 - precision: 0.9337 - recall: 0.9337\n",
      "Epoch 24/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 271ms/step - accuracy: 0.9422 - auc: 0.9867 - f1_score: 0.8800 - loss: 0.1423 - precision: 0.9422 - recall: 0.9422\n",
      "Epoch 25/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9442 - auc: 0.9885 - f1_score: 0.8905 - loss: 0.1302 - precision: 0.9442 - recall: 0.9442\n",
      "Epoch 26/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258ms/step - accuracy: 0.9433 - auc: 0.9891 - f1_score: 0.8828 - loss: 0.1214 - precision: 0.9433 - recall: 0.9433\n",
      "Epoch 27/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9346 - auc: 0.9873 - f1_score: 0.8697 - loss: 0.1466 - precision: 0.9346 - recall: 0.9346\n",
      "Epoch 28/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 264ms/step - accuracy: 0.9469 - auc: 0.9901 - f1_score: 0.8902 - loss: 0.1280 - precision: 0.9469 - recall: 0.9469\n",
      "Epoch 29/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258ms/step - accuracy: 0.9435 - auc: 0.9881 - f1_score: 0.8883 - loss: 0.1235 - precision: 0.9435 - recall: 0.9435\n",
      "Epoch 30/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9417 - auc: 0.9877 - f1_score: 0.8797 - loss: 0.1404 - precision: 0.9417 - recall: 0.9417\n",
      "Epoch 31/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9345 - auc: 0.9864 - f1_score: 0.8700 - loss: 0.1304 - precision: 0.9345 - recall: 0.9345\n",
      "Epoch 32/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 269ms/step - accuracy: 0.9376 - auc: 0.9872 - f1_score: 0.8729 - loss: 0.1271 - precision: 0.9376 - recall: 0.9376\n",
      "Epoch 33/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9366 - auc: 0.9877 - f1_score: 0.8705 - loss: 0.1263 - precision: 0.9366 - recall: 0.9366\n",
      "Epoch 34/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9287 - auc: 0.9854 - f1_score: 0.8641 - loss: 0.1472 - precision: 0.9287 - recall: 0.9287\n",
      "Epoch 35/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 262ms/step - accuracy: 0.9448 - auc: 0.9887 - f1_score: 0.8876 - loss: 0.1310 - precision: 0.9448 - recall: 0.9448\n",
      "Epoch 36/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 277ms/step - accuracy: 0.9370 - auc: 0.9862 - f1_score: 0.8722 - loss: 0.1264 - precision: 0.9370 - recall: 0.9370\n",
      "Epoch 37/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 265ms/step - accuracy: 0.9274 - auc: 0.9860 - f1_score: 0.8616 - loss: 0.1434 - precision: 0.9274 - recall: 0.9274\n",
      "Epoch 38/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 259ms/step - accuracy: 0.9407 - auc: 0.9863 - f1_score: 0.8770 - loss: 0.1380 - precision: 0.9407 - recall: 0.9407\n",
      "Epoch 39/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9391 - auc: 0.9871 - f1_score: 0.8739 - loss: 0.1343 - precision: 0.9391 - recall: 0.9391\n",
      "Epoch 40/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258ms/step - accuracy: 0.9351 - auc: 0.9858 - f1_score: 0.8634 - loss: 0.1364 - precision: 0.9351 - recall: 0.9351\n",
      "Epoch 41/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 265ms/step - accuracy: 0.9353 - auc: 0.9873 - f1_score: 0.8714 - loss: 0.1233 - precision: 0.9353 - recall: 0.9353\n",
      "Epoch 42/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 259ms/step - accuracy: 0.9362 - auc: 0.9874 - f1_score: 0.8650 - loss: 0.1207 - precision: 0.9362 - recall: 0.9362\n",
      "Epoch 43/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9386 - auc: 0.9867 - f1_score: 0.8755 - loss: 0.1268 - precision: 0.9386 - recall: 0.9386\n",
      "Epoch 44/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258ms/step - accuracy: 0.9381 - auc: 0.9875 - f1_score: 0.8791 - loss: 0.1401 - precision: 0.9381 - recall: 0.9381\n",
      "Epoch 45/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 262ms/step - accuracy: 0.9365 - auc: 0.9874 - f1_score: 0.8705 - loss: 0.1368 - precision: 0.9365 - recall: 0.9365\n",
      "Epoch 46/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258ms/step - accuracy: 0.9407 - auc: 0.9890 - f1_score: 0.8788 - loss: 0.1371 - precision: 0.9407 - recall: 0.9407\n",
      "Epoch 47/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.9422 - auc: 0.9886 - f1_score: 0.8776 - loss: 0.1215 - precision: 0.9422 - recall: 0.9422\n",
      "Epoch 48/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 271ms/step - accuracy: 0.9353 - auc: 0.9858 - f1_score: 0.8712 - loss: 0.1422 - precision: 0.9353 - recall: 0.9353\n",
      "Epoch 49/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 275ms/step - accuracy: 0.9461 - auc: 0.9903 - f1_score: 0.8910 - loss: 0.1273 - precision: 0.9461 - recall: 0.9461\n",
      "Epoch 50/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 262ms/step - accuracy: 0.9374 - auc: 0.9879 - f1_score: 0.8693 - loss: 0.1226 - precision: 0.9374 - recall: 0.9374\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.9708 - auc: 0.9963 - f1_score: 0.9318 - loss: 0.0754 - precision: 0.9708 - recall: 0.9708\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8923 - auc: 0.9561 - f1_score: 0.7338 - loss: 0.3609 - precision: 0.8923 - recall: 0.8923\n",
      "\n",
      "=== Phase 1: Test Evaluation Results ===\n",
      "  loss  accuracy  precision  recall    auc  f1_score\n",
      "0.5306    0.8076     0.8076  0.8076 0.9026    0.6595\n",
      "\n",
      "=== Phase 3: Train Evaluation Results ===\n",
      "  loss  accuracy  precision  recall    auc  f1_score\n",
      "0.0743    0.9710     0.9710  0.9710 0.9966    0.9325\n",
      "\n",
      "=== Phase 3: Test Evaluation Results ===\n",
      "  loss  accuracy  precision  recall    auc  f1_score\n",
      "0.3646    0.8969     0.8969  0.8969 0.9550    0.7452\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step\n",
      "Processing target column: SR-p53\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_feature\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"model_feature\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_176 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_177 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_178 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_176 (\u001b[38;5;33mConv1D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m9,152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_177 (\u001b[38;5;33mConv1D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_178 (\u001b[38;5;33mConv1D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_112\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_112\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_112 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_448 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_64          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_192 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_449 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_65          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_193 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_450 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_451 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_112 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_448 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_64          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_192 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_449 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_65          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_193 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_450 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_451 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">217,026</span> (847.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m217,026\u001b[0m (847.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">215,490</span> (841.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m215,490\u001b[0m (841.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"interactionModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"interactionModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ model_feature (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_112 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">217,026</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_48     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ model_feature (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m58,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_112 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │       \u001b[38;5;34m217,026\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_48     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">275,522</span> (1.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m275,522\u001b[0m (1.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">273,986</span> (1.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m273,986\u001b[0m (1.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_64', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_65', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_115\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_115\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_113     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ input_layer_113[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ input_layer_113[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_32      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ difference (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ difference[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_113     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_64 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ input_layer_113[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_65 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ input_layer_113[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_64 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_65 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_65[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_32      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_65[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate_32[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ difference (\u001b[38;5;33mLambda\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ difference[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">297,216</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m297,216\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">297,216</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m297,216\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_66', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_67', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_118\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_118\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_116     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ model_feature       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> │ input_layer_116[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,608</span> │ model_feature[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_33      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_472 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_472[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_202         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_473 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_202[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_473[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_203         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_474 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ dropout_203[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_475 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │ dense_474[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_116     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ model_feature       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m58,496\u001b[0m │ input_layer_116[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_66 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ model_feature[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block_67 (\u001b[38;5;33mBlock\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m148,608\u001b[0m │ model_feature[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_66 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_67 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_33      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate_33[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_472 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m1,536\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ dense_472[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_202         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_473 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_202[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_473[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_203         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_474 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m16,448\u001b[0m │ dropout_203[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_475 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m130\u001b[0m │ dense_474[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">508,226</span> (1.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m508,226\u001b[0m (1.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,978</span> (589.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,978\u001b[0m (589.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">357,248</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m357,248\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 355ms/step - accuracy: 0.9141 - auc: 0.9260 - f1_score: 0.5034 - loss: 0.8910 - precision: 0.9141 - recall: 0.9141\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 348ms/step - accuracy: 0.7110 - auc: 0.7208 - f1_score: 0.5018 - loss: 0.6395 - precision: 0.7110 - recall: 0.7110\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 345ms/step - accuracy: 0.7207 - auc: 0.7576 - f1_score: 0.4995 - loss: 0.6149 - precision: 0.7207 - recall: 0.7207\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 345ms/step - accuracy: 0.7246 - auc: 0.7612 - f1_score: 0.5250 - loss: 0.6393 - precision: 0.7246 - recall: 0.7246\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 343ms/step - accuracy: 0.7070 - auc: 0.7523 - f1_score: 0.5093 - loss: 0.6407 - precision: 0.7070 - recall: 0.7070\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 363ms/step - accuracy: 0.7513 - auc: 0.7998 - f1_score: 0.5229 - loss: 0.6254 - precision: 0.7513 - recall: 0.7513\n",
      "Epoch 7/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 354ms/step - accuracy: 0.7155 - auc: 0.7632 - f1_score: 0.5185 - loss: 0.6418 - precision: 0.7155 - recall: 0.7155\n",
      "Epoch 8/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 345ms/step - accuracy: 0.7442 - auc: 0.8050 - f1_score: 0.5340 - loss: 0.6508 - precision: 0.7442 - recall: 0.7442\n",
      "Epoch 9/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 350ms/step - accuracy: 0.7708 - auc: 0.8246 - f1_score: 0.5517 - loss: 0.6253 - precision: 0.7708 - recall: 0.7708\n",
      "Epoch 10/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 345ms/step - accuracy: 0.7381 - auc: 0.7906 - f1_score: 0.5268 - loss: 0.6137 - precision: 0.7381 - recall: 0.7381\n",
      "Epoch 11/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 345ms/step - accuracy: 0.7474 - auc: 0.8098 - f1_score: 0.5291 - loss: 0.6123 - precision: 0.7474 - recall: 0.7474\n",
      "Epoch 12/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 348ms/step - accuracy: 0.6593 - auc: 0.7133 - f1_score: 0.4901 - loss: 0.6272 - precision: 0.6593 - recall: 0.6593\n",
      "Epoch 13/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 344ms/step - accuracy: 0.7889 - auc: 0.8546 - f1_score: 0.5510 - loss: 0.6105 - precision: 0.7889 - recall: 0.7889\n",
      "Epoch 14/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 351ms/step - accuracy: 0.7240 - auc: 0.7630 - f1_score: 0.5299 - loss: 0.6301 - precision: 0.7240 - recall: 0.7240\n",
      "Epoch 15/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 371ms/step - accuracy: 0.7694 - auc: 0.8405 - f1_score: 0.5526 - loss: 0.6064 - precision: 0.7694 - recall: 0.7694\n",
      "Epoch 16/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 350ms/step - accuracy: 0.7611 - auc: 0.8211 - f1_score: 0.5472 - loss: 0.6040 - precision: 0.7611 - recall: 0.7611\n",
      "Epoch 17/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 362ms/step - accuracy: 0.7578 - auc: 0.8212 - f1_score: 0.5520 - loss: 0.6032 - precision: 0.7578 - recall: 0.7578\n",
      "Epoch 18/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 351ms/step - accuracy: 0.7251 - auc: 0.7751 - f1_score: 0.5307 - loss: 0.6353 - precision: 0.7251 - recall: 0.7251\n",
      "Epoch 19/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 351ms/step - accuracy: 0.7684 - auc: 0.8247 - f1_score: 0.5472 - loss: 0.6077 - precision: 0.7684 - recall: 0.7684\n",
      "Epoch 20/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 354ms/step - accuracy: 0.7100 - auc: 0.7755 - f1_score: 0.5256 - loss: 0.6184 - precision: 0.7100 - recall: 0.7100\n",
      "Epoch 21/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 351ms/step - accuracy: 0.7444 - auc: 0.7946 - f1_score: 0.5410 - loss: 0.6051 - precision: 0.7444 - recall: 0.7444\n",
      "Epoch 22/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 357ms/step - accuracy: 0.7734 - auc: 0.8352 - f1_score: 0.5492 - loss: 0.6027 - precision: 0.7734 - recall: 0.7734\n",
      "Epoch 23/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 360ms/step - accuracy: 0.7596 - auc: 0.8092 - f1_score: 0.5551 - loss: 0.6105 - precision: 0.7596 - recall: 0.7596\n",
      "Epoch 24/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 344ms/step - accuracy: 0.7733 - auc: 0.8337 - f1_score: 0.5570 - loss: 0.6228 - precision: 0.7733 - recall: 0.7733\n",
      "Epoch 25/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 325ms/step - accuracy: 0.8006 - auc: 0.8694 - f1_score: 0.5579 - loss: 0.5758 - precision: 0.8006 - recall: 0.8006\n",
      "Epoch 26/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 327ms/step - accuracy: 0.7472 - auc: 0.8014 - f1_score: 0.5422 - loss: 0.6067 - precision: 0.7472 - recall: 0.7472\n",
      "Epoch 27/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 329ms/step - accuracy: 0.7754 - auc: 0.8438 - f1_score: 0.5546 - loss: 0.6137 - precision: 0.7754 - recall: 0.7754\n",
      "Epoch 28/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 325ms/step - accuracy: 0.7430 - auc: 0.8153 - f1_score: 0.5494 - loss: 0.6197 - precision: 0.7430 - recall: 0.7430\n",
      "Epoch 29/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 350ms/step - accuracy: 0.7942 - auc: 0.8515 - f1_score: 0.5539 - loss: 0.5941 - precision: 0.7942 - recall: 0.7942\n",
      "Epoch 30/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 343ms/step - accuracy: 0.7771 - auc: 0.8303 - f1_score: 0.5604 - loss: 0.5838 - precision: 0.7771 - recall: 0.7771\n",
      "Epoch 31/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 342ms/step - accuracy: 0.7476 - auc: 0.8053 - f1_score: 0.5515 - loss: 0.5978 - precision: 0.7476 - recall: 0.7476\n",
      "Epoch 32/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 348ms/step - accuracy: 0.7440 - auc: 0.8071 - f1_score: 0.5459 - loss: 0.6298 - precision: 0.7440 - recall: 0.7440\n",
      "Epoch 33/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 362ms/step - accuracy: 0.8047 - auc: 0.8756 - f1_score: 0.5749 - loss: 0.5959 - precision: 0.8047 - recall: 0.8047\n",
      "Epoch 34/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 326ms/step - accuracy: 0.7521 - auc: 0.8065 - f1_score: 0.5560 - loss: 0.6155 - precision: 0.7521 - recall: 0.7521\n",
      "Epoch 35/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 326ms/step - accuracy: 0.7939 - auc: 0.8639 - f1_score: 0.5833 - loss: 0.6092 - precision: 0.7939 - recall: 0.7939\n",
      "Epoch 36/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 332ms/step - accuracy: 0.7924 - auc: 0.8486 - f1_score: 0.5776 - loss: 0.6162 - precision: 0.7924 - recall: 0.7924\n",
      "Epoch 37/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 338ms/step - accuracy: 0.8007 - auc: 0.8678 - f1_score: 0.5698 - loss: 0.5882 - precision: 0.8007 - recall: 0.8007\n",
      "Epoch 38/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 329ms/step - accuracy: 0.7333 - auc: 0.8005 - f1_score: 0.5442 - loss: 0.5923 - precision: 0.7333 - recall: 0.7333\n",
      "Epoch 39/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 333ms/step - accuracy: 0.7367 - auc: 0.8063 - f1_score: 0.5449 - loss: 0.6157 - precision: 0.7367 - recall: 0.7367\n",
      "Epoch 40/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 346ms/step - accuracy: 0.8068 - auc: 0.8736 - f1_score: 0.5886 - loss: 0.5880 - precision: 0.8068 - recall: 0.8068\n",
      "Epoch 41/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 357ms/step - accuracy: 0.7410 - auc: 0.8055 - f1_score: 0.5481 - loss: 0.6157 - precision: 0.7410 - recall: 0.7410\n",
      "Epoch 42/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 350ms/step - accuracy: 0.7785 - auc: 0.8491 - f1_score: 0.5754 - loss: 0.6019 - precision: 0.7785 - recall: 0.7785\n",
      "Epoch 43/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 330ms/step - accuracy: 0.7738 - auc: 0.8499 - f1_score: 0.5685 - loss: 0.6256 - precision: 0.7738 - recall: 0.7738\n",
      "Epoch 44/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 325ms/step - accuracy: 0.7939 - auc: 0.8652 - f1_score: 0.5761 - loss: 0.6101 - precision: 0.7939 - recall: 0.7939\n",
      "Epoch 45/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 324ms/step - accuracy: 0.7857 - auc: 0.8538 - f1_score: 0.5695 - loss: 0.5955 - precision: 0.7857 - recall: 0.7857\n",
      "Epoch 46/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 353ms/step - accuracy: 0.7519 - auc: 0.8109 - f1_score: 0.5496 - loss: 0.5930 - precision: 0.7519 - recall: 0.7519\n",
      "Epoch 47/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 342ms/step - accuracy: 0.7781 - auc: 0.8564 - f1_score: 0.5675 - loss: 0.6127 - precision: 0.7781 - recall: 0.7781\n",
      "Epoch 48/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 336ms/step - accuracy: 0.8137 - auc: 0.8798 - f1_score: 0.5870 - loss: 0.5906 - precision: 0.8137 - recall: 0.8137\n",
      "Epoch 49/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 342ms/step - accuracy: 0.7423 - auc: 0.8094 - f1_score: 0.5515 - loss: 0.6021 - precision: 0.7423 - recall: 0.7423\n",
      "Epoch 50/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 329ms/step - accuracy: 0.8224 - auc: 0.8925 - f1_score: 0.5935 - loss: 0.5779 - precision: 0.8224 - recall: 0.8224\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8048 - auc: 0.8580 - f1_score: 0.5716 - loss: 0.5207 - precision: 0.8048 - recall: 0.8048\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 484ms/step - accuracy: 0.6022 - auc: 0.5588 - loss: 1.0011\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 474ms/step - accuracy: 0.6604 - auc: 0.6529 - loss: 0.6535\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 472ms/step - accuracy: 0.6723 - auc: 0.6945 - loss: 0.6496\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 471ms/step - accuracy: 0.7200 - auc: 0.7132 - loss: 0.6376\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 475ms/step - accuracy: 0.5839 - auc: 0.7092 - loss: 0.6558\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 489ms/step - accuracy: 0.6721 - auc: 0.7226 - loss: 0.6255\n",
      "Epoch 7/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 486ms/step - accuracy: 0.7340 - auc: 0.7440 - loss: 0.5993\n",
      "Epoch 8/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 471ms/step - accuracy: 0.7181 - auc: 0.7352 - loss: 0.6127\n",
      "Epoch 9/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 473ms/step - accuracy: 0.6757 - auc: 0.7297 - loss: 0.6361\n",
      "Epoch 10/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 473ms/step - accuracy: 0.6932 - auc: 0.7720 - loss: 0.5929\n",
      "Epoch 11/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 480ms/step - accuracy: 0.6769 - auc: 0.7740 - loss: 0.5833\n",
      "Epoch 12/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 481ms/step - accuracy: 0.6993 - auc: 0.7780 - loss: 0.5797\n",
      "Epoch 13/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 492ms/step - accuracy: 0.7324 - auc: 0.7973 - loss: 0.5468\n",
      "Epoch 14/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 479ms/step - accuracy: 0.6812 - auc: 0.7665 - loss: 0.5998\n",
      "Epoch 15/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 478ms/step - accuracy: 0.7861 - auc: 0.7828 - loss: 0.5818\n",
      "Epoch 16/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 476ms/step - accuracy: 0.7416 - auc: 0.8200 - loss: 0.5246\n",
      "Epoch 17/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 478ms/step - accuracy: 0.7960 - auc: 0.8403 - loss: 0.5083\n",
      "Epoch 18/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 478ms/step - accuracy: 0.7615 - auc: 0.8407 - loss: 0.5143\n",
      "Epoch 19/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 486ms/step - accuracy: 0.8216 - auc: 0.8485 - loss: 0.4978\n",
      "Epoch 20/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 491ms/step - accuracy: 0.7993 - auc: 0.8702 - loss: 0.4667\n",
      "Epoch 21/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 486ms/step - accuracy: 0.8085 - auc: 0.8794 - loss: 0.4434\n",
      "Epoch 22/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 482ms/step - accuracy: 0.7896 - auc: 0.8730 - loss: 0.4413\n",
      "Epoch 23/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 476ms/step - accuracy: 0.8034 - auc: 0.8791 - loss: 0.4478\n",
      "Epoch 24/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 483ms/step - accuracy: 0.8234 - auc: 0.8895 - loss: 0.4148\n",
      "Epoch 25/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 484ms/step - accuracy: 0.8568 - auc: 0.8963 - loss: 0.4053\n",
      "Epoch 26/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 507ms/step - accuracy: 0.8612 - auc: 0.9334 - loss: 0.3575\n",
      "Epoch 27/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 482ms/step - accuracy: 0.8600 - auc: 0.9237 - loss: 0.3632\n",
      "Epoch 28/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 482ms/step - accuracy: 0.8707 - auc: 0.9402 - loss: 0.3213\n",
      "Epoch 29/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 477ms/step - accuracy: 0.8629 - auc: 0.9231 - loss: 0.3652\n",
      "Epoch 30/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 479ms/step - accuracy: 0.8817 - auc: 0.9330 - loss: 0.3418\n",
      "Epoch 31/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 479ms/step - accuracy: 0.8741 - auc: 0.9418 - loss: 0.3190\n",
      "Epoch 32/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 488ms/step - accuracy: 0.8093 - auc: 0.9096 - loss: 0.3914\n",
      "Epoch 33/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 500ms/step - accuracy: 0.8675 - auc: 0.9502 - loss: 0.2985\n",
      "Epoch 34/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 480ms/step - accuracy: 0.7950 - auc: 0.8302 - loss: 0.6268\n",
      "Epoch 35/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 479ms/step - accuracy: 0.8351 - auc: 0.9022 - loss: 0.4182\n",
      "Epoch 36/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 478ms/step - accuracy: 0.8809 - auc: 0.9454 - loss: 0.3129\n",
      "Epoch 37/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 491ms/step - accuracy: 0.8674 - auc: 0.9537 - loss: 0.2839\n",
      "Epoch 38/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 482ms/step - accuracy: 0.9102 - auc: 0.9703 - loss: 0.2353\n",
      "Epoch 39/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 499ms/step - accuracy: 0.9023 - auc: 0.9681 - loss: 0.2381\n",
      "Epoch 40/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 479ms/step - accuracy: 0.8928 - auc: 0.9642 - loss: 0.2490\n",
      "Epoch 41/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 479ms/step - accuracy: 0.9064 - auc: 0.9722 - loss: 0.2244\n",
      "Epoch 42/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 478ms/step - accuracy: 0.9154 - auc: 0.9726 - loss: 0.2103\n",
      "Epoch 43/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 486ms/step - accuracy: 0.9320 - auc: 0.9863 - loss: 0.1673\n",
      "Epoch 44/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 507ms/step - accuracy: 0.9265 - auc: 0.9845 - loss: 0.1739\n",
      "Epoch 45/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 530ms/step - accuracy: 0.9394 - auc: 0.9834 - loss: 0.1696\n",
      "Epoch 46/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 531ms/step - accuracy: 0.9356 - auc: 0.9838 - loss: 0.1672\n",
      "Epoch 47/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 522ms/step - accuracy: 0.9229 - auc: 0.9764 - loss: 0.1976\n",
      "Epoch 48/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 532ms/step - accuracy: 0.9231 - auc: 0.9729 - loss: 0.2110\n",
      "Epoch 49/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 520ms/step - accuracy: 0.8758 - auc: 0.9625 - loss: 0.2496\n",
      "Epoch 50/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 515ms/step - accuracy: 0.8879 - auc: 0.9740 - loss: 0.2225\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Epoch 1/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 259ms/step - accuracy: 0.8125 - auc: 0.8604 - f1_score: 0.6125 - loss: 0.3673 - precision: 0.8125 - recall: 0.8125\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 280ms/step - accuracy: 0.9193 - auc: 0.9599 - f1_score: 0.7710 - loss: 0.1994 - precision: 0.9193 - recall: 0.9193\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 266ms/step - accuracy: 0.9212 - auc: 0.9649 - f1_score: 0.7755 - loss: 0.1903 - precision: 0.9212 - recall: 0.9212\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 258ms/step - accuracy: 0.9265 - auc: 0.9739 - f1_score: 0.7879 - loss: 0.1816 - precision: 0.9265 - recall: 0.9265\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 261ms/step - accuracy: 0.9263 - auc: 0.9728 - f1_score: 0.7869 - loss: 0.1755 - precision: 0.9263 - recall: 0.9263\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 262ms/step - accuracy: 0.9251 - auc: 0.9744 - f1_score: 0.7866 - loss: 0.1676 - precision: 0.9251 - recall: 0.9251\n",
      "Epoch 7/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 253ms/step - accuracy: 0.9369 - auc: 0.9792 - f1_score: 0.8084 - loss: 0.1503 - precision: 0.9369 - recall: 0.9369\n",
      "Epoch 8/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 239ms/step - accuracy: 0.9204 - auc: 0.9737 - f1_score: 0.7733 - loss: 0.1888 - precision: 0.9204 - recall: 0.9204\n",
      "Epoch 9/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 238ms/step - accuracy: 0.9274 - auc: 0.9792 - f1_score: 0.7991 - loss: 0.1580 - precision: 0.9274 - recall: 0.9274\n",
      "Epoch 10/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 242ms/step - accuracy: 0.9381 - auc: 0.9812 - f1_score: 0.8162 - loss: 0.1480 - precision: 0.9381 - recall: 0.9381\n",
      "Epoch 11/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 253ms/step - accuracy: 0.9244 - auc: 0.9774 - f1_score: 0.7753 - loss: 0.1611 - precision: 0.9244 - recall: 0.9244\n",
      "Epoch 12/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 266ms/step - accuracy: 0.9246 - auc: 0.9804 - f1_score: 0.7872 - loss: 0.1598 - precision: 0.9246 - recall: 0.9246\n",
      "Epoch 13/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 245ms/step - accuracy: 0.9269 - auc: 0.9791 - f1_score: 0.7776 - loss: 0.1597 - precision: 0.9269 - recall: 0.9269\n",
      "Epoch 14/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 256ms/step - accuracy: 0.9097 - auc: 0.9717 - f1_score: 0.7538 - loss: 0.1593 - precision: 0.9097 - recall: 0.9097\n",
      "Epoch 15/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 249ms/step - accuracy: 0.9237 - auc: 0.9803 - f1_score: 0.7924 - loss: 0.1494 - precision: 0.9237 - recall: 0.9237\n",
      "Epoch 16/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 244ms/step - accuracy: 0.9354 - auc: 0.9854 - f1_score: 0.8046 - loss: 0.1323 - precision: 0.9354 - recall: 0.9354\n",
      "Epoch 17/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 238ms/step - accuracy: 0.9191 - auc: 0.9791 - f1_score: 0.7756 - loss: 0.1545 - precision: 0.9191 - recall: 0.9191\n",
      "Epoch 18/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 240ms/step - accuracy: 0.9260 - auc: 0.9818 - f1_score: 0.7854 - loss: 0.1521 - precision: 0.9260 - recall: 0.9260\n",
      "Epoch 19/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 237ms/step - accuracy: 0.9272 - auc: 0.9816 - f1_score: 0.7819 - loss: 0.1408 - precision: 0.9272 - recall: 0.9272\n",
      "Epoch 20/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 238ms/step - accuracy: 0.9262 - auc: 0.9800 - f1_score: 0.7823 - loss: 0.1496 - precision: 0.9262 - recall: 0.9262\n",
      "Epoch 21/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 239ms/step - accuracy: 0.9246 - auc: 0.9806 - f1_score: 0.7886 - loss: 0.1510 - precision: 0.9246 - recall: 0.9246\n",
      "Epoch 22/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 237ms/step - accuracy: 0.9328 - auc: 0.9851 - f1_score: 0.8066 - loss: 0.1430 - precision: 0.9328 - recall: 0.9328\n",
      "Epoch 23/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 238ms/step - accuracy: 0.9310 - auc: 0.9847 - f1_score: 0.8003 - loss: 0.1538 - precision: 0.9310 - recall: 0.9310\n",
      "Epoch 24/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 238ms/step - accuracy: 0.9221 - auc: 0.9809 - f1_score: 0.7736 - loss: 0.1434 - precision: 0.9221 - recall: 0.9221\n",
      "Epoch 25/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 242ms/step - accuracy: 0.9283 - auc: 0.9828 - f1_score: 0.8026 - loss: 0.1402 - precision: 0.9283 - recall: 0.9283\n",
      "Epoch 26/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 247ms/step - accuracy: 0.9338 - auc: 0.9863 - f1_score: 0.8019 - loss: 0.1485 - precision: 0.9338 - recall: 0.9338\n",
      "Epoch 27/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 260ms/step - accuracy: 0.9217 - auc: 0.9812 - f1_score: 0.7776 - loss: 0.1601 - precision: 0.9217 - recall: 0.9217\n",
      "Epoch 28/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 238ms/step - accuracy: 0.9251 - auc: 0.9821 - f1_score: 0.7912 - loss: 0.1799 - precision: 0.9251 - recall: 0.9251\n",
      "Epoch 29/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 243ms/step - accuracy: 0.9350 - auc: 0.9854 - f1_score: 0.8146 - loss: 0.1549 - precision: 0.9350 - recall: 0.9350\n",
      "Epoch 30/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 237ms/step - accuracy: 0.9197 - auc: 0.9794 - f1_score: 0.7637 - loss: 0.1756 - precision: 0.9197 - recall: 0.9197\n",
      "Epoch 31/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 238ms/step - accuracy: 0.9274 - auc: 0.9822 - f1_score: 0.7864 - loss: 0.1376 - precision: 0.9274 - recall: 0.9274\n",
      "Epoch 32/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 236ms/step - accuracy: 0.9276 - auc: 0.9822 - f1_score: 0.7924 - loss: 0.1611 - precision: 0.9276 - recall: 0.9276\n",
      "Epoch 33/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 239ms/step - accuracy: 0.9239 - auc: 0.9821 - f1_score: 0.7902 - loss: 0.1496 - precision: 0.9239 - recall: 0.9239\n",
      "Epoch 34/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 243ms/step - accuracy: 0.9243 - auc: 0.9818 - f1_score: 0.7763 - loss: 0.1582 - precision: 0.9243 - recall: 0.9243\n",
      "Epoch 35/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 237ms/step - accuracy: 0.9217 - auc: 0.9813 - f1_score: 0.7791 - loss: 0.1454 - precision: 0.9217 - recall: 0.9217\n",
      "Epoch 36/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 238ms/step - accuracy: 0.9265 - auc: 0.9821 - f1_score: 0.7900 - loss: 0.1599 - precision: 0.9265 - recall: 0.9265\n",
      "Epoch 37/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 237ms/step - accuracy: 0.9366 - auc: 0.9857 - f1_score: 0.8018 - loss: 0.1412 - precision: 0.9366 - recall: 0.9366\n",
      "Epoch 38/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 243ms/step - accuracy: 0.9224 - auc: 0.9809 - f1_score: 0.7864 - loss: 0.1437 - precision: 0.9224 - recall: 0.9224\n",
      "Epoch 39/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 251ms/step - accuracy: 0.9349 - auc: 0.9857 - f1_score: 0.8013 - loss: 0.1468 - precision: 0.9349 - recall: 0.9349\n",
      "Epoch 40/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 250ms/step - accuracy: 0.9302 - auc: 0.9855 - f1_score: 0.8029 - loss: 0.1328 - precision: 0.9302 - recall: 0.9302\n",
      "Epoch 41/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 239ms/step - accuracy: 0.9375 - auc: 0.9878 - f1_score: 0.8212 - loss: 0.1377 - precision: 0.9375 - recall: 0.9375\n",
      "Epoch 42/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 241ms/step - accuracy: 0.9308 - auc: 0.9860 - f1_score: 0.8003 - loss: 0.1561 - precision: 0.9308 - recall: 0.9308\n",
      "Epoch 43/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 238ms/step - accuracy: 0.9210 - auc: 0.9808 - f1_score: 0.7772 - loss: 0.1446 - precision: 0.9210 - recall: 0.9210\n",
      "Epoch 44/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 240ms/step - accuracy: 0.9268 - auc: 0.9849 - f1_score: 0.7930 - loss: 0.1329 - precision: 0.9268 - recall: 0.9268\n",
      "Epoch 45/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 239ms/step - accuracy: 0.9326 - auc: 0.9843 - f1_score: 0.7955 - loss: 0.1512 - precision: 0.9326 - recall: 0.9326\n",
      "Epoch 46/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 246ms/step - accuracy: 0.9299 - auc: 0.9857 - f1_score: 0.7943 - loss: 0.1468 - precision: 0.9299 - recall: 0.9299\n",
      "Epoch 47/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 244ms/step - accuracy: 0.9240 - auc: 0.9835 - f1_score: 0.7895 - loss: 0.1496 - precision: 0.9240 - recall: 0.9240\n",
      "Epoch 48/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 239ms/step - accuracy: 0.9269 - auc: 0.9852 - f1_score: 0.7903 - loss: 0.1512 - precision: 0.9269 - recall: 0.9269\n",
      "Epoch 49/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 237ms/step - accuracy: 0.9297 - auc: 0.9864 - f1_score: 0.7967 - loss: 0.1441 - precision: 0.9297 - recall: 0.9297\n",
      "Epoch 50/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 244ms/step - accuracy: 0.9286 - auc: 0.9859 - f1_score: 0.7932 - loss: 0.1409 - precision: 0.9286 - recall: 0.9286\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9723 - auc: 0.9966 - f1_score: 0.8915 - loss: 0.0727 - precision: 0.9723 - recall: 0.9723\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9065 - auc: 0.9529 - f1_score: 0.6380 - loss: 0.4100 - precision: 0.9065 - recall: 0.9065\n",
      "\n",
      "=== Phase 1: Test Evaluation Results ===\n",
      "  loss  accuracy  precision  recall    auc  f1_score\n",
      "0.5192    0.8022     0.8022  0.8022 0.8613    0.5560\n",
      "\n",
      "=== Phase 3: Train Evaluation Results ===\n",
      "  loss  accuracy  precision  recall    auc  f1_score\n",
      "0.0720    0.9723     0.9723  0.9723 0.9967    0.8915\n",
      "\n",
      "=== Phase 3: Test Evaluation Results ===\n",
      "  loss  accuracy  precision  recall    auc  f1_score\n",
      "0.3289    0.9166     0.9166  0.9166 0.9637    0.6549\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step\n",
      "      target_col  train_loss  train_accuracy  test_loss  test_accuracy  \\\n",
      "0  NR-PPAR-gamma    0.036197        0.987984   0.152075       0.961240   \n",
      "1         NR-AhR    0.074251        0.970987   0.364621       0.896947   \n",
      "2         SR-p53    0.071956        0.972320   0.328897       0.916605   \n",
      "\n",
      "         confusion_matrix                        confusion_matrix_normalized  \n",
      "0  [[1226, 27], [23, 14]]  [[0.9784517158818835, 0.02154828411811652], [0...  \n",
      "1  [[1093, 63], [72, 82]]  [[0.9455017301038062, 0.054498269896193774], [...  \n",
      "2  [[1211, 59], [54, 31]]  [[0.9535433070866142, 0.046456692913385826], [...  \n"
     ]
    }
   ],
   "source": [
    "# Target columns to process\n",
    "target_cols = ['NR-PPAR-gamma', 'NR-AhR', 'SR-p53']\n",
    "dataset_path = '../Data/tox21.csv'\n",
    "smiles_col = 'smiles'\n",
    "n_epochs = 50\n",
    "\n",
    "# Call the function\n",
    "results_df, models_dict = process_multiple_targets(dataset_path, target_cols, smiles_col, n_epochs)\n",
    "\n",
    "# Display the results dataframe\n",
    "print(results_df)\n",
    "\n",
    "# Access a specific model by target column\n",
    "model_ppar_gamma = models_dict['NR-PPAR-gamma']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c395b",
   "metadata": {},
   "source": [
    "# Predict on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4130be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def predict_with_models(models_dict, X_test_dict, y_test_cat_dict):\n",
    "    \"\"\"\n",
    "    Use trained models to predict on test data and calculate evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        models_dict (dict): Dictionary containing trained models for each target column.\n",
    "        X_test_dict (dict): Dictionary containing test features for each target column.\n",
    "        y_test_cat_dict (dict): Dictionary containing test labels (one-hot encoded) for each target column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with prediction results and evaluation metrics for each target column.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store results for each target column\n",
    "    prediction_results = []\n",
    "\n",
    "    for target_col, model in models_dict.items():\n",
    "        print(f\"Predicting for target column: {target_col}\")\n",
    "\n",
    "        # Get the corresponding test data for this target column\n",
    "        X_test = X_test_dict[target_col]\n",
    "        y_test_cat = y_test_cat_dict[target_col]\n",
    "\n",
    "        # Convert one-hot encoded labels to single-label format\n",
    "        y_test = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred_proba = model.predict(X_test)  # Predicted probabilities\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)  # Predicted class labels\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba[:, 1]) if y_pred_proba.shape[1] > 1 else None\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Append results for this target column\n",
    "        prediction_results.append({\n",
    "            'target_col': target_col,\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc,\n",
    "            'confusion_matrix': cm.tolist()\n",
    "        })\n",
    "\n",
    "        print(f\"Accuracy for {target_col}: {accuracy:.4f}\")\n",
    "        if auc is not None:\n",
    "            print(f\"AUC for {target_col}: {auc:.4f}\")\n",
    "        print(f\"Confusion Matrix for {target_col}:\\n{cm}\\n\")\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    predictions_df = pd.DataFrame(prediction_results)\n",
    "    return predictions_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cced5e",
   "metadata": {},
   "source": [
    "## Prep SMILES data for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b075cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load subset of tox21 dataset for predictions\n",
    "new_data = pd.read_csv('../Data/tox21.csv').dropna(subset=['NR-PPAR-gamma','NR-AhR','SR-p53','smiles']).head(100)\n",
    "smiles_list = new_data['smiles']\n",
    "\n",
    "X_new = np.array([label_smiles(str(s), 100, smiles_dict) for s in smiles_list])\n",
    "X_new = to_categorical(X_new, num_classes=71)\n",
    "\n",
    "target_cols = ['NR-PPAR-gamma','NR-AhR','SR-p53']\n",
    "X_test_dict = {t: X_new for t in target_cols}\n",
    "y_test_cat_dict = {t: to_categorical(new_data[t].astype(int), num_classes=2) for t in target_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494e16d",
   "metadata": {},
   "source": [
    "# Run Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "07c98325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for target column: NR-PPAR-gamma\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Accuracy for NR-PPAR-gamma: 0.9800\n",
      "AUC for NR-PPAR-gamma: 0.4949\n",
      "Confusion Matrix for NR-PPAR-gamma:\n",
      "[[98  1]\n",
      " [ 1  0]]\n",
      "\n",
      "Predicting for target column: NR-AhR\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Accuracy for NR-AhR: 0.9800\n",
      "AUC for NR-AhR: 0.9991\n",
      "Confusion Matrix for NR-AhR:\n",
      "[[87  1]\n",
      " [ 1 11]]\n",
      "\n",
      "Predicting for target column: SR-p53\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Accuracy for SR-p53: 1.0000\n",
      "AUC for SR-p53: 1.0000\n",
      "Confusion Matrix for SR-p53:\n",
      "[[97  0]\n",
      " [ 0  3]]\n",
      "\n",
      "      target_col  accuracy       auc    confusion_matrix\n",
      "0  NR-PPAR-gamma      0.98  0.494949   [[98, 1], [1, 0]]\n",
      "1         NR-AhR      0.98  0.999053  [[87, 1], [1, 11]]\n",
      "2         SR-p53      1.00  1.000000   [[97, 0], [0, 3]]\n"
     ]
    }
   ],
   "source": [
    "predictions_df = predict_with_models(models_dict, X_test_dict, y_test_cat_dict)\n",
    "print(predictions_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
