{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_cbn(dataset_path, target_col, smiles_col):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    from tensorflow.keras import optimizers, layers, metrics\n",
    "    from tensorflow.keras.layers import (\n",
    "        Input, Dense, Dropout, BatchNormalization, Conv1D,\n",
    "        GlobalAveragePooling1D, Lambda, Activation, Concatenate\n",
    "    )\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "    # === Load Dataset ===\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    smiles = data[smiles_col]\n",
    "    labels = data[target_col]\n",
    "        # === Tokenization ===\n",
    "        # # Dictionary for converting SMILES characters to numbers\n",
    "    smiles_dict = {\n",
    "    \"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n",
    "    \"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n",
    "    \"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n",
    "    \"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n",
    "    \"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50, \"T\": 17, \"W\": 51,\n",
    "    \"V\": 18, \"Y\": 52, \"[\": 53, \"Z\": 19, \"]\": 54, \"\\\\\": 20, \"a\": 55, \"c\": 56,\n",
    "    \"b\": 21, \"e\": 57, \"d\": 22, \"g\": 58, \"f\": 23, \"i\": 59, \"h\": 24, \"m\": 60,\n",
    "    \"l\": 25, \"o\": 61, \"n\": 26, \"s\": 62, \"r\": 27, \"u\": 63, \"t\": 28, \"y\": 64,\n",
    "    \" \": 65, \":\": 66, \",\": 67, \"p\": 68, \"j\": 69, \"*\": 70\n",
    "    }\n",
    "\n",
    "    def label_smiles(line, MAX_SMI_LEN, smi_ch_ind):\n",
    "        X = np.zeros(MAX_SMI_LEN, dtype=int)\n",
    "        for i, ch in enumerate(line[:MAX_SMI_LEN]):\n",
    "            if ch != '\\n' and ch in smi_ch_ind:\n",
    "                X[i] = smi_ch_ind[ch]\n",
    "        return X\n",
    "\n",
    "    XD = np.array([label_smiles(str(smi), 100, smiles_dict) for smi in smiles])\n",
    "    labels = labels.values\n",
    "\n",
    "    # Convert to categorical\n",
    "    XD = to_categorical(XD, num_classes=71)\n",
    "\n",
    "    # Feature model definition (unchanged)\n",
    "    XDinput = Input(shape=(100, 71), name='XDinput')\n",
    "    encode_smiles = Conv1D(filters=64, kernel_size=2, activation='relu', padding='valid', strides=1)(XDinput)  # (99,64)\n",
    "    encode_smiles = Conv1D(filters=64, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)  # (96,64)\n",
    "    encode_smiles = Conv1D(filters=128, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)  # (93,128)\n",
    "    model_feature = Model(inputs=XDinput, outputs=encode_smiles, name='model_feature')\n",
    "    model_feature.summary()\n",
    "\n",
    "\n",
    "    # Prediction model definition\n",
    "    input_extracted_feature = Input(shape=(93, 128))\n",
    "    FC1 = Dense(512, activation='relu')(input_extracted_feature)\n",
    "    FC1 = BatchNormalization()(FC1)\n",
    "    FC2 = Dropout(0.1)(FC1)\n",
    "    FC3 = Dense(256, activation='relu')(FC2)\n",
    "    FC3 = BatchNormalization()(FC3)\n",
    "    FC4 = Dropout(0.1)(FC3)\n",
    "    FC5 = Dense(64, activation='relu')(FC4)\n",
    "    predictions = Dense(2, activation='softmax')(FC5)\n",
    "    model_pred = Model(inputs=input_extracted_feature, outputs=predictions)\n",
    "    model_pred.summary()\n",
    "\n",
    "    interaction_input = XDinput\n",
    "    encoded_features = model_feature(interaction_input)\n",
    "    predicted_output = model_pred(encoded_features)\n",
    "    pooled_output = GlobalAveragePooling1D()(predicted_output)\n",
    "    interactionModel = Model(inputs=interaction_input, outputs=pooled_output, name='interactionModel')\n",
    "\n",
    "    class DropPath(layers.Layer):\n",
    "        def __init__(self, drop_prob=0.):\n",
    "            super().__init__()\n",
    "            self.drop_prob = drop_prob\n",
    "        def call(self, x, training=None):\n",
    "            if (not training) or self.drop_prob == 0.:\n",
    "                return x\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            random_tensor = keep_prob\n",
    "            random_tensor += tf.random.uniform(tf.shape(x), dtype=x.dtype)\n",
    "            binary_tensor = tf.floor(random_tensor)\n",
    "            return tf.divide(x, keep_prob) * binary_tensor\n",
    "\n",
    "    class TopkRouting(layers.Layer):\n",
    "        def __init__(self, qk_dim, topk=16, qk_scale=None, param_routing=False, diff_routing=False):\n",
    "            super().__init__()\n",
    "            self.topk = topk\n",
    "            self.qk_dim = qk_dim\n",
    "            self.scale = qk_scale if qk_scale is not None else qk_dim**-0.5\n",
    "            self.diff_routing = diff_routing\n",
    "            if param_routing:\n",
    "                self.emb = layers.Dense(qk_dim)\n",
    "            else:\n",
    "                self.emb = lambda x: x\n",
    "            self.routing_act = lambda x, axis: tf.nn.softmax(x, axis=axis)\n",
    "        def call(self, query, key, training=None):\n",
    "            if not self.diff_routing:\n",
    "                query = tf.stop_gradient(query)\n",
    "                key = tf.stop_gradient(key)\n",
    "            query_hat = self.emb(query)\n",
    "            key_hat = self.emb(key)\n",
    "            attn_logit = tf.einsum('bnc,bqc->bnq', query_hat*self.scale, key_hat)\n",
    "            topk_attn_logit, topk_index = tf.math.top_k(attn_logit, k=self.topk, sorted=True)\n",
    "            r_weight = self.routing_act(topk_attn_logit, axis=-1)\n",
    "            return r_weight, topk_index\n",
    "\n",
    "    def tf_gather_kv(kv, r_idx):\n",
    "        n = tf.shape(kv)[0]\n",
    "        p2 = tf.shape(kv)[1]\n",
    "        c_kv = tf.shape(kv)[2]\n",
    "        topk = tf.shape(r_idx)[2]\n",
    "        batch_idx = tf.reshape(tf.range(n), [n, 1, 1])\n",
    "        batch_idx = tf.tile(batch_idx, [1, p2, topk])\n",
    "        p2_idx = tf.reshape(tf.range(p2), [1, p2, 1])\n",
    "        p2_idx = tf.tile(p2_idx, [n, 1, topk])\n",
    "        gather_indices = tf.stack([batch_idx, p2_idx, r_idx], axis=-1)\n",
    "        gathered = tf.gather_nd(kv, gather_indices)\n",
    "        return gathered\n",
    "\n",
    "    class KVGather(layers.Layer):\n",
    "        def __init__(self, mul_weight='none'):\n",
    "            super().__init__()\n",
    "            assert mul_weight in ['none', 'soft', 'hard']\n",
    "            self.mul_weight = mul_weight\n",
    "        def call(self, r_idx, r_weight, kv, training=None):\n",
    "            topk_kv = tf_gather_kv(kv, r_idx)\n",
    "            if self.mul_weight == 'soft':\n",
    "                r_weight_exp = tf.expand_dims(tf.expand_dims(r_weight, -1), -1)\n",
    "                topk_kv = topk_kv * r_weight_exp\n",
    "            return topk_kv\n",
    "\n",
    "    class QKVLinear(layers.Layer):\n",
    "        def __init__(self, dim, qk_dim, bias=True):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.qk_dim = qk_dim\n",
    "            self.qkv = layers.Dense(qk_dim + qk_dim + dim, use_bias=bias)\n",
    "        def call(self, x, training=None):\n",
    "            qkv = self.qkv(x)\n",
    "            q, kv = tf.split(qkv, [self.qk_dim, self.qk_dim + self.dim], axis=-1)\n",
    "            return q, kv\n",
    "\n",
    "    class BiLevelRoutingAttention(layers.Layer):\n",
    "        def __init__(self, dim, n_win=7, num_heads=8, qk_dim=None, qk_scale=None,\n",
    "                     kv_per_win=4, kv_downsample_ratio=4, kv_downsample_mode='identity',\n",
    "                     topk=4, param_attention=\"qkvo\", param_routing=False, diff_routing=False, soft_routing=False,\n",
    "                     side_dwconv=3, auto_pad=True):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.n_win = n_win\n",
    "            self.num_heads = num_heads\n",
    "            self.qk_dim = qk_dim if qk_dim is not None else dim\n",
    "            self.scale = qk_scale if qk_scale is not None else self.qk_dim**-0.5\n",
    "            self.topk = topk\n",
    "            self.param_routing = param_routing\n",
    "            self.diff_routing = diff_routing\n",
    "            self.soft_routing = soft_routing\n",
    "            self.auto_pad = auto_pad\n",
    "            if side_dwconv > 0:\n",
    "                self.lepe = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n",
    "            else:\n",
    "                self.lepe = lambda x: tf.zeros_like(x)\n",
    "            self.router = TopkRouting(qk_dim=self.qk_dim, topk=self.topk, qk_scale=self.scale,\n",
    "                                      param_routing=self.param_routing, diff_routing=self.diff_routing)\n",
    "            mul_weight = 'none'\n",
    "            if self.soft_routing:\n",
    "                mul_weight = 'soft'\n",
    "            self.kv_gather = KVGather(mul_weight=mul_weight)\n",
    "            if param_attention in ['qkvo', 'qkv']:\n",
    "                self.qkv = QKVLinear(self.dim, self.qk_dim)\n",
    "                if param_attention == 'qkvo':\n",
    "                    self.wo = layers.Dense(self.dim)\n",
    "                else:\n",
    "                    self.wo = lambda x: x\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported param_attention mode\")\n",
    "            self.attn_act = lambda x: tf.nn.softmax(x, axis=-1)\n",
    "            self.kv_down = lambda x: x\n",
    "        def call(self, x, training=None, ret_attn_mask=False):\n",
    "            if ret_attn_mask:\n",
    "                return x, None, None, None\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "    class Attention(layers.Layer):\n",
    "        def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "            super().__init__()\n",
    "            self.num_heads = num_heads\n",
    "            head_dim = dim // num_heads\n",
    "            self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n",
    "        def call(self, x, training=None):\n",
    "            return x\n",
    "\n",
    "    class AttentionLePE(layers.Layer):\n",
    "        def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., side_dwconv=3):\n",
    "            super().__init__()\n",
    "            self.num_heads = num_heads\n",
    "            head_dim = dim // num_heads\n",
    "            self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n",
    "            self.side_dwconv = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n",
    "        def call(self, x, training=None):\n",
    "            return x\n",
    "\n",
    "    class Mlp(layers.Layer):\n",
    "        def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
    "            super().__init__()\n",
    "            out_features = out_features or in_features\n",
    "            hidden_features = hidden_features or in_features\n",
    "            self.net = keras.Sequential([\n",
    "                layers.Dense(hidden_features),\n",
    "                layers.Activation('gelu'),\n",
    "                layers.Dropout(rate=drop),\n",
    "                layers.Dense(out_features),\n",
    "                layers.Dropout(rate=drop)\n",
    "            ])\n",
    "        def call(self, x, training=True):\n",
    "            return self.net(x, training=training)\n",
    "\n",
    "    class Block(layers.Layer):\n",
    "        def __init__(self, dim, drop_path=0.1, layer_scale_init_value=-1,\n",
    "                     num_heads=8, n_win=7, qk_dim=128, qk_scale=None,\n",
    "                     kv_per_win=8, kv_downsample_ratio=1, kv_downsample_mode='identity',\n",
    "                     topk=8, param_attention=\"qkvo\", param_routing=True, diff_routing=False, soft_routing=True,\n",
    "                     mlp_ratio=4, mlp_dwconv=False, side_dwconv=5, before_attn_dwconv=3, pre_norm=True, auto_pad=True):\n",
    "            super().__init__()\n",
    "            qk_dim = qk_dim or dim\n",
    "            if before_attn_dwconv > 0:\n",
    "                self.pos_embed = layers.Conv1D(dim, kernel_size=before_attn_dwconv, padding='same', activation='relu')\n",
    "            else:\n",
    "                self.pos_embed = lambda x: tf.zeros_like(x)\n",
    "            self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            if topk > 0:\n",
    "                self.attn = BiLevelRoutingAttention(dim=dim, num_heads=num_heads, n_win=n_win, qk_dim=qk_dim,\n",
    "                                                   qk_scale=qk_scale, kv_per_win=kv_per_win, kv_downsample_ratio=kv_downsample_ratio,\n",
    "                                                   kv_downsample_mode=kv_downsample_mode, topk=topk, param_attention=param_attention,\n",
    "                                                   param_routing=param_routing, diff_routing=diff_routing, soft_routing=soft_routing,\n",
    "                                                   side_dwconv=side_dwconv, auto_pad=auto_pad)\n",
    "            elif topk == -1:\n",
    "                self.attn = Attention(dim=dim, num_heads=num_heads, qk_scale=qk_scale)\n",
    "            elif topk == -2:\n",
    "                self.attn = AttentionLePE(dim=dim, num_heads=num_heads, qk_scale=qk_scale, side_dwconv=side_dwconv)\n",
    "            else:\n",
    "                self.attn = keras.Sequential([\n",
    "                    layers.Dense(dim),\n",
    "                    layers.Conv1D(dim, kernel_size=5, padding='same', activation='relu'),\n",
    "                    layers.Dense(dim)\n",
    "                ])\n",
    "            self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            mlp_hidden_dim = int(mlp_ratio * dim)\n",
    "            mlp_layers = [layers.Dense(mlp_hidden_dim)]\n",
    "            if mlp_dwconv:\n",
    "                mlp_layers.append(layers.Conv1D(mlp_hidden_dim, kernel_size=3, padding='same', activation='relu'))\n",
    "            mlp_layers.append(layers.Activation('gelu'))\n",
    "            mlp_layers.append(layers.Dense(dim))\n",
    "            mlp_layers.insert(1, layers.Dropout(0.2))\n",
    "            mlp_layers.append(layers.Dropout(0.2))\n",
    "            self.mlp = keras.Sequential(mlp_layers)\n",
    "            self.drop_path = DropPath(drop_path) if drop_path > 0. else layers.Lambda(lambda x: x)\n",
    "            if layer_scale_init_value > 0:\n",
    "                self.use_layer_scale = True\n",
    "                self.gamma1 = self.add_weight(shape=(dim,), initializer=tf.keras.initializers.Constant(layer_scale_init_value), trainable=True)\n",
    "                self.gamma2 = self.add_weight(shape=(dim,), initializer=tf.keras.initializers.Constant(layer_scale_init_value), trainable=True)\n",
    "            else:\n",
    "                self.use_layer_scale = False\n",
    "        def call(self, x, training=None):\n",
    "            input_x = x\n",
    "            x = self.pos_embed(x)\n",
    "            x = self.norm1(x)\n",
    "            attn = self.attn(x, training=training)\n",
    "            if self.use_layer_scale:\n",
    "                attn = self.gamma1 * attn\n",
    "            x = input_x + self.drop_path(attn, training=training)\n",
    "            input_x = x\n",
    "            x = self.norm2(x)\n",
    "            mlp_output = self.mlp(x, training=training)\n",
    "            if self.use_layer_scale:\n",
    "                mlp_output = self.gamma2 * mlp_output\n",
    "            x = input_x + self.drop_path(mlp_output, training=training)\n",
    "            return x\n",
    "\n",
    "    dim = 128\n",
    "    num_heads = 8\n",
    "\n",
    "    input_phaz2 = Input(shape=(93, 128))\n",
    "    processed_input = input_phaz2\n",
    "    transformer_block1 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "    transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "    transformer_output1 = transformer_block1(processed_input)\n",
    "    transformer_output1 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1)\n",
    "    transformer_output2 = transformer_block2(processed_input)\n",
    "    transformer_output2 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2)\n",
    "    combined_outputs = Concatenate(axis=-1)([transformer_output1, transformer_output2])\n",
    "    pooled_outputs = GlobalAveragePooling1D()(combined_outputs)\n",
    "    difference = Lambda(lambda x: tf.expand_dims(x[:, 0] - x[:, 1], axis=-1), name='difference')(pooled_outputs)\n",
    "    condition_2 = Activation('sigmoid', name='activation_11')(difference)\n",
    "    model_feature.trainable = False\n",
    "    model_phaz2 = Model(inputs=input_phaz2, outputs=condition_2)\n",
    "\n",
    "    new_transformer_block = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "    new_transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n",
    "    XDinput_phaz3 = Input(shape=(100, 71))\n",
    "    model_feature_output = model_feature(XDinput_phaz3)\n",
    "    transformer_output1_phaz3 = new_transformer_block(model_feature_output)\n",
    "    transformer_output1_phaz3 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1_phaz3)\n",
    "    transformer_output2_phaz3 = new_transformer_block2(model_feature_output)\n",
    "    transformer_output2_phaz3 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2_phaz3)\n",
    "    concatenated_phaz3 = Concatenate(axis=-1)([transformer_output1_phaz3, transformer_output2_phaz3])\n",
    "    model_feature.trainable = False\n",
    "    new_transformer_block.trainable = False\n",
    "    new_transformer_block2.trainable = False\n",
    "    pooled_phaz3 = GlobalAveragePooling1D()(concatenated_phaz3)\n",
    "    FC1_phaz3 = Dense(512, activation='relu')(pooled_phaz3)\n",
    "    FC1_phaz3 = BatchNormalization()(FC1_phaz3)\n",
    "    FC2_phaz3 = Dropout(0.1)(FC1_phaz3)\n",
    "    FC3_phaz3 = Dense(256, activation='relu')(FC2_phaz3)\n",
    "    FC3_phaz3 = BatchNormalization()(FC3_phaz3)\n",
    "    FC4_phaz3 = Dropout(0.1)(FC3_phaz3)\n",
    "    FC5_phaz3 = Dense(64, activation='relu')(FC4_phaz3)\n",
    "    predictions_phaz3 = Dense(2, activation='softmax')(FC5_phaz3)\n",
    "    model_phaz3 = Model(inputs=XDinput_phaz3, outputs=predictions_phaz3)\n",
    "\n",
    "    METRICS_BINARY = [\n",
    "        metrics.BinaryAccuracy(name='accuracy'),\n",
    "        metrics.AUC(name='auc'),\n",
    "    ]\n",
    "    METRICS_CATEGORICAL = [\n",
    "        metrics.CategoricalAccuracy(name='accuracy'),\n",
    "        metrics.Precision(name='precision'),\n",
    "        metrics.Recall(name='recall'),\n",
    "        metrics.AUC(name='auc'),\n",
    "        metrics.F1Score(average='macro', name='f1_score')\n",
    "    ]\n",
    "\n",
    "    # === Data Preparation ===\n",
    "    XD_np = np.array(XD)\n",
    "    labels_np = np.array(labels)\n",
    "    index = np.where(~np.isnan(labels_np))\n",
    "    labels_np, XD_np = labels_np[index[0]], XD_np[index[0], :]\n",
    "\n",
    "    # === Train-Test Split ===\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        XD_np, labels_np, test_size=0.2, stratify=labels_np, random_state=9\n",
    "    )\n",
    "\n",
    "    # Convert labels to categorical format\n",
    "    y_train_cat = to_categorical(y_train, num_classes=2)\n",
    "    y_test_cat = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    # Compute class weights to handle class imbalance\n",
    "    class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train.ravel()\n",
    "    )\n",
    "    class_weight = {\n",
    "    0: class_weights_array[0],\n",
    "    1: class_weights_array[1]\n",
    "    }\n",
    "\n",
    "    # === Define Optimizer for Interaction Model ===\n",
    "    opt = optimizers.Adam(learning_rate=0.0001)\n",
    "    interactionModel.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=METRICS_CATEGORICAL)\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=30, verbose=1, mode='min', restore_best_weights=True)\n",
    "    reduce_lr = LearningRateScheduler(lambda epoch: 1e-3 * 0.9 ** epoch)\n",
    "\n",
    "    # === Phase 1: Train Interaction Model ===\n",
    "    interactionModel.fit(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=256, epochs=100, class_weight=class_weight,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate Phase 1 on test data\n",
    "    test_eval_phase1 = interactionModel.evaluate(X_test, y_test_cat, verbose=1)\n",
    "\n",
    "    # === Feature Extraction ===\n",
    "    feature_train = model_feature.predict(X_train)\n",
    "    feature_test = model_feature.predict(X_test)\n",
    "\n",
    "    # === Phase 2: Train Model Phase 2 ===\n",
    "    opt_phaz2 = optimizers.Adam(learning_rate=0.001)\n",
    "    model_phaz2.compile(optimizer=opt_phaz2, loss=\"binary_crossentropy\", metrics=METRICS_BINARY)\n",
    "    model_phaz2.fit(\n",
    "    feature_train, y_train,\n",
    "    batch_size=256, epochs=100,class_weight=class_weight,\n",
    "    callbacks=[early_stopping], verbose=1\n",
    "    )\n",
    "\n",
    "    # === Freeze Transformer Blocks ===\n",
    "    new_transformer_block.set_weights(transformer_block1.get_weights())\n",
    "    new_transformer_block2.set_weights(transformer_block2.get_weights())  # Corrected as per your comment\n",
    "\n",
    "    # === Phase 3: Train Model Phase 3 ===\n",
    "    opt_phaz3 = optimizers.Adam(learning_rate=0.0001)\n",
    "    model_phaz3.compile(optimizer=opt_phaz3, loss=\"categorical_crossentropy\", metrics=METRICS_CATEGORICAL)\n",
    "\n",
    "    model_phaz3.fit(\n",
    "        X_train, y_train_cat,\n",
    "        batch_size=256, epochs=100,class_weight=class_weight,\n",
    "        callbacks=[early_stopping], verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate Phase 3 on both training and testing data\n",
    "    train_eval_phase3 = model_phaz3.evaluate(X_train, y_train_cat, verbose=1)\n",
    "    test_eval_phase3 = model_phaz3.evaluate(X_test, y_test_cat, verbose=1)\n",
    "\n",
    "    # === Function to Format Results ===\n",
    "    def format_results(phase1_result, phase3_train_result, phase3_test_result):\n",
    "            # Define Metric Names\n",
    "        phase1_metrics = ['loss', 'accuracy', 'precision', 'recall', 'auc', 'f1_score']\n",
    "        phase3_metrics = ['loss', 'accuracy', 'precision', 'recall', 'auc', 'f1_score']\n",
    "\n",
    "        # Create DataFrames\n",
    "        phase1_test_df = pd.DataFrame([phase1_result], columns=phase1_metrics)\n",
    "        phase3_train_df = pd.DataFrame([phase3_train_result], columns=phase3_metrics)\n",
    "        phase3_test_df = pd.DataFrame([test_eval_phase3], columns=phase3_metrics)\n",
    "\n",
    "        # Display the Results\n",
    "        print(\"\\n=== Phase 1: Test Evaluation Results ===\")\n",
    "        print(phase1_test_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "        print(\"\\n=== Phase 3: Train Evaluation Results ===\")\n",
    "        print(phase3_train_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "        print(\"\\n=== Phase 3: Test Evaluation Results ===\")\n",
    "        print(phase3_test_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "\n",
    "    # === Display Results ===\n",
    "    format_results(test_eval_phase1, train_eval_phase3, test_eval_phase3)\n",
    "\n",
    "    return model_phaz3, train_eval_phase3, test_eval_phase3, X_test, y_test_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e8600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_feature\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"model_feature\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ XDinput (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_33 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m9,152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_34 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_35 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,496</span> (228.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,496\u001b[0m (228.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_21\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_21\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_21 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_84 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_36 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_85 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_37 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_86 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_87 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">217,026</span> (847.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m217,026\u001b[0m (847.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">215,490</span> (841.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m215,490\u001b[0m (841.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_12', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_13', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_14', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scott.Coffin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'bi_level_routing_attention_15', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 522ms/step - accuracy: 0.6704 - auc: 0.7202 - f1_score: 0.4281 - loss: 0.6759 - precision: 0.6704 - recall: 0.6704\n",
      "Epoch 2/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 480ms/step - accuracy: 0.5798 - auc: 0.5734 - f1_score: 0.4050 - loss: 0.6546 - precision: 0.5798 - recall: 0.5798\n",
      "Epoch 3/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 468ms/step - accuracy: 0.7919 - auc: 0.8595 - f1_score: 0.4972 - loss: 0.6591 - precision: 0.7919 - recall: 0.7919\n",
      "Epoch 4/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 485ms/step - accuracy: 0.5806 - auc: 0.5842 - f1_score: 0.4116 - loss: 0.6775 - precision: 0.5806 - recall: 0.5806\n",
      "Epoch 5/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 533ms/step - accuracy: 0.7300 - auc: 0.7860 - f1_score: 0.4786 - loss: 0.6502 - precision: 0.7300 - recall: 0.7300\n",
      "Epoch 6/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 660ms/step - accuracy: 0.7571 - auc: 0.8295 - f1_score: 0.4870 - loss: 0.6133 - precision: 0.7571 - recall: 0.7571\n",
      "Epoch 7/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 840ms/step - accuracy: 0.4783 - auc: 0.4456 - f1_score: 0.3600 - loss: 0.6612 - precision: 0.4783 - recall: 0.4783\n",
      "Epoch 8/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 839ms/step - accuracy: 0.7678 - auc: 0.8377 - f1_score: 0.4952 - loss: 0.6220 - precision: 0.7678 - recall: 0.7678\n",
      "Epoch 9/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 872ms/step - accuracy: 0.7687 - auc: 0.8357 - f1_score: 0.4987 - loss: 0.6203 - precision: 0.7687 - recall: 0.7687\n",
      "Epoch 10/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 741ms/step - accuracy: 0.6964 - auc: 0.7301 - f1_score: 0.4670 - loss: 0.6685 - precision: 0.6964 - recall: 0.6964\n",
      "Epoch 11/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 759ms/step - accuracy: 0.7302 - auc: 0.7840 - f1_score: 0.4860 - loss: 0.6272 - precision: 0.7302 - recall: 0.7302\n",
      "Epoch 12/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 888ms/step - accuracy: 0.8332 - auc: 0.9099 - f1_score: 0.5270 - loss: 0.5988 - precision: 0.8332 - recall: 0.8332\n",
      "Epoch 13/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 833ms/step - accuracy: 0.7842 - auc: 0.8549 - f1_score: 0.4982 - loss: 0.6029 - precision: 0.7842 - recall: 0.7842\n",
      "Epoch 14/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 755ms/step - accuracy: 0.7187 - auc: 0.7809 - f1_score: 0.4752 - loss: 0.6386 - precision: 0.7187 - recall: 0.7187\n",
      "Epoch 15/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 710ms/step - accuracy: 0.7985 - auc: 0.8769 - f1_score: 0.5169 - loss: 0.6367 - precision: 0.7985 - recall: 0.7985\n",
      "Epoch 16/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 742ms/step - accuracy: 0.8351 - auc: 0.9121 - f1_score: 0.5293 - loss: 0.5804 - precision: 0.8351 - recall: 0.8351\n",
      "Epoch 17/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 840ms/step - accuracy: 0.7613 - auc: 0.8293 - f1_score: 0.5003 - loss: 0.6237 - precision: 0.7613 - recall: 0.7613\n",
      "Epoch 18/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 845ms/step - accuracy: 0.8215 - auc: 0.8978 - f1_score: 0.5340 - loss: 0.6048 - precision: 0.8215 - recall: 0.8215\n",
      "Epoch 19/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 854ms/step - accuracy: 0.7590 - auc: 0.8381 - f1_score: 0.5003 - loss: 0.6143 - precision: 0.7590 - recall: 0.7590\n",
      "Epoch 20/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 686ms/step - accuracy: 0.8227 - auc: 0.8978 - f1_score: 0.5235 - loss: 0.5901 - precision: 0.8227 - recall: 0.8227\n",
      "Epoch 21/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 683ms/step - accuracy: 0.8217 - auc: 0.9019 - f1_score: 0.5265 - loss: 0.5895 - precision: 0.8217 - recall: 0.8217\n",
      "Epoch 22/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 777ms/step - accuracy: 0.7897 - auc: 0.8724 - f1_score: 0.5054 - loss: 0.5659 - precision: 0.7897 - recall: 0.7897\n",
      "Epoch 23/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.6683 - auc: 0.7108 - f1_score: 0.4571 - loss: 0.5943 - precision: 0.6683 - recall: 0.6683\n",
      "Epoch 24/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 882ms/step - accuracy: 0.6861 - auc: 0.7433 - f1_score: 0.4725 - loss: 0.6184 - precision: 0.6861 - recall: 0.6861\n",
      "Epoch 25/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 916ms/step - accuracy: 0.7739 - auc: 0.8395 - f1_score: 0.5126 - loss: 0.6045 - precision: 0.7739 - recall: 0.7739\n",
      "Epoch 26/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 828ms/step - accuracy: 0.8298 - auc: 0.9066 - f1_score: 0.5332 - loss: 0.5959 - precision: 0.8298 - recall: 0.8298\n",
      "Epoch 27/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 731ms/step - accuracy: 0.7842 - auc: 0.8655 - f1_score: 0.5195 - loss: 0.6070 - precision: 0.7842 - recall: 0.7842\n",
      "Epoch 28/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 874ms/step - accuracy: 0.6575 - auc: 0.7111 - f1_score: 0.4568 - loss: 0.6675 - precision: 0.6575 - recall: 0.6575\n",
      "Epoch 29/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 872ms/step - accuracy: 0.8349 - auc: 0.9112 - f1_score: 0.5447 - loss: 0.6007 - precision: 0.8349 - recall: 0.8349\n",
      "Epoch 30/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 710ms/step - accuracy: 0.8551 - auc: 0.9264 - f1_score: 0.5595 - loss: 0.5970 - precision: 0.8551 - recall: 0.8551\n",
      "Epoch 31/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 817ms/step - accuracy: 0.8043 - auc: 0.8826 - f1_score: 0.5368 - loss: 0.6027 - precision: 0.8043 - recall: 0.8043\n",
      "Epoch 32/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 712ms/step - accuracy: 0.7868 - auc: 0.8602 - f1_score: 0.5189 - loss: 0.5972 - precision: 0.7868 - recall: 0.7868\n",
      "Epoch 33/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 765ms/step - accuracy: 0.7944 - auc: 0.8772 - f1_score: 0.5235 - loss: 0.6089 - precision: 0.7944 - recall: 0.7944\n",
      "Epoch 34/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 729ms/step - accuracy: 0.8012 - auc: 0.8755 - f1_score: 0.5347 - loss: 0.5852 - precision: 0.8012 - recall: 0.8012\n",
      "Epoch 35/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 751ms/step - accuracy: 0.8231 - auc: 0.8994 - f1_score: 0.5434 - loss: 0.5925 - precision: 0.8231 - recall: 0.8231\n",
      "Epoch 36/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 701ms/step - accuracy: 0.8273 - auc: 0.9044 - f1_score: 0.5587 - loss: 0.6158 - precision: 0.8273 - recall: 0.8273\n",
      "Epoch 37/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 778ms/step - accuracy: 0.8263 - auc: 0.9073 - f1_score: 0.5485 - loss: 0.6056 - precision: 0.8263 - recall: 0.8263\n",
      "Epoch 38/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 551ms/step - accuracy: 0.8525 - auc: 0.9270 - f1_score: 0.5758 - loss: 0.6019 - precision: 0.8525 - recall: 0.8525\n",
      "Epoch 39/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 580ms/step - accuracy: 0.8411 - auc: 0.9185 - f1_score: 0.5568 - loss: 0.5999 - precision: 0.8411 - recall: 0.8411\n",
      "Epoch 40/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 796ms/step - accuracy: 0.8401 - auc: 0.9101 - f1_score: 0.5522 - loss: 0.5895 - precision: 0.8401 - recall: 0.8401\n",
      "Epoch 41/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 756ms/step - accuracy: 0.7146 - auc: 0.7731 - f1_score: 0.4900 - loss: 0.6039 - precision: 0.7146 - recall: 0.7146\n",
      "Epoch 42/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 874ms/step - accuracy: 0.7786 - auc: 0.8579 - f1_score: 0.5192 - loss: 0.6169 - precision: 0.7786 - recall: 0.7786\n",
      "Epoch 43/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 759ms/step - accuracy: 0.8544 - auc: 0.9254 - f1_score: 0.5624 - loss: 0.5767 - precision: 0.8544 - recall: 0.8544\n",
      "Epoch 44/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 716ms/step - accuracy: 0.8918 - auc: 0.9521 - f1_score: 0.5962 - loss: 0.5799 - precision: 0.8918 - recall: 0.8918\n",
      "Epoch 45/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 787ms/step - accuracy: 0.8270 - auc: 0.9014 - f1_score: 0.5555 - loss: 0.5965 - precision: 0.8270 - recall: 0.8270\n",
      "Epoch 46/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 744ms/step - accuracy: 0.8383 - auc: 0.9164 - f1_score: 0.5560 - loss: 0.5799 - precision: 0.8383 - recall: 0.8383\n",
      "Epoch 47/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 844ms/step - accuracy: 0.8254 - auc: 0.8989 - f1_score: 0.5479 - loss: 0.5606 - precision: 0.8254 - recall: 0.8254\n",
      "Epoch 48/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 878ms/step - accuracy: 0.8115 - auc: 0.8897 - f1_score: 0.5396 - loss: 0.5544 - precision: 0.8115 - recall: 0.8115\n",
      "Epoch 49/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 698ms/step - accuracy: 0.7617 - auc: 0.8277 - f1_score: 0.5051 - loss: 0.5841 - precision: 0.7617 - recall: 0.7617\n",
      "Epoch 50/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 607ms/step - accuracy: 0.8155 - auc: 0.8976 - f1_score: 0.5421 - loss: 0.5736 - precision: 0.8155 - recall: 0.8155\n",
      "Epoch 51/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 659ms/step - accuracy: 0.8526 - auc: 0.9305 - f1_score: 0.5666 - loss: 0.5927 - precision: 0.8526 - recall: 0.8526\n",
      "Epoch 52/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 771ms/step - accuracy: 0.8514 - auc: 0.9227 - f1_score: 0.5881 - loss: 0.6380 - precision: 0.8514 - recall: 0.8514\n",
      "Epoch 53/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 706ms/step - accuracy: 0.8834 - auc: 0.9487 - f1_score: 0.5844 - loss: 0.5517 - precision: 0.8834 - recall: 0.8834\n",
      "Epoch 54/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 632ms/step - accuracy: 0.7896 - auc: 0.8616 - f1_score: 0.5356 - loss: 0.5976 - precision: 0.7896 - recall: 0.7896\n",
      "Epoch 55/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 667ms/step - accuracy: 0.7898 - auc: 0.8655 - f1_score: 0.5339 - loss: 0.6048 - precision: 0.7898 - recall: 0.7898\n",
      "Epoch 56/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 804ms/step - accuracy: 0.8834 - auc: 0.9513 - f1_score: 0.5921 - loss: 0.5579 - precision: 0.8834 - recall: 0.8834\n",
      "Epoch 57/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 691ms/step - accuracy: 0.8904 - auc: 0.9527 - f1_score: 0.5919 - loss: 0.5471 - precision: 0.8904 - recall: 0.8904\n",
      "Epoch 58/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 669ms/step - accuracy: 0.8245 - auc: 0.9085 - f1_score: 0.5454 - loss: 0.5764 - precision: 0.8245 - recall: 0.8245\n",
      "Epoch 59/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 711ms/step - accuracy: 0.8465 - auc: 0.9194 - f1_score: 0.5637 - loss: 0.5617 - precision: 0.8465 - recall: 0.8465\n",
      "Epoch 60/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 752ms/step - accuracy: 0.8695 - auc: 0.9360 - f1_score: 0.5825 - loss: 0.5562 - precision: 0.8695 - recall: 0.8695\n",
      "Epoch 61/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 695ms/step - accuracy: 0.8381 - auc: 0.9127 - f1_score: 0.5618 - loss: 0.5878 - precision: 0.8381 - recall: 0.8381\n",
      "Epoch 62/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 603ms/step - accuracy: 0.8859 - auc: 0.9507 - f1_score: 0.5969 - loss: 0.5484 - precision: 0.8859 - recall: 0.8859\n",
      "Epoch 63/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 778ms/step - accuracy: 0.8607 - auc: 0.9370 - f1_score: 0.5930 - loss: 0.6054 - precision: 0.8607 - recall: 0.8607\n",
      "Epoch 64/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 597ms/step - accuracy: 0.8756 - auc: 0.9493 - f1_score: 0.5906 - loss: 0.5667 - precision: 0.8756 - recall: 0.8756\n",
      "Epoch 65/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 571ms/step - accuracy: 0.8528 - auc: 0.9241 - f1_score: 0.5733 - loss: 0.5847 - precision: 0.8528 - recall: 0.8528\n",
      "Epoch 66/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 531ms/step - accuracy: 0.8387 - auc: 0.9178 - f1_score: 0.5706 - loss: 0.5964 - precision: 0.8387 - recall: 0.8387\n",
      "Epoch 67/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 469ms/step - accuracy: 0.8285 - auc: 0.9053 - f1_score: 0.5500 - loss: 0.5726 - precision: 0.8285 - recall: 0.8285\n",
      "Epoch 68/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 629ms/step - accuracy: 0.8961 - auc: 0.9638 - f1_score: 0.6053 - loss: 0.5688 - precision: 0.8961 - recall: 0.8961\n",
      "Epoch 69/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 654ms/step - accuracy: 0.8720 - auc: 0.9446 - f1_score: 0.5977 - loss: 0.5965 - precision: 0.8720 - recall: 0.8720\n",
      "Epoch 70/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 589ms/step - accuracy: 0.8773 - auc: 0.9486 - f1_score: 0.5962 - loss: 0.5839 - precision: 0.8773 - recall: 0.8773\n",
      "Epoch 71/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 578ms/step - accuracy: 0.8985 - auc: 0.9595 - f1_score: 0.6159 - loss: 0.5809 - precision: 0.8985 - recall: 0.8985\n",
      "Epoch 72/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 626ms/step - accuracy: 0.8704 - auc: 0.9389 - f1_score: 0.5971 - loss: 0.6047 - precision: 0.8704 - recall: 0.8704\n",
      "Epoch 73/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 559ms/step - accuracy: 0.8853 - auc: 0.9540 - f1_score: 0.5992 - loss: 0.5653 - precision: 0.8853 - recall: 0.8853\n",
      "Epoch 74/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 509ms/step - accuracy: 0.8660 - auc: 0.9389 - f1_score: 0.5860 - loss: 0.5969 - precision: 0.8660 - recall: 0.8660\n",
      "Epoch 75/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 495ms/step - accuracy: 0.8701 - auc: 0.9408 - f1_score: 0.5817 - loss: 0.5819 - precision: 0.8701 - recall: 0.8701\n",
      "Epoch 76/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 508ms/step - accuracy: 0.8942 - auc: 0.9626 - f1_score: 0.6152 - loss: 0.5725 - precision: 0.8942 - recall: 0.8942\n",
      "Epoch 77/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 529ms/step - accuracy: 0.8991 - auc: 0.9629 - f1_score: 0.6175 - loss: 0.5780 - precision: 0.8991 - recall: 0.8991\n",
      "Epoch 78/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 537ms/step - accuracy: 0.9135 - auc: 0.9707 - f1_score: 0.6461 - loss: 0.5768 - precision: 0.9135 - recall: 0.9135\n",
      "Epoch 79/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 583ms/step - accuracy: 0.8888 - auc: 0.9518 - f1_score: 0.6110 - loss: 0.5677 - precision: 0.8888 - recall: 0.8888\n",
      "Epoch 80/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 562ms/step - accuracy: 0.8671 - auc: 0.9423 - f1_score: 0.5930 - loss: 0.5690 - precision: 0.8671 - recall: 0.8671\n",
      "Epoch 81/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 536ms/step - accuracy: 0.8760 - auc: 0.9461 - f1_score: 0.5873 - loss: 0.5507 - precision: 0.8760 - recall: 0.8760\n",
      "Epoch 82/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 523ms/step - accuracy: 0.8756 - auc: 0.9451 - f1_score: 0.6097 - loss: 0.6042 - precision: 0.8756 - recall: 0.8756\n",
      "Epoch 83/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 529ms/step - accuracy: 0.8236 - auc: 0.8979 - f1_score: 0.5683 - loss: 0.6309 - precision: 0.8236 - recall: 0.8236\n",
      "Epoch 84/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 536ms/step - accuracy: 0.8947 - auc: 0.9625 - f1_score: 0.6247 - loss: 0.5784 - precision: 0.8947 - recall: 0.8947\n",
      "Epoch 85/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 517ms/step - accuracy: 0.8927 - auc: 0.9562 - f1_score: 0.6161 - loss: 0.5610 - precision: 0.8927 - recall: 0.8927\n",
      "Epoch 86/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 512ms/step - accuracy: 0.8729 - auc: 0.9456 - f1_score: 0.5946 - loss: 0.5563 - precision: 0.8729 - recall: 0.8729\n",
      "Epoch 87/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 522ms/step - accuracy: 0.8949 - auc: 0.9583 - f1_score: 0.6107 - loss: 0.5710 - precision: 0.8949 - recall: 0.8949\n",
      "Epoch 88/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 524ms/step - accuracy: 0.9163 - auc: 0.9720 - f1_score: 0.6217 - loss: 0.5367 - precision: 0.9163 - recall: 0.9163\n",
      "Epoch 89/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 547ms/step - accuracy: 0.8859 - auc: 0.9521 - f1_score: 0.6094 - loss: 0.5519 - precision: 0.8859 - recall: 0.8859\n",
      "Epoch 90/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 593ms/step - accuracy: 0.9040 - auc: 0.9644 - f1_score: 0.6362 - loss: 0.5600 - precision: 0.9040 - recall: 0.9040\n",
      "Epoch 91/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 505ms/step - accuracy: 0.8936 - auc: 0.9582 - f1_score: 0.6236 - loss: 0.5613 - precision: 0.8936 - recall: 0.8936\n",
      "Epoch 92/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 518ms/step - accuracy: 0.8822 - auc: 0.9569 - f1_score: 0.6060 - loss: 0.5686 - precision: 0.8822 - recall: 0.8822\n",
      "Epoch 93/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 722ms/step - accuracy: 0.8958 - auc: 0.9623 - f1_score: 0.6270 - loss: 0.5651 - precision: 0.8958 - recall: 0.8958\n",
      "Epoch 94/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 749ms/step - accuracy: 0.9158 - auc: 0.9717 - f1_score: 0.6457 - loss: 0.5518 - precision: 0.9158 - recall: 0.9158\n",
      "Epoch 95/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 723ms/step - accuracy: 0.9247 - auc: 0.9784 - f1_score: 0.6557 - loss: 0.5603 - precision: 0.9247 - recall: 0.9247\n",
      "Epoch 96/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 745ms/step - accuracy: 0.8510 - auc: 0.9209 - f1_score: 0.5905 - loss: 0.5743 - precision: 0.8510 - recall: 0.8510\n",
      "Epoch 97/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 704ms/step - accuracy: 0.8375 - auc: 0.9179 - f1_score: 0.5845 - loss: 0.6032 - precision: 0.8375 - recall: 0.8375\n",
      "Epoch 98/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 836ms/step - accuracy: 0.9235 - auc: 0.9777 - f1_score: 0.6756 - loss: 0.5813 - precision: 0.9235 - recall: 0.9235\n",
      "Epoch 99/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 717ms/step - accuracy: 0.9065 - auc: 0.9675 - f1_score: 0.6553 - loss: 0.5874 - precision: 0.9065 - recall: 0.9065\n",
      "Epoch 100/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 728ms/step - accuracy: 0.8913 - auc: 0.9598 - f1_score: 0.6171 - loss: 0.5626 - precision: 0.8913 - recall: 0.8913\n",
      "Restoring model weights from the end of the best epoch: 100.\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - accuracy: 0.9177 - auc: 0.9680 - f1_score: 0.5526 - loss: 0.5124 - precision: 0.9177 - recall: 0.9177\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "Epoch 1/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 996ms/step - accuracy: 0.4750 - auc: 0.4945 - loss: 1.1119\n",
      "Epoch 2/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 996ms/step - accuracy: 0.6178 - auc: 0.6173 - loss: 0.6781\n",
      "Epoch 3/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.6284 - auc: 0.6252 - loss: 0.6924\n",
      "Epoch 4/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.6095 - auc: 0.6529 - loss: 0.6761\n",
      "Epoch 5/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 911ms/step - accuracy: 0.4063 - auc: 0.5932 - loss: 0.6963\n",
      "Epoch 6/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 972ms/step - accuracy: 0.6331 - auc: 0.6567 - loss: 0.6826\n",
      "Epoch 7/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 882ms/step - accuracy: 0.5833 - auc: 0.6296 - loss: 0.6664\n",
      "Epoch 8/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 838ms/step - accuracy: 0.6023 - auc: 0.6524 - loss: 0.6668\n",
      "Epoch 9/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 846ms/step - accuracy: 0.7437 - auc: 0.6841 - loss: 0.6477\n",
      "Epoch 10/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 866ms/step - accuracy: 0.5922 - auc: 0.6768 - loss: 0.6536\n",
      "Epoch 11/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 891ms/step - accuracy: 0.6497 - auc: 0.7206 - loss: 0.6120\n",
      "Epoch 12/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 855ms/step - accuracy: 0.5969 - auc: 0.7107 - loss: 0.6334\n",
      "Epoch 13/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 827ms/step - accuracy: 0.6043 - auc: 0.7119 - loss: 0.6684\n",
      "Epoch 14/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 810ms/step - accuracy: 0.7189 - auc: 0.6828 - loss: 0.6349\n",
      "Epoch 15/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 847ms/step - accuracy: 0.5628 - auc: 0.6887 - loss: 0.6784\n",
      "Epoch 16/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 820ms/step - accuracy: 0.6319 - auc: 0.6399 - loss: 0.6487\n",
      "Epoch 17/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 877ms/step - accuracy: 0.7105 - auc: 0.6976 - loss: 0.6380\n",
      "Epoch 18/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 844ms/step - accuracy: 0.7837 - auc: 0.7658 - loss: 0.5433\n",
      "Epoch 19/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 859ms/step - accuracy: 0.7245 - auc: 0.7470 - loss: 0.6058\n",
      "Epoch 20/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 825ms/step - accuracy: 0.6826 - auc: 0.7794 - loss: 0.5875\n",
      "Epoch 21/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 826ms/step - accuracy: 0.7743 - auc: 0.7989 - loss: 0.5360\n",
      "Epoch 22/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 826ms/step - accuracy: 0.7843 - auc: 0.7564 - loss: 0.6339\n",
      "Epoch 23/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 860ms/step - accuracy: 0.5755 - auc: 0.7526 - loss: 0.6558\n",
      "Epoch 24/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 835ms/step - accuracy: 0.5950 - auc: 0.7547 - loss: 0.6081\n",
      "Epoch 25/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 831ms/step - accuracy: 0.8212 - auc: 0.8015 - loss: 0.5587\n",
      "Epoch 26/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 870ms/step - accuracy: 0.8038 - auc: 0.8577 - loss: 0.5025\n",
      "Epoch 27/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 686ms/step - accuracy: 0.7763 - auc: 0.7619 - loss: 0.6527\n",
      "Epoch 28/100\n",
      "\u001b[1m 3/21\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 787ms/step - accuracy: 0.6424 - auc: 0.7940 - loss: 0.5718"
     ]
    }
   ],
   "source": [
    "model_phaz3, train_eval_phase3, test_eval_phase3, X_test, y_test_cat = train_deep_cbn(dataset_path = '../Data/tox21.csv', target_col = 'NR-PPAR-gamma', smiles_col= 'smiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a17958",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_phaz3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# === Predict class probabilities from Phase 3 model ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m y_test_probs = \u001b[43mmodel_phaz3\u001b[49m.predict(X_test)[:, \u001b[32m1\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# === ROC Curve and AUC ===\u001b[39;00m\n\u001b[32m      9\u001b[39m fpr_test, tpr_test, _ = roc_curve(y_test_cat, y_test_probs)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_phaz3' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Predict class probabilities from Phase 3 model ===\n",
    "y_test_probs = model_phaz3.predict(X_test)[:, 1]\n",
    "\n",
    "# === ROC Curve and AUC ===\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test_cat, y_test_probs)\n",
    "auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# === Plot ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_test, tpr_test, '-', label=f'Test ROC (AUC = {auc_test:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Phase 3 Model ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
