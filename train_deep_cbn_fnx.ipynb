{"cells": [{"cell_type": "code", "metadata": {}, "source": ["def train_deep_cbn(dataset_path, target_col, smiles_col):\n", "    import os\n", "    import numpy as np\n", "    import pandas as pd\n", "    import tensorflow as tf\n", "    import keras\n", "    from sklearn.model_selection import train_test_split\n", "    from sklearn.utils.class_weight import compute_class_weight\n", "    from tensorflow.keras import optimizers, layers, metrics\n", "    from tensorflow.keras.layers import (\n", "        Input, Dense, Dropout, BatchNormalization, Conv1D,\n", "        GlobalAveragePooling1D, Lambda, Activation, Concatenate\n", "    )\n", "    from tensorflow.keras.models import Model\n", "    from tensorflow.keras.utils import to_categorical\n", "    from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n", "\n", "    # === Load Dataset ===\n", "    data = pd.read_csv(dataset_path)\n", "    smiles = data[smiles_col]\n", "    labels = data[target_col]\n", "\n", "    # === Tokenization ===\n", "    smiles_dict = {\n", "        \"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n", "        \"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n", "        \"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n", "        \"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n", "        \"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50, \"T\": 17, \"W\": 51,\n", "        \"V\": 18, \"Y\": 52, \"[\": 53, \"Z\": 19, \"]\": 54, \"\\\": 20, \"a\": 55, \"c\": 56,\n", "        \"b\": 21, \"e\": 57, \"d\": 22, \"g\": 58, \"f\": 23, \"i\": 59, \"h\": 24, \"m\": 60,\n", "        \"l\": 25, \"o\": 61, \"n\": 26, \"s\": 62, \"r\": 27, \"u\": 63, \"t\": 28, \"y\": 64,\n", "        \" \": 65, \":\": 66, \",\": 67, \"p\": 68, \"j\": 69, \"*\": 70\n", "    }\n", "\n", "    def label_smiles(line, max_len, smi_ch_ind):\n", "        x = np.zeros(max_len, dtype=int)\n", "        for i, ch in enumerate(line[:max_len]):\n", "            if ch != \"\n", "\" and ch in smi_ch_ind:\n", "                x[i] = smi_ch_ind[ch]\n", "        return x\n", "\n", "    XD = np.array([label_smiles(str(s), 100, smiles_dict) for s in smiles])\n", "    labels = labels.values\n", "    XD = to_categorical(XD, num_classes=71)\n", "\n", "    # === Feature model ===\n", "    XDinput = Input(shape=(100, 71), name='XDinput')\n", "    encode_smiles = Conv1D(filters=64, kernel_size=2, activation='relu', padding='valid', strides=1)(XDinput)\n", "    encode_smiles = Conv1D(filters=64, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)\n", "    encode_smiles = Conv1D(filters=128, kernel_size=4, activation='relu', padding='valid', strides=1)(encode_smiles)\n", "    model_feature = Model(inputs=XDinput, outputs=encode_smiles, name='model_feature')\n", "\n", "    input_extracted_feature = Input(shape=(93, 128))\n", "    FC1 = Dense(512, activation='relu')(input_extracted_feature)\n", "    FC1 = BatchNormalization()(FC1)\n", "    FC2 = Dropout(0.1)(FC1)\n", "    FC3 = Dense(256, activation='relu')(FC2)\n", "    FC3 = BatchNormalization()(FC3)\n", "    FC4 = Dropout(0.1)(FC3)\n", "    FC5 = Dense(64, activation='relu')(FC4)\n", "    predictions = Dense(2, activation='softmax')(FC5)\n", "    model_pred = Model(inputs=input_extracted_feature, outputs=predictions)\n", "\n", "    interaction_input = XDinput\n", "    encoded_features = model_feature(interaction_input)\n", "    predicted_output = model_pred(encoded_features)\n", "    pooled_output = GlobalAveragePooling1D()(predicted_output)\n", "    interactionModel = Model(inputs=interaction_input, outputs=pooled_output, name='interactionModel')\n", "\n", "    class DropPath(layers.Layer):\n", "        def __init__(self, drop_prob=0.):\n", "            super().__init__()\n", "            self.drop_prob = drop_prob\n", "        def call(self, x, training=None):\n", "            if (not training) or self.drop_prob == 0.:\n", "                return x\n", "            keep_prob = 1 - self.drop_prob\n", "            random_tensor = keep_prob\n", "            random_tensor += tf.random.uniform(tf.shape(x), dtype=x.dtype)\n", "            binary_tensor = tf.floor(random_tensor)\n", "            return tf.divide(x, keep_prob) * binary_tensor\n", "\n", "    class TopkRouting(layers.Layer):\n", "        def __init__(self, qk_dim, topk=16, qk_scale=None, param_routing=False, diff_routing=False):\n", "            super().__init__()\n", "            self.topk = topk\n", "            self.qk_dim = qk_dim\n", "            self.scale = qk_scale if qk_scale is not None else qk_dim**-0.5\n", "            self.diff_routing = diff_routing\n", "            if param_routing:\n", "                self.emb = layers.Dense(qk_dim)\n", "            else:\n", "                self.emb = lambda x: x\n", "            self.routing_act = lambda x, axis: tf.nn.softmax(x, axis=axis)\n", "        def call(self, query, key, training=None):\n", "            if not self.diff_routing:\n", "                query = tf.stop_gradient(query)\n", "                key = tf.stop_gradient(key)\n", "            query_hat = self.emb(query)\n", "            key_hat = self.emb(key)\n", "            attn_logit = tf.einsum('bnc,bqc->bnq', query_hat*self.scale, key_hat)\n", "            topk_attn_logit, topk_index = tf.math.top_k(attn_logit, k=self.topk, sorted=True)\n", "            r_weight = self.routing_act(topk_attn_logit, axis=-1)\n", "            return r_weight, topk_index\n", "\n", "    def tf_gather_kv(kv, r_idx):\n", "        n = tf.shape(kv)[0]\n", "        p2 = tf.shape(kv)[1]\n", "        c_kv = tf.shape(kv)[2]\n", "        topk = tf.shape(r_idx)[2]\n", "        batch_idx = tf.reshape(tf.range(n), [n, 1, 1])\n", "        batch_idx = tf.tile(batch_idx, [1, p2, topk])\n", "        p2_idx = tf.reshape(tf.range(p2), [1, p2, 1])\n", "        p2_idx = tf.tile(p2_idx, [n, 1, topk])\n", "        gather_indices = tf.stack([batch_idx, p2_idx, r_idx], axis=-1)\n", "        gathered = tf.gather_nd(kv, gather_indices)\n", "        return gathered\n", "\n", "    class KVGather(layers.Layer):\n", "        def __init__(self, mul_weight='none'):\n", "            super().__init__()\n", "            assert mul_weight in ['none', 'soft', 'hard']\n", "            self.mul_weight = mul_weight\n", "        def call(self, r_idx, r_weight, kv, training=None):\n", "            topk_kv = tf_gather_kv(kv, r_idx)\n", "            if self.mul_weight == 'soft':\n", "                r_weight_exp = tf.expand_dims(tf.expand_dims(r_weight, -1), -1)\n", "                topk_kv = topk_kv * r_weight_exp\n", "            return topk_kv\n", "\n", "    class QKVLinear(layers.Layer):\n", "        def __init__(self, dim, qk_dim, bias=True):\n", "            super().__init__()\n", "            self.dim = dim\n", "            self.qk_dim = qk_dim\n", "            self.qkv = layers.Dense(qk_dim + qk_dim + dim, use_bias=bias)\n", "        def call(self, x, training=None):\n", "            qkv = self.qkv(x)\n", "            q, kv = tf.split(qkv, [self.qk_dim, self.qk_dim + self.dim], axis=-1)\n", "            return q, kv\n", "\n", "    class BiLevelRoutingAttention(layers.Layer):\n", "        def __init__(self, dim, n_win=7, num_heads=8, qk_dim=None, qk_scale=None,\n", "                     kv_per_win=4, kv_downsample_ratio=4, kv_downsample_mode='identity',\n", "                     topk=4, param_attention=\"qkvo\", param_routing=False, diff_routing=False, soft_routing=False,\n", "                     side_dwconv=3, auto_pad=True):\n", "            super().__init__()\n", "            self.dim = dim\n", "            self.n_win = n_win\n", "            self.num_heads = num_heads\n", "            self.qk_dim = qk_dim if qk_dim is not None else dim\n", "            self.scale = qk_scale if qk_scale is not None else self.qk_dim**-0.5\n", "            self.topk = topk\n", "            self.param_routing = param_routing\n", "            self.diff_routing = diff_routing\n", "            self.soft_routing = soft_routing\n", "            self.auto_pad = auto_pad\n", "            if side_dwconv > 0:\n", "                self.lepe = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n", "            else:\n", "                self.lepe = lambda x: tf.zeros_like(x)\n", "            self.router = TopkRouting(qk_dim=self.qk_dim, topk=self.topk, qk_scale=self.scale,\n", "                                      param_routing=self.param_routing, diff_routing=self.diff_routing)\n", "            mul_weight = 'none'\n", "            if self.soft_routing:\n", "                mul_weight = 'soft'\n", "            self.kv_gather = KVGather(mul_weight=mul_weight)\n", "            if param_attention in ['qkvo', 'qkv']:\n", "                self.qkv = QKVLinear(self.dim, self.qk_dim)\n", "                if param_attention == 'qkvo':\n", "                    self.wo = layers.Dense(self.dim)\n", "                else:\n", "                    self.wo = lambda x: x\n", "            else:\n", "                raise ValueError(\"Unsupported param_attention mode\")\n", "            self.attn_act = lambda x: tf.nn.softmax(x, axis=-1)\n", "            self.kv_down = lambda x: x\n", "        def call(self, x, training=None, ret_attn_mask=False):\n", "            if ret_attn_mask:\n", "                return x, None, None, None\n", "            else:\n", "                return x\n", "\n", "    class Attention(layers.Layer):\n", "        def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n", "            super().__init__()\n", "            self.num_heads = num_heads\n", "            head_dim = dim // num_heads\n", "            self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n", "        def call(self, x, training=None):\n", "            return x\n", "\n", "    class AttentionLePE(layers.Layer):\n", "        def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., side_dwconv=3):\n", "            super().__init__()\n", "            self.num_heads = num_heads\n", "            head_dim = dim // num_heads\n", "            self.scale = qk_scale if qk_scale is not None else head_dim**-0.5\n", "            self.side_dwconv = layers.Conv1D(dim, kernel_size=side_dwconv, padding='same', activation='relu')\n", "        def call(self, x, training=None):\n", "            return x\n", "\n", "    class Mlp(layers.Layer):\n", "        def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n", "            super().__init__()\n", "            out_features = out_features or in_features\n", "            hidden_features = hidden_features or in_features\n", "            self.net = keras.Sequential([\n", "                layers.Dense(hidden_features),\n", "                layers.Activation('gelu'),\n", "                layers.Dropout(rate=drop),\n", "                layers.Dense(out_features),\n", "                layers.Dropout(rate=drop)\n", "            ])\n", "        def call(self, x, training=True):\n", "            return self.net(x, training=training)\n", "\n", "    class Block(layers.Layer):\n", "        def __init__(self, dim, drop_path=0.1, layer_scale_init_value=-1,\n", "                     num_heads=8, n_win=7, qk_dim=128, qk_scale=None,\n", "                     kv_per_win=8, kv_downsample_ratio=1, kv_downsample_mode='identity',\n", "                     topk=8, param_attention=\"qkvo\", param_routing=True, diff_routing=False, soft_routing=True,\n", "                     mlp_ratio=4, mlp_dwconv=False, side_dwconv=5, before_attn_dwconv=3, pre_norm=True, auto_pad=True):\n", "            super().__init__()\n", "            qk_dim = qk_dim or dim\n", "            if before_attn_dwconv > 0:\n", "                self.pos_embed = layers.Conv1D(dim, kernel_size=before_attn_dwconv, padding='same', activation='relu')\n", "            else:\n", "                self.pos_embed = lambda x: tf.zeros_like(x)\n", "            self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n", "            if topk > 0:\n", "                self.attn = BiLevelRoutingAttention(dim=dim, num_heads=num_heads, n_win=n_win, qk_dim=qk_dim,\n", "                                                   qk_scale=qk_scale, kv_per_win=kv_per_win, kv_downsample_ratio=kv_downsample_ratio,\n", "                                                   kv_downsample_mode=kv_downsample_mode, topk=topk, param_attention=param_attention,\n", "                                                   param_routing=param_routing, diff_routing=diff_routing, soft_routing=soft_routing,\n", "                                                   side_dwconv=side_dwconv, auto_pad=auto_pad)\n", "            elif topk == -1:\n", "                self.attn = Attention(dim=dim, num_heads=num_heads, qk_scale=qk_scale)\n", "            elif topk == -2:\n", "                self.attn = AttentionLePE(dim=dim, num_heads=num_heads, qk_scale=qk_scale, side_dwconv=side_dwconv)\n", "            else:\n", "                self.attn = keras.Sequential([\n", "                    layers.Dense(dim),\n", "                    layers.Conv1D(dim, kernel_size=5, padding='same', activation='relu'),\n", "                    layers.Dense(dim)\n", "                ])\n", "            self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n", "            mlp_hidden_dim = int(mlp_ratio * dim)\n", "            mlp_layers = [layers.Dense(mlp_hidden_dim)]\n", "            if mlp_dwconv:\n", "                mlp_layers.append(layers.Conv1D(mlp_hidden_dim, kernel_size=3, padding='same', activation='relu'))\n", "            mlp_layers.append(layers.Activation('gelu'))\n", "            mlp_layers.append(layers.Dense(dim))\n", "            mlp_layers.insert(1, layers.Dropout(0.2))\n", "            mlp_layers.append(layers.Dropout(0.2))\n", "            self.mlp = keras.Sequential(mlp_layers)\n", "            self.drop_path = DropPath(drop_path) if drop_path > 0. else layers.Lambda(lambda x: x)\n", "            if layer_scale_init_value > 0:\n", "                self.use_layer_scale = True\n", "                self.gamma1 = self.add_weight(shape=(dim,), initializer=tf.keras.initializers.Constant(layer_scale_init_value), trainable=True)\n", "                self.gamma2 = self.add_weight(shape=(dim,), initializer=tf.keras.initializers.Constant(layer_scale_init_value), trainable=True)\n", "            else:\n", "                self.use_layer_scale = False\n", "        def call(self, x, training=None):\n", "            input_x = x\n", "            x = self.pos_embed(x)\n", "            x = self.norm1(x)\n", "            attn = self.attn(x, training=training)\n", "            if self.use_layer_scale:\n", "                attn = self.gamma1 * attn\n", "            x = input_x + self.drop_path(attn, training=training)\n", "            input_x = x\n", "            x = self.norm2(x)\n", "            mlp_output = self.mlp(x, training=training)\n", "            if self.use_layer_scale:\n", "                mlp_output = self.gamma2 * mlp_output\n", "            x = input_x + self.drop_path(mlp_output, training=training)\n", "            return x\n", "\n", "    dim = 128\n", "    num_heads = 8\n", "\n", "    input_phaz2 = Input(shape=(93, 128))\n", "    processed_input = input_phaz2\n", "    transformer_block1 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n", "    transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n", "    transformer_output1 = transformer_block1(processed_input)\n", "    transformer_output1 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1)\n", "    transformer_output2 = transformer_block2(processed_input)\n", "    transformer_output2 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2)\n", "    combined_outputs = Concatenate(axis=-1)([transformer_output1, transformer_output2])\n", "    pooled_outputs = GlobalAveragePooling1D()(combined_outputs)\n", "    difference = Lambda(lambda x: tf.expand_dims(x[:, 0] - x[:, 1], axis=-1), name='difference')(pooled_outputs)\n", "    condition_2 = Activation('sigmoid', name='activation_11')(difference)\n", "    model_feature.trainable = False\n", "    model_phaz2 = Model(inputs=input_phaz2, outputs=condition_2)\n", "\n", "    new_transformer_block = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n", "    new_transformer_block2 = Block(dim=dim, drop_path=0.2, num_heads=num_heads, topk=16, mlp_ratio=3)\n", "    XDinput_phaz3 = Input(shape=(100, 71))\n", "    model_feature_output = model_feature(XDinput_phaz3)\n", "    transformer_output1_phaz3 = new_transformer_block(model_feature_output)\n", "    transformer_output1_phaz3 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output1_phaz3)\n", "    transformer_output2_phaz3 = new_transformer_block2(model_feature_output)\n", "    transformer_output2_phaz3 = Lambda(lambda x: tf.norm(x, axis=-1, keepdims=True))(transformer_output2_phaz3)\n", "    concatenated_phaz3 = Concatenate(axis=-1)([transformer_output1_phaz3, transformer_output2_phaz3])\n", "    model_feature.trainable = False\n", "    new_transformer_block.trainable = False\n", "    new_transformer_block2.trainable = False\n", "    pooled_phaz3 = GlobalAveragePooling1D()(concatenated_phaz3)\n", "    FC1_phaz3 = Dense(512, activation='relu')(pooled_phaz3)\n", "    FC1_phaz3 = BatchNormalization()(FC1_phaz3)\n", "    FC2_phaz3 = Dropout(0.1)(FC1_phaz3)\n", "    FC3_phaz3 = Dense(256, activation='relu')(FC2_phaz3)\n", "    FC3_phaz3 = BatchNormalization()(FC3_phaz3)\n", "    FC4_phaz3 = Dropout(0.1)(FC3_phaz3)\n", "    FC5_phaz3 = Dense(64, activation='relu')(FC4_phaz3)\n", "    predictions_phaz3 = Dense(2, activation='softmax')(FC5_phaz3)\n", "    model_phaz3 = Model(inputs=XDinput_phaz3, outputs=predictions_phaz3)\n", "\n", "    METRICS_BINARY = [\n", "        metrics.BinaryAccuracy(name='accuracy'),\n", "        metrics.AUC(name='auc'),\n", "    ]\n", "    METRICS_CATEGORICAL = [\n", "        metrics.CategoricalAccuracy(name='accuracy'),\n", "        metrics.Precision(name='precision'),\n", "        metrics.Recall(name='recall'),\n", "        metrics.AUC(name='auc'),\n", "        metrics.F1Score(average='macro', name='f1_score')\n", "    ]\n", "\n", "    XD_np = np.array(XD)\n", "    labels_np = np.array(labels)\n", "    index = np.where(~np.isnan(labels_np))\n", "    labels_np, XD_np = labels_np[index[0]], XD_np[index[0], :]\n", "\n", "    X_train, X_test, y_train, y_test = train_test_split(\n", "        XD_np, labels_np, test_size=0.2, stratify=labels_np, random_state=9\n", "    )\n", "\n", "    y_train_cat = to_categorical(y_train, num_classes=2)\n", "    y_test_cat = to_categorical(y_test, num_classes=2)\n", "\n", "    class_weights_array = compute_class_weight(\n", "        class_weight='balanced',\n", "        classes=np.unique(y_train),\n", "        y=y_train.ravel()\n", "    )\n", "    class_weight = {0: class_weights_array[0], 1: class_weights_array[1]}\n", "\n", "    opt = optimizers.Adam(learning_rate=0.0001)\n", "    interactionModel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=METRICS_CATEGORICAL)\n", "\n", "    early_stopping = EarlyStopping(monitor='loss', patience=30, verbose=1, mode='min', restore_best_weights=True)\n", "\n", "    interactionModel.fit(\n", "        X_train, y_train_cat,\n", "        batch_size=256, epochs=100, class_weight=class_weight,\n", "        callbacks=[early_stopping], verbose=1\n", "    )\n", "    test_eval_phase1 = interactionModel.evaluate(X_test, y_test_cat, verbose=1)\n", "\n", "    feature_train = model_feature.predict(X_train)\n", "    feature_test = model_feature.predict(X_test)\n", "\n", "    opt_phaz2 = optimizers.Adam(learning_rate=0.001)\n", "    model_phaz2.compile(optimizer=opt_phaz2, loss='binary_crossentropy', metrics=METRICS_BINARY)\n", "    model_phaz2.fit(\n", "        feature_train, y_train,\n", "        batch_size=256, epochs=100, class_weight=class_weight,\n", "        callbacks=[early_stopping], verbose=1\n", "    )\n", "\n", "    new_transformer_block.set_weights(transformer_block1.get_weights())\n", "    new_transformer_block2.set_weights(transformer_block2.get_weights())\n", "\n", "    opt_phaz3 = optimizers.Adam(learning_rate=0.0001)\n", "    model_phaz3.compile(optimizer=opt_phaz3, loss='categorical_crossentropy', metrics=METRICS_CATEGORICAL)\n", "    model_phaz3.fit(\n", "        X_train, y_train_cat,\n", "        batch_size=256, epochs=100, class_weight=class_weight,\n", "        callbacks=[early_stopping], verbose=1\n", "    )\n", "    train_eval_phase3 = model_phaz3.evaluate(X_train, y_train_cat, verbose=1)\n", "    test_eval_phase3 = model_phaz3.evaluate(X_test, y_test_cat, verbose=1)\n", "\n", "    def format_results(phase1_result, phase3_train_result, phase3_test_result):\n", "        phase1_metrics = ['loss', 'accuracy', 'precision', 'recall', 'auc', 'f1_score']\n", "        phase3_metrics = ['loss', 'accuracy', 'precision', 'recall', 'auc', 'f1_score']\n", "        phase1_test_df = pd.DataFrame([phase1_result], columns=phase1_metrics)\n", "        phase3_train_df = pd.DataFrame([phase3_train_result], columns=phase3_metrics)\n", "        phase3_test_df = pd.DataFrame([phase3_test_result], columns=phase3_metrics)\n", "        print(\"\n", "=== Phase 1: Test Evaluation Results ===\")\n", "        print(phase1_test_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n", "        print(\"\n", "=== Phase 3: Train Evaluation Results ===\")\n", "        print(phase3_train_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n", "        print(\"\n", "=== Phase 3: Test Evaluation Results ===\")\n", "        print(phase3_test_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n", "\n", "    format_results(test_eval_phase1, train_eval_phase3, test_eval_phase3)\n", "    return interactionModel, model_phaz2, model_phaz3\n"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}
